==PROF== Connected to process 100474 (/home/driffyn/Documents/CMDA_4634/heat_eq/cuda_heat_equation)
==PROF== Profiling "heat_kernel_2d" - 0: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 1: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 2: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 3: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 4: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 5: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 6: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 7: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 8: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 9: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 10: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 11: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 12: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 13: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 14: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 15: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 16: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 17: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 18: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 19: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 20: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 21: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 22: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 23: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 24: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 25: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 26: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 27: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 28: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 29: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 30: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 31: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 32: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 33: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 34: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 35: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 36: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 37: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 38: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 39: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 40: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 41: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 42: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 43: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 44: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 45: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 46: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 47: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 48: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 49: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 50: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 51: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 52: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 53: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 54: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 55: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 56: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 57: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 58: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 59: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 60: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 61: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 62: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 63: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 64: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 65: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 66: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 67: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 68: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 69: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 70: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 71: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 72: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 73: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 74: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 75: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 76: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 77: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 78: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 79: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 80: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 81: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 82: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 83: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 84: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 85: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 86: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 87: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 88: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 89: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 90: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 91: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 92: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 93: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 94: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 95: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 96: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 97: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 98: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 99: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 100: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 101: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 102: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 103: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 104: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 105: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 106: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 107: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 108: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 109: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 110: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 111: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 112: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 113: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 114: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 115: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 116: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 117: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 118: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 119: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 120: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 121: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 122: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 123: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 124: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 125: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 126: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 127: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 128: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 129: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 130: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 131: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 132: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 133: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 134: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 135: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 136: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 137: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 138: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 139: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 140: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 141: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 142: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 143: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 144: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 145: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 146: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 147: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 148: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 149: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 150: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 151: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 152: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 153: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 154: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 155: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 156: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 157: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 158: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 159: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 160: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 161: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 162: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 163: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 164: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 165: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 166: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 167: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 168: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 169: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 170: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 171: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 172: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 173: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 174: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 175: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 176: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 177: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 178: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 179: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 180: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 181: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 182: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 183: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 184: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 185: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 186: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 187: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 188: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 189: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 190: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 191: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 192: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 193: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 194: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 195: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 196: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 197: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 198: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 199: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 200: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 201: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 202: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 203: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 204: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 205: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 206: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 207: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 208: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 209: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 210: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 211: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 212: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 213: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 214: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 215: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 216: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 217: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 218: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 219: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 220: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 221: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 222: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 223: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 224: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 225: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 226: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 227: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 228: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 229: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 230: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 231: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 232: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 233: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 234: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 235: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 236: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 237: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 238: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 239: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 240: 0%....50%....100% - 8 passes
==PROF== Disconnected from process 100474
[100474] cuda_heat_equation@127.0.0.1
  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      104,667
    Memory Throughput                 %        77.83
    DRAM Throughput                   %        77.83
    Duration                         us        45.86
    L1/TEX Cache Throughput           %        14.71
    L2 Cache Throughput               %        46.16
    SM Active Cycles              cycle      108,621
    Compute (SM) Throughput           %        30.39
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.76
    Achieved Active Warps Per SM           warp        33.49
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 30.24%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      320,396
    Total DRAM Elapsed Cycles        cycle    1,646,592
    Average L1 Active Cycles         cycle      108,621
    Total L1 Elapsed Cycles          cycle    3,469,030
    Average L2 Active Cycles         cycle    81,699.69
    Total L2 Elapsed Cycles          cycle    1,390,768
    Average SM Active Cycles         cycle      108,621
    Total SM Elapsed Cycles          cycle    3,469,030
    Average SMSP Active Cycles       cycle    98,971.84
    Total SMSP Elapsed Cycles        cycle   13,876,120
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       71,166
    Memory Throughput                 %        92.25
    DRAM Throughput                   %        92.25
    Duration                         us        31.14
    L1/TEX Cache Throughput           %        17.20
    L2 Cache Throughput               %        20.78
    SM Active Cycles              cycle    68,814.82
    Compute (SM) Throughput           %        23.81
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.29
    Achieved Active Warps Per SM           warp        36.14
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.71%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      257,892
    Total DRAM Elapsed Cycles        cycle    1,118,208
    Average L1 Active Cycles         cycle    68,814.82
    Total L1 Elapsed Cycles          cycle    2,279,756
    Average L2 Active Cycles         cycle    55,265.88
    Total L2 Elapsed Cycles          cycle      944,672
    Average SM Active Cycles         cycle    68,814.82
    Total SM Elapsed Cycles          cycle    2,279,756
    Average SMSP Active Cycles       cycle    62,851.24
    Total SMSP Elapsed Cycles        cycle    9,119,024
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       97,379
    Memory Throughput                 %        72.96
    DRAM Throughput                   %        72.96
    Duration                         us        42.72
    L1/TEX Cache Throughput           %        21.16
    L2 Cache Throughput               %        47.89
    SM Active Cycles              cycle       73,179
    Compute (SM) Throughput           %        32.28
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.37
    Achieved Active Warps Per SM           warp        46.26
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      279,784
    Total DRAM Elapsed Cycles        cycle    1,533,952
    Average L1 Active Cycles         cycle       73,179
    Total L1 Elapsed Cycles          cycle    3,265,558
    Average L2 Active Cycles         cycle    76,141.88
    Total L2 Elapsed Cycles          cycle    1,295,120
    Average SM Active Cycles         cycle       73,179
    Total SM Elapsed Cycles          cycle    3,265,558
    Average SMSP Active Cycles       cycle    91,713.86
    Total SMSP Elapsed Cycles        cycle   13,062,232
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       71,152
    Memory Throughput                 %        91.22
    DRAM Throughput                   %        91.22
    Duration                         us        31.17
    L1/TEX Cache Throughput           %        21.86
    L2 Cache Throughput               %        20.80
    SM Active Cycles              cycle    35,276.47
    Compute (SM) Throughput           %        23.91
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       145.39
    Achieved Active Warps Per SM           warp        69.78
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      255,016
    Total DRAM Elapsed Cycles        cycle    1,118,208
    Average L1 Active Cycles         cycle    35,276.47
    Total L1 Elapsed Cycles          cycle    2,269,742
    Average L2 Active Cycles         cycle    64,216.31
    Total L2 Elapsed Cycles          cycle      943,984
    Average SM Active Cycles         cycle    35,276.47
    Total SM Elapsed Cycles          cycle    2,269,742
    Average SMSP Active Cycles       cycle    75,413.83
    Total SMSP Elapsed Cycles        cycle    9,078,968
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      100,065
    Memory Throughput                 %        72.61
    DRAM Throughput                   %        72.61
    Duration                         us        43.84
    L1/TEX Cache Throughput           %        19.23
    L2 Cache Throughput               %        47.50
    SM Active Cycles              cycle    80,524.41
    Compute (SM) Throughput           %        32.16
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        89.21
    Achieved Active Warps Per SM           warp        42.82
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.79%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (89.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      286,072
    Total DRAM Elapsed Cycles        cycle    1,575,936
    Average L1 Active Cycles         cycle    80,524.41
    Total L1 Elapsed Cycles          cycle    3,278,092
    Average L2 Active Cycles         cycle    49,385.38
    Total L2 Elapsed Cycles          cycle    1,330,368
    Average SM Active Cycles         cycle    80,524.41
    Total SM Elapsed Cycles          cycle    3,278,092
    Average SMSP Active Cycles       cycle    59,559.57
    Total SMSP Elapsed Cycles        cycle   13,112,368
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       86,405
    Memory Throughput                 %        86.38
    DRAM Throughput                   %        86.38
    Duration                         us        37.73
    L1/TEX Cache Throughput           %        21.89
    L2 Cache Throughput               %        17.14
    SM Active Cycles              cycle    35,225.82
    Compute (SM) Throughput           %        23.03
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       155.86
    Achieved Active Warps Per SM           warp        74.81
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      292,772
    Total DRAM Elapsed Cycles        cycle    1,355,776
    Average L1 Active Cycles         cycle    35,225.82
    Total L1 Elapsed Cycles          cycle    2,357,134
    Average L2 Active Cycles         cycle    52,802.44
    Total L2 Elapsed Cycles          cycle    1,145,232
    Average SM Active Cycles         cycle    35,225.82
    Total SM Elapsed Cycles          cycle    2,357,134
    Average SMSP Active Cycles       cycle    61,831.92
    Total SMSP Elapsed Cycles        cycle    9,428,536
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       95,133
    Memory Throughput                 %        70.76
    DRAM Throughput                   %        70.76
    Duration                         us        41.73
    L1/TEX Cache Throughput           %        17.02
    L2 Cache Throughput               %        46.82
    SM Active Cycles              cycle    90,980.82
    Compute (SM) Throughput           %        32.48
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.55
    Achieved Active Warps Per SM           warp        35.78
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.45%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      265,032
    Total DRAM Elapsed Cycles        cycle    1,498,112
    Average L1 Active Cycles         cycle    90,980.82
    Total L1 Elapsed Cycles          cycle    3,245,798
    Average L2 Active Cycles         cycle       73,967
    Total L2 Elapsed Cycles          cycle    1,264,672
    Average SM Active Cycles         cycle    90,980.82
    Total SM Elapsed Cycles          cycle    3,245,798
    Average SMSP Active Cycles       cycle    89,357.26
    Total SMSP Elapsed Cycles        cycle   12,983,192
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         9.00
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       65,729
    Memory Throughput                 %        92.54
    DRAM Throughput                   %        92.54
    Duration                         us        28.80
    L1/TEX Cache Throughput           %        17.10
    L2 Cache Throughput               %        22.49
    SM Active Cycles              cycle    62,528.74
    Compute (SM) Throughput           %        23.66
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.87
    Achieved Active Warps Per SM           warp        36.42
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.13%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      239,736
    Total DRAM Elapsed Cycles        cycle    1,036,288
    Average L1 Active Cycles         cycle    62,528.74
    Total L1 Elapsed Cycles          cycle    2,294,206
    Average L2 Active Cycles         cycle    29,029.75
    Total L2 Elapsed Cycles          cycle      873,328
    Average SM Active Cycles         cycle    62,528.74
    Total SM Elapsed Cycles          cycle    2,294,206
    Average SMSP Active Cycles       cycle    35,081.58
    Total SMSP Elapsed Cycles        cycle    9,176,824
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.73
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle        4,013
    Memory Throughput                 %         1.72
    DRAM Throughput                   %         1.72
    Duration                         us         1.76
    L1/TEX Cache Throughput           %         3.59
    L2 Cache Throughput               %         0.48
    SM Active Cycles              cycle       334.44
    Compute (SM) Throughput           %         0.42
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.51
    Achieved Active Warps Per SM           warp         7.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.49%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          264
    Total DRAM Elapsed Cycles        cycle       61,440
    Average L1 Active Cycles         cycle       334.44
    Total L1 Elapsed Cycles          cycle      121,528
    Average L2 Active Cycles         cycle       409.81
    Total L2 Elapsed Cycles          cycle       52,992
    Average SM Active Cycles         cycle       334.44
    Total SM Elapsed Cycles          cycle      121,528
    Average SMSP Active Cycles       cycle       313.36
    Total SMSP Elapsed Cycles        cycle      486,112
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.636%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.92% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.409%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.10% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.636%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.92% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.418%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 43.78% above the average, while the minimum instance value is 89.75% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       99,550
    Memory Throughput                 %        71.93
    DRAM Throughput                   %        71.93
    Duration                         us        43.65
    L1/TEX Cache Throughput           %        17.09
    L2 Cache Throughput               %        45.48
    SM Active Cycles              cycle    90,631.06
    Compute (SM) Throughput           %        33.10
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.59
    Achieved Active Warps Per SM           warp        37.72
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.41%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      282,092
    Total DRAM Elapsed Cycles        cycle    1,568,768
    Average L1 Active Cycles         cycle    90,631.06
    Total L1 Elapsed Cycles          cycle    3,184,680
    Average L2 Active Cycles         cycle    72,485.06
    Total L2 Elapsed Cycles          cycle    1,323,776
    Average SM Active Cycles         cycle    90,631.06
    Total SM Elapsed Cycles          cycle    3,184,680
    Average SMSP Active Cycles       cycle    87,424.76
    Total SMSP Elapsed Cycles        cycle   12,738,720
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       67,071
    Memory Throughput                 %        92.53
    DRAM Throughput                   %        92.53
    Duration                         us        29.44
    L1/TEX Cache Throughput           %        16.63
    L2 Cache Throughput               %        22.01
    SM Active Cycles              cycle    78,094.62
    Compute (SM) Throughput           %        23.01
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        63.27
    Achieved Active Warps Per SM           warp        30.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 36.73%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (63.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      244,464
    Total DRAM Elapsed Cycles        cycle    1,056,768
    Average L1 Active Cycles         cycle    78,094.62
    Total L1 Elapsed Cycles          cycle    2,358,448
    Average L2 Active Cycles         cycle    55,419.12
    Total L2 Elapsed Cycles          cycle      891,680
    Average SM Active Cycles         cycle    78,094.62
    Total SM Elapsed Cycles          cycle    2,358,448
    Average SMSP Active Cycles       cycle    64,267.60
    Total SMSP Elapsed Cycles        cycle    9,433,792
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       96,696
    Memory Throughput                 %        72.75
    DRAM Throughput                   %        72.75
    Duration                         us        42.40
    L1/TEX Cache Throughput           %        16.99
    L2 Cache Throughput               %        47.20
    SM Active Cycles              cycle    91,142.76
    Compute (SM) Throughput           %        32.55
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.43
    Achieved Active Warps Per SM           warp        36.69
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.57%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      277,116
    Total DRAM Elapsed Cycles        cycle    1,523,712
    Average L1 Active Cycles         cycle    91,142.76
    Total L1 Elapsed Cycles          cycle    3,238,426
    Average L2 Active Cycles         cycle    48,985.69
    Total L2 Elapsed Cycles          cycle    1,285,824
    Average SM Active Cycles         cycle    91,142.76
    Total SM Elapsed Cycles          cycle    3,238,426
    Average SMSP Active Cycles       cycle    59,071.55
    Total SMSP Elapsed Cycles        cycle   12,953,704
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       67,403
    Memory Throughput                 %        92.53
    DRAM Throughput                   %        92.53
    Duration                         us        29.57
    L1/TEX Cache Throughput           %        15.85
    L2 Cache Throughput               %        21.93
    SM Active Cycles              cycle    62,521.94
    Compute (SM) Throughput           %        21.94
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.14
    Achieved Active Warps Per SM           warp        37.51
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.86%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      245,408
    Total DRAM Elapsed Cycles        cycle    1,060,864
    Average L1 Active Cycles         cycle    62,521.94
    Total L1 Elapsed Cycles          cycle    2,474,310
    Average L2 Active Cycles         cycle    57,602.62
    Total L2 Elapsed Cycles          cycle      895,264
    Average SM Active Cycles         cycle    62,521.94
    Total SM Elapsed Cycles          cycle    2,474,310
    Average SMSP Active Cycles       cycle    66,194.65
    Total SMSP Elapsed Cycles        cycle    9,897,240
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle      102,015
    Memory Throughput                 %        73.18
    DRAM Throughput                   %        73.18
    Duration                         us        44.45
    L1/TEX Cache Throughput           %        16.62
    L2 Cache Throughput               %        46.98
    SM Active Cycles              cycle    93,171.21
    Compute (SM) Throughput           %        32.89
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.92
    Achieved Active Warps Per SM           warp        37.88
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.08%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      292,240
    Total DRAM Elapsed Cycles        cycle    1,597,440
    Average L1 Active Cycles         cycle    93,171.21
    Total L1 Elapsed Cycles          cycle    3,204,820
    Average L2 Active Cycles         cycle    81,251.81
    Total L2 Elapsed Cycles          cycle    1,352,448
    Average SM Active Cycles         cycle    93,171.21
    Total SM Elapsed Cycles          cycle    3,204,820
    Average SMSP Active Cycles       cycle    98,649.07
    Total SMSP Elapsed Cycles        cycle   12,819,280
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.25
    Elapsed Cycles                cycle       61,470
    Memory Throughput                 %        83.02
    DRAM Throughput                   %        83.02
    Duration                         us        27.33
    L1/TEX Cache Throughput           %        22.15
    L2 Cache Throughput               %        23.88
    SM Active Cycles              cycle    34,801.53
    Compute (SM) Throughput           %        18.71
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       114.83
    Achieved Active Warps Per SM           warp        55.12
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      203,616
    Total DRAM Elapsed Cycles        cycle      980,992
    Average L1 Active Cycles         cycle    34,801.53
    Total L1 Elapsed Cycles          cycle    2,900,184
    Average L2 Active Cycles         cycle    58,430.88
    Total L2 Elapsed Cycles          cycle      821,664
    Average SM Active Cycles         cycle    34,801.53
    Total SM Elapsed Cycles          cycle    2,900,184
    Average SMSP Active Cycles       cycle    67,778.03
    Total SMSP Elapsed Cycles        cycle   11,600,736
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      104,021
    Memory Throughput                 %        76.76
    DRAM Throughput                   %        76.76
    Duration                         us        45.50
    L1/TEX Cache Throughput           %        26.34
    L2 Cache Throughput               %        45.78
    SM Active Cycles              cycle    58,796.15
    Compute (SM) Throughput           %        24.79
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       128.15
    Achieved Active Warps Per SM           warp        61.51
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      313,996
    Total DRAM Elapsed Cycles        cycle    1,636,352
    Average L1 Active Cycles         cycle    58,796.15
    Total L1 Elapsed Cycles          cycle    4,252,670
    Average L2 Active Cycles         cycle    79,525.62
    Total L2 Elapsed Cycles          cycle    1,381,680
    Average SM Active Cycles         cycle    58,796.15
    Total SM Elapsed Cycles          cycle    4,252,670
    Average SMSP Active Cycles       cycle    95,936.43
    Total SMSP Elapsed Cycles        cycle   17,010,680
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       71,187
    Memory Throughput                 %        92.45
    DRAM Throughput                   %        92.45
    Duration                         us        31.14
    L1/TEX Cache Throughput           %        16.68
    L2 Cache Throughput               %        20.89
    SM Active Cycles              cycle    67,842.50
    Compute (SM) Throughput           %        23.07
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.34
    Achieved Active Warps Per SM           warp        35.68
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.66%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      258,436
    Total DRAM Elapsed Cycles        cycle    1,118,208
    Average L1 Active Cycles         cycle    67,842.50
    Total L1 Elapsed Cycles          cycle    2,352,862
    Average L2 Active Cycles         cycle    29,215.44
    Total L2 Elapsed Cycles          cycle      944,592
    Average SM Active Cycles         cycle    67,842.50
    Total SM Elapsed Cycles          cycle    2,352,862
    Average SMSP Active Cycles       cycle    35,394.10
    Total SMSP Elapsed Cycles        cycle    9,411,448
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.70
    SM Frequency                    Ghz         2.23
    Elapsed Cycles                cycle        3,287
    Memory Throughput                 %         1.91
    DRAM Throughput                   %         1.91
    Duration                         us         1.47
    L1/TEX Cache Throughput           %         3.65
    L2 Cache Throughput               %         0.57
    SM Active Cycles              cycle       329.09
    Compute (SM) Throughput           %         0.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.41
    Achieved Active Warps Per SM           warp         7.40
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.59%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          244
    Total DRAM Elapsed Cycles        cycle       51,200
    Average L1 Active Cycles         cycle       329.09
    Total L1 Elapsed Cycles          cycle      115,286
    Average L2 Active Cycles         cycle     1,137.12
    Total L2 Elapsed Cycles          cycle       43,360
    Average SM Active Cycles         cycle       329.09
    Total SM Elapsed Cycles          cycle      115,286
    Average SMSP Active Cycles       cycle       301.51
    Total SMSP Elapsed Cycles        cycle      461,144
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.884%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.93% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.511%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.22% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.884%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.93% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.35%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 27.06% above the average, while the minimum instance value is 27.62% below  
          the average.                                                                                                  

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       95,818
    Memory Throughput                 %        71.82
    DRAM Throughput                   %        71.82
    Duration                         us        42.05
    L1/TEX Cache Throughput           %        14.55
    L2 Cache Throughput               %        46.77
    SM Active Cycles              cycle   108,069.06
    Compute (SM) Throughput           %        30.10
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        63.71
    Achieved Active Warps Per SM           warp        30.58
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 36.29%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (63.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      271,360
    Total DRAM Elapsed Cycles        cycle    1,511,424
    Average L1 Active Cycles         cycle   108,069.06
    Total L1 Elapsed Cycles          cycle    3,502,342
    Average L2 Active Cycles         cycle    81,474.94
    Total L2 Elapsed Cycles          cycle    1,274,240
    Average SM Active Cycles         cycle   108,069.06
    Total SM Elapsed Cycles          cycle    3,502,342
    Average SMSP Active Cycles       cycle    98,226.21
    Total SMSP Elapsed Cycles        cycle   14,009,368
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       92,194
    Memory Throughput                 %        91.93
    DRAM Throughput                   %        91.93
    Duration                         us        40.22
    L1/TEX Cache Throughput           %        18.66
    L2 Cache Throughput               %        16.06
    SM Active Cycles              cycle    75,050.09
    Compute (SM) Throughput           %        25.83
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.71
    Achieved Active Warps Per SM           warp        36.82
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.29%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      332,300
    Total DRAM Elapsed Cycles        cycle    1,445,888
    Average L1 Active Cycles         cycle    75,050.09
    Total L1 Elapsed Cycles          cycle    2,101,326
    Average L2 Active Cycles         cycle    64,479.12
    Total L2 Elapsed Cycles          cycle    1,221,856
    Average SM Active Cycles         cycle    75,050.09
    Total SM Elapsed Cycles          cycle    2,101,326
    Average SMSP Active Cycles       cycle    75,008.65
    Total SMSP Elapsed Cycles        cycle    8,405,304
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.27
    Elapsed Cycles                cycle       94,694
    Memory Throughput                 %        70.68
    DRAM Throughput                   %        70.68
    Duration                         us        41.60
    L1/TEX Cache Throughput           %        18.21
    L2 Cache Throughput               %        47.37
    SM Active Cycles              cycle    85,036.79
    Compute (SM) Throughput           %        29.94
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        80.16
    Achieved Active Warps Per SM           warp        38.48
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 19.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (80.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      264,000
    Total DRAM Elapsed Cycles        cycle    1,494,016
    Average L1 Active Cycles         cycle    85,036.79
    Total L1 Elapsed Cycles          cycle    3,520,858
    Average L2 Active Cycles         cycle    97,441.69
    Total L2 Elapsed Cycles          cycle    1,260,336
    Average SM Active Cycles         cycle    85,036.79
    Total SM Elapsed Cycles          cycle    3,520,858
    Average SMSP Active Cycles       cycle   118,047.49
    Total SMSP Elapsed Cycles        cycle   14,083,432
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       69,490
    Memory Throughput                 %        92.36
    DRAM Throughput                   %        92.36
    Duration                         us        30.43
    L1/TEX Cache Throughput           %        15.00
    L2 Cache Throughput               %        21.26
    SM Active Cycles              cycle    62,731.47
    Compute (SM) Throughput           %        20.76
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.57
    Achieved Active Warps Per SM           warp        38.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.43%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      252,516
    Total DRAM Elapsed Cycles        cycle    1,093,632
    Average L1 Active Cycles         cycle    62,731.47
    Total L1 Elapsed Cycles          cycle    2,614,170
    Average L2 Active Cycles         cycle    60,855.88
    Total L2 Elapsed Cycles          cycle      922,784
    Average SM Active Cycles         cycle    62,731.47
    Total SM Elapsed Cycles          cycle    2,614,170
    Average SMSP Active Cycles       cycle    70,407.06
    Total SMSP Elapsed Cycles        cycle   10,456,680
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       94,179
    Memory Throughput                 %        71.76
    DRAM Throughput                   %        71.76
    Duration                         us        41.34
    L1/TEX Cache Throughput           %        18.63
    L2 Cache Throughput               %        47.40
    SM Active Cycles              cycle    83,152.18
    Compute (SM) Throughput           %        30.11
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        81.69
    Achieved Active Warps Per SM           warp        39.21
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 18.31%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (81.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      266,272
    Total DRAM Elapsed Cycles        cycle    1,484,288
    Average L1 Active Cycles         cycle    83,152.18
    Total L1 Elapsed Cycles          cycle    3,500,474
    Average L2 Active Cycles         cycle    81,536.19
    Total L2 Elapsed Cycles          cycle    1,252,912
    Average SM Active Cycles         cycle    83,152.18
    Total SM Elapsed Cycles          cycle    3,500,474
    Average SMSP Active Cycles       cycle    99,074.48
    Total SMSP Elapsed Cycles        cycle   14,001,896
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       67,649
    Memory Throughput                 %        92.60
    DRAM Throughput                   %        92.60
    Duration                         us        29.66
    L1/TEX Cache Throughput           %        16.86
    L2 Cache Throughput               %        21.82
    SM Active Cycles              cycle    65,893.38
    Compute (SM) Throughput           %        23.33
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.02
    Achieved Active Warps Per SM           warp        36.01
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      246,532
    Total DRAM Elapsed Cycles        cycle    1,064,960
    Average L1 Active Cycles         cycle    65,893.38
    Total L1 Elapsed Cycles          cycle    2,326,536
    Average L2 Active Cycles         cycle    56,218.81
    Total L2 Elapsed Cycles          cycle      899,440
    Average SM Active Cycles         cycle    65,893.38
    Total SM Elapsed Cycles          cycle    2,326,536
    Average SMSP Active Cycles       cycle    64,568.49
    Total SMSP Elapsed Cycles        cycle    9,306,144
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      102,685
    Memory Throughput                 %        73.69
    DRAM Throughput                   %        73.69
    Duration                         us        44.99
    L1/TEX Cache Throughput           %        23.97
    L2 Cache Throughput               %        45.01
    SM Active Cycles              cycle    64,623.41
    Compute (SM) Throughput           %        31.00
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       114.12
    Achieved Active Warps Per SM           warp        54.78
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      297,872
    Total DRAM Elapsed Cycles        cycle    1,616,896
    Average L1 Active Cycles         cycle    64,623.41
    Total L1 Elapsed Cycles          cycle    3,400,010
    Average L2 Active Cycles         cycle    49,358.31
    Total L2 Elapsed Cycles          cycle    1,365,104
    Average SM Active Cycles         cycle    64,623.41
    Total SM Elapsed Cycles          cycle    3,400,010
    Average SMSP Active Cycles       cycle    59,431.92
    Total SMSP Elapsed Cycles        cycle   13,600,040
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       68,826
    Memory Throughput                 %        92.11
    DRAM Throughput                   %        92.11
    Duration                         us        30.18
    L1/TEX Cache Throughput           %        15.98
    L2 Cache Throughput               %        22.18
    SM Active Cycles              cycle    68,509.24
    Compute (SM) Throughput           %        22.10
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.38
    Achieved Active Warps Per SM           warp        35.70
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.62%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      249,480
    Total DRAM Elapsed Cycles        cycle    1,083,392
    Average L1 Active Cycles         cycle    68,509.24
    Total L1 Elapsed Cycles          cycle    2,455,446
    Average L2 Active Cycles         cycle    29,065.62
    Total L2 Elapsed Cycles          cycle      914,480
    Average SM Active Cycles         cycle    68,509.24
    Total SM Elapsed Cycles          cycle    2,455,446
    Average SMSP Active Cycles       cycle    35,146.23
    Total SMSP Elapsed Cycles        cycle    9,821,784
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.85
    SM Frequency                    Ghz         2.24
    Elapsed Cycles                cycle        3,378
    Memory Throughput                 %         2.25
    DRAM Throughput                   %         2.25
    Duration                         us         1.50
    L1/TEX Cache Throughput           %         3.46
    L2 Cache Throughput               %         0.58
    SM Active Cycles              cycle       346.53
    Compute (SM) Throughput           %         0.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.80
    Achieved Active Warps Per SM           warp         7.10
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.2%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (14.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          300
    Total DRAM Elapsed Cycles        cycle       53,248
    Average L1 Active Cycles         cycle       346.53
    Total L1 Elapsed Cycles          cycle      114,740
    Average L2 Active Cycles         cycle       367.44
    Total L2 Elapsed Cycles          cycle       44,496
    Average SM Active Cycles         cycle       346.53
    Total SM Elapsed Cycles          cycle      114,740
    Average SMSP Active Cycles       cycle       313.77
    Total SMSP Elapsed Cycles        cycle      458,960
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.281%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.90% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.796%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.09% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.281%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.90% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.713%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 50.81% above the average, while the minimum instance value is 88.57% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       96,148
    Memory Throughput                 %        73.49
    DRAM Throughput                   %        73.49
    Duration                         us        42.21
    L1/TEX Cache Throughput           %        16.62
    L2 Cache Throughput               %        48.05
    SM Active Cycles              cycle    93,184.76
    Compute (SM) Throughput           %        27.24
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.29
    Achieved Active Warps Per SM           warp        35.66
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.71%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      278,444
    Total DRAM Elapsed Cycles        cycle    1,515,520
    Average L1 Active Cycles         cycle    93,184.76
    Total L1 Elapsed Cycles          cycle    3,869,428
    Average L2 Active Cycles         cycle    89,189.94
    Total L2 Elapsed Cycles          cycle    1,279,872
    Average SM Active Cycles         cycle    93,184.76
    Total SM Elapsed Cycles          cycle    3,869,428
    Average SMSP Active Cycles       cycle   107,905.95
    Total SMSP Elapsed Cycles        cycle   15,477,712
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       69,972
    Memory Throughput                 %        92.11
    DRAM Throughput                   %        92.11
    Duration                         us        30.66
    L1/TEX Cache Throughput           %        17.15
    L2 Cache Throughput               %        21.11
    SM Active Cycles              cycle    64,447.74
    Compute (SM) Throughput           %        23.74
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.41
    Achieved Active Warps Per SM           warp        36.20
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.59%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      253,492
    Total DRAM Elapsed Cycles        cycle    1,100,800
    Average L1 Active Cycles         cycle    64,447.74
    Total L1 Elapsed Cycles          cycle    2,286,736
    Average L2 Active Cycles         cycle    57,192.81
    Total L2 Elapsed Cycles          cycle      929,312
    Average SM Active Cycles         cycle    64,447.74
    Total SM Elapsed Cycles          cycle    2,286,736
    Average SMSP Active Cycles       cycle    66,603.68
    Total SMSP Elapsed Cycles        cycle    9,146,944
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle      113,259
    Memory Throughput                 %        77.22
    DRAM Throughput                   %        77.22
    Duration                         us        49.47
    L1/TEX Cache Throughput           %        15.95
    L2 Cache Throughput               %        45.08
    SM Active Cycles              cycle   107,173.91
    Compute (SM) Throughput           %        32.98
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.71
    Achieved Active Warps Per SM           warp        36.34
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.29%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      343,160
    Total DRAM Elapsed Cycles        cycle    1,777,664
    Average L1 Active Cycles         cycle   107,173.91
    Total L1 Elapsed Cycles          cycle    3,196,548
    Average L2 Active Cycles         cycle    74,534.38
    Total L2 Elapsed Cycles          cycle    1,503,264
    Average SM Active Cycles         cycle   107,173.91
    Total SM Elapsed Cycles          cycle    3,196,548
    Average SMSP Active Cycles       cycle    89,978.35
    Total SMSP Elapsed Cycles        cycle   12,786,192
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       69,134
    Memory Throughput                 %        92.06
    DRAM Throughput                   %        92.06
    Duration                         us        30.27
    L1/TEX Cache Throughput           %        17.56
    L2 Cache Throughput               %        21.38
    SM Active Cycles              cycle    60,902.65
    Compute (SM) Throughput           %        24.29
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        81.00
    Achieved Active Warps Per SM           warp        38.88
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 19%                                                                                       
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (81.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      250,280
    Total DRAM Elapsed Cycles        cycle    1,087,488
    Average L1 Active Cycles         cycle    60,902.65
    Total L1 Elapsed Cycles          cycle    2,234,198
    Average L2 Active Cycles         cycle    54,342.25
    Total L2 Elapsed Cycles          cycle      917,664
    Average SM Active Cycles         cycle    60,902.65
    Total SM Elapsed Cycles          cycle    2,234,198
    Average SMSP Active Cycles       cycle    63,554.64
    Total SMSP Elapsed Cycles        cycle    8,936,792
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.51
    SM Frequency                    Ghz         2.23
    Elapsed Cycles                cycle        3,357
    Memory Throughput                 %         2.12
    DRAM Throughput                   %         2.12
    Duration                         us         1.50
    L1/TEX Cache Throughput           %         3.51
    L2 Cache Throughput               %         0.58
    SM Active Cycles              cycle       342.26
    Compute (SM) Throughput           %         0.43
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.37
    Achieved Active Warps Per SM           warp         6.90
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.63%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (14.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          272
    Total DRAM Elapsed Cycles        cycle       51,200
    Average L1 Active Cycles         cycle       342.26
    Total L1 Elapsed Cycles          cycle      116,466
    Average L2 Active Cycles         cycle       427.94
    Total L2 Elapsed Cycles          cycle       44,256
    Average SM Active Cycles         cycle       342.26
    Total SM Elapsed Cycles          cycle      116,466
    Average SMSP Active Cycles       cycle       301.79
    Total SMSP Elapsed Cycles        cycle      465,864
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.086%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.92% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.447%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.17% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.086%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.92% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.729%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 56.42% above the average, while the minimum instance value is 90.19% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      100,449
    Memory Throughput                 %        77.13
    DRAM Throughput                   %        77.13
    Duration                         us           44
    L1/TEX Cache Throughput           %        16.22
    L2 Cache Throughput               %        47.58
    SM Active Cycles              cycle    95,506.65
    Compute (SM) Throughput           %        32.16
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.57
    Achieved Active Warps Per SM           warp        36.76
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.43%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      304,688
    Total DRAM Elapsed Cycles        cycle    1,580,032
    Average L1 Active Cycles         cycle    95,506.65
    Total L1 Elapsed Cycles          cycle    3,277,836
    Average L2 Active Cycles         cycle    65,849.12
    Total L2 Elapsed Cycles          cycle    1,334,432
    Average SM Active Cycles         cycle    95,506.65
    Total SM Elapsed Cycles          cycle    3,277,836
    Average SMSP Active Cycles       cycle    79,407.57
    Total SMSP Elapsed Cycles        cycle   13,111,344
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       68,748
    Memory Throughput                 %        91.82
    DRAM Throughput                   %        91.82
    Duration                         us        30.14
    L1/TEX Cache Throughput           %        16.84
    L2 Cache Throughput               %        21.49
    SM Active Cycles              cycle    68,199.97
    Compute (SM) Throughput           %        23.29
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.52
    Achieved Active Warps Per SM           warp        35.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.48%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      248,224
    Total DRAM Elapsed Cycles        cycle    1,081,344
    Average L1 Active Cycles         cycle    68,199.97
    Total L1 Elapsed Cycles          cycle    2,330,326
    Average L2 Active Cycles         cycle    59,983.38
    Total L2 Elapsed Cycles          cycle      913,632
    Average SM Active Cycles         cycle    68,199.97
    Total SM Elapsed Cycles          cycle    2,330,326
    Average SMSP Active Cycles       cycle    70,092.90
    Total SMSP Elapsed Cycles        cycle    9,321,304
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       93,964
    Memory Throughput                 %        72.95
    DRAM Throughput                   %        72.95
    Duration                         us        41.25
    L1/TEX Cache Throughput           %        16.46
    L2 Cache Throughput               %        47.03
    SM Active Cycles              cycle    94,099.09
    Compute (SM) Throughput           %        31.67
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.75
    Achieved Active Warps Per SM           warp        34.44
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.25%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      270,048
    Total DRAM Elapsed Cycles        cycle    1,480,704
    Average L1 Active Cycles         cycle    94,099.09
    Total L1 Elapsed Cycles          cycle    3,327,806
    Average L2 Active Cycles         cycle    80,597.88
    Total L2 Elapsed Cycles          cycle    1,250,400
    Average SM Active Cycles         cycle    94,099.09
    Total SM Elapsed Cycles          cycle    3,327,806
    Average SMSP Active Cycles       cycle    97,424.65
    Total SMSP Elapsed Cycles        cycle   13,311,224
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       68,669
    Memory Throughput                 %        92.35
    DRAM Throughput                   %        92.35
    Duration                         us        30.11
    L1/TEX Cache Throughput           %        16.63
    L2 Cache Throughput               %        21.54
    SM Active Cycles              cycle    64,856.41
    Compute (SM) Throughput           %        23.00
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.60
    Achieved Active Warps Per SM           warp        37.25
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.4%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      249,644
    Total DRAM Elapsed Cycles        cycle    1,081,344
    Average L1 Active Cycles         cycle    64,856.41
    Total L1 Elapsed Cycles          cycle    2,359,382
    Average L2 Active Cycles         cycle    29,191.38
    Total L2 Elapsed Cycles          cycle      912,384
    Average SM Active Cycles         cycle    64,856.41
    Total SM Elapsed Cycles          cycle    2,359,382
    Average SMSP Active Cycles       cycle    35,292.08
    Total SMSP Elapsed Cycles        cycle    9,437,528
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.89
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       14,403
    Memory Throughput                 %         2.41
    DRAM Throughput                   %         0.44
    Duration                         us         6.30
    L1/TEX Cache Throughput           %         3.48
    L2 Cache Throughput               %         2.41
    SM Active Cycles              cycle       345.26
    Compute (SM) Throughput           %         0.41
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        93.89
    Achieved Active Warps Per SM           warp        45.07
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          244
    Total DRAM Elapsed Cycles        cycle      224,256
    Average L1 Active Cycles         cycle       345.26
    Total L1 Elapsed Cycles          cycle      116,830
    Average L2 Active Cycles         cycle       437.12
    Total L2 Elapsed Cycles          cycle      189,968
    Average SM Active Cycles         cycle       345.26
    Total SM Elapsed Cycles          cycle      116,830
    Average SMSP Active Cycles       cycle       326.26
    Total SMSP Elapsed Cycles        cycle      467,320
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.13%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.96% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.93%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 72.99% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.13%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.96% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       99,387
    Memory Throughput                 %        77.45
    DRAM Throughput                   %        77.45
    Duration                         us        43.58
    L1/TEX Cache Throughput           %        25.49
    L2 Cache Throughput               %        47.15
    SM Active Cycles              cycle    60,771.82
    Compute (SM) Throughput           %        26.97
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       118.54
    Achieved Active Warps Per SM           warp        56.90
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      303,348
    Total DRAM Elapsed Cycles        cycle    1,566,720
    Average L1 Active Cycles         cycle    60,771.82
    Total L1 Elapsed Cycles          cycle    3,907,378
    Average L2 Active Cycles         cycle    77,937.75
    Total L2 Elapsed Cycles          cycle    1,321,680
    Average SM Active Cycles         cycle    60,771.82
    Total SM Elapsed Cycles          cycle    3,907,378
    Average SMSP Active Cycles       cycle    94,275.29
    Total SMSP Elapsed Cycles        cycle   15,629,512
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       68,038
    Memory Throughput                 %        92.62
    DRAM Throughput                   %        92.62
    Duration                         us        29.82
    L1/TEX Cache Throughput           %        15.57
    L2 Cache Throughput               %        21.70
    SM Active Cycles              cycle    65,196.12
    Compute (SM) Throughput           %        21.54
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.52
    Achieved Active Warps Per SM           warp        36.25
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.48%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      248,004
    Total DRAM Elapsed Cycles        cycle    1,071,104
    Average L1 Active Cycles         cycle    65,196.12
    Total L1 Elapsed Cycles          cycle    2,519,626
    Average L2 Active Cycles         cycle    29,094.31
    Total L2 Elapsed Cycles          cycle      904,448
    Average SM Active Cycles         cycle    65,196.12
    Total SM Elapsed Cycles          cycle    2,519,626
    Average SMSP Active Cycles       cycle    35,174.97
    Total SMSP Elapsed Cycles        cycle   10,078,504
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       99,942
    Memory Throughput                 %        73.95
    DRAM Throughput                   %        73.95
    Duration                         us        43.78
    L1/TEX Cache Throughput           %        16.43
    L2 Cache Throughput               %        45.27
    SM Active Cycles              cycle    94,279.71
    Compute (SM) Throughput           %        31.41
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.20
    Achieved Active Warps Per SM           warp        36.58
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.8%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      290,960
    Total DRAM Elapsed Cycles        cycle    1,573,888
    Average L1 Active Cycles         cycle    94,279.71
    Total L1 Elapsed Cycles          cycle    3,355,180
    Average L2 Active Cycles         cycle    80,459.50
    Total L2 Elapsed Cycles          cycle    1,328,880
    Average SM Active Cycles         cycle    94,279.71
    Total SM Elapsed Cycles          cycle    3,355,180
    Average SMSP Active Cycles       cycle    97,276.01
    Total SMSP Elapsed Cycles        cycle   13,420,720
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       69,205
    Memory Throughput                 %        92.40
    DRAM Throughput                   %        92.40
    Duration                         us        30.30
    L1/TEX Cache Throughput           %        14.72
    L2 Cache Throughput               %        21.45
    SM Active Cycles              cycle    72,862.09
    Compute (SM) Throughput           %        20.37
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.70
    Achieved Active Warps Per SM           warp        33.46
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 30.3%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      251,452
    Total DRAM Elapsed Cycles        cycle    1,088,512
    Average L1 Active Cycles         cycle    72,862.09
    Total L1 Elapsed Cycles          cycle    2,663,974
    Average L2 Active Cycles         cycle       55,119
    Total L2 Elapsed Cycles          cycle      918,928
    Average SM Active Cycles         cycle    72,862.09
    Total SM Elapsed Cycles          cycle    2,663,974
    Average SMSP Active Cycles       cycle    64,298.90
    Total SMSP Elapsed Cycles        cycle   10,655,896
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.89
    SM Frequency                    Ghz         2.27
    Elapsed Cycles                cycle        3,918
    Memory Throughput                 %         1.69
    DRAM Throughput                   %         1.69
    Duration                         us         1.73
    L1/TEX Cache Throughput           %         3.52
    L2 Cache Throughput               %         0.48
    SM Active Cycles              cycle       341.12
    Compute (SM) Throughput           %         0.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.40
    Achieved Active Warps Per SM           warp         6.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.6%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (14.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          260
    Total DRAM Elapsed Cycles        cycle       61,440
    Average L1 Active Cycles         cycle       341.12
    Total L1 Elapsed Cycles          cycle      114,638
    Average L2 Active Cycles         cycle          391
    Total L2 Elapsed Cycles          cycle       51,696
    Average SM Active Cycles         cycle       341.12
    Total SM Elapsed Cycles          cycle      114,638
    Average SMSP Active Cycles       cycle       303.04
    Total SMSP Elapsed Cycles        cycle      458,552
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.175%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.92% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.577%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.18% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.175%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.92% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.323%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 43.98% above the average, while the minimum instance value is 89.26% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       97,661
    Memory Throughput                 %        72.53
    DRAM Throughput                   %        72.53
    Duration                         us        42.78
    L1/TEX Cache Throughput           %        15.81
    L2 Cache Throughput               %        44.50
    SM Active Cycles              cycle    97,944.15
    Compute (SM) Throughput           %        27.67
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.78
    Achieved Active Warps Per SM           warp        34.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.22%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      278,708
    Total DRAM Elapsed Cycles        cycle    1,537,024
    Average L1 Active Cycles         cycle    97,944.15
    Total L1 Elapsed Cycles          cycle    3,808,966
    Average L2 Active Cycles         cycle    89,790.19
    Total L2 Elapsed Cycles          cycle    1,297,472
    Average SM Active Cycles         cycle    97,944.15
    Total SM Elapsed Cycles          cycle    3,808,966
    Average SMSP Active Cycles       cycle   108,590.96
    Total SMSP Elapsed Cycles        cycle   15,235,864
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       78,604
    Memory Throughput                 %        90.96
    DRAM Throughput                   %        90.96
    Duration                         us        34.40
    L1/TEX Cache Throughput           %        16.73
    L2 Cache Throughput               %        18.81
    SM Active Cycles              cycle    62,595.94
    Compute (SM) Throughput           %        23.15
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        86.78
    Achieved Active Warps Per SM           warp        41.66
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 13.22%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (86.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      280,836
    Total DRAM Elapsed Cycles        cycle    1,234,944
    Average L1 Active Cycles         cycle    62,595.94
    Total L1 Elapsed Cycles          cycle    2,344,092
    Average L2 Active Cycles         cycle    61,092.44
    Total L2 Elapsed Cycles          cycle    1,043,264
    Average SM Active Cycles         cycle    62,595.94
    Total SM Elapsed Cycles          cycle    2,344,092
    Average SMSP Active Cycles       cycle    71,158.66
    Total SMSP Elapsed Cycles        cycle    9,376,368
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.49
    SM Frequency                    Ghz         2.22
    Elapsed Cycles                cycle        3,482
    Memory Throughput                 %         2.73
    DRAM Throughput                   %         2.73
    Duration                         us         1.57
    L1/TEX Cache Throughput           %         3.55
    L2 Cache Throughput               %         0.56
    SM Active Cycles              cycle       337.65
    Compute (SM) Throughput           %         0.40
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.33
    Achieved Active Warps Per SM           warp         7.36
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.67%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          364
    Total DRAM Elapsed Cycles        cycle       53,248
    Average L1 Active Cycles         cycle       337.65
    Total L1 Elapsed Cycles          cycle      117,190
    Average L2 Active Cycles         cycle       395.81
    Total L2 Elapsed Cycles          cycle       45,936
    Average SM Active Cycles         cycle       337.65
    Total SM Elapsed Cycles          cycle      117,190
    Average SMSP Active Cycles       cycle       313.35
    Total SMSP Elapsed Cycles        cycle      468,760
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.95%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.94% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.64%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.03% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.95%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.94% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.133%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 44.49% above the average, while the minimum instance value is 89.39% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      101,517
    Memory Throughput                 %        74.89
    DRAM Throughput                   %        74.89
    Duration                         us        44.45
    L1/TEX Cache Throughput           %        14.95
    L2 Cache Throughput               %        46.44
    SM Active Cycles              cycle   122,840.18
    Compute (SM) Throughput           %        30.88
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        58.96
    Achieved Active Warps Per SM           warp        28.30
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 41.04%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (59.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      299,088
    Total DRAM Elapsed Cycles        cycle    1,597,440
    Average L1 Active Cycles         cycle   122,840.18
    Total L1 Elapsed Cycles          cycle    3,412,068
    Average L2 Active Cycles         cycle    78,918.25
    Total L2 Elapsed Cycles          cycle    1,349,424
    Average SM Active Cycles         cycle   122,840.18
    Total SM Elapsed Cycles          cycle    3,412,068
    Average SMSP Active Cycles       cycle    95,627.49
    Total SMSP Elapsed Cycles        cycle   13,648,272
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       75,575
    Memory Throughput                 %        91.97
    DRAM Throughput                   %        91.97
    Duration                         us        33.06
    L1/TEX Cache Throughput           %        16.97
    L2 Cache Throughput               %        19.57
    SM Active Cycles              cycle    80,556.09
    Compute (SM) Throughput           %        23.49
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        65.99
    Achieved Active Warps Per SM           warp        31.67
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 34.01%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (66.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      273,100
    Total DRAM Elapsed Cycles        cycle    1,187,840
    Average L1 Active Cycles         cycle    80,556.09
    Total L1 Elapsed Cycles          cycle    2,310,400
    Average L2 Active Cycles         cycle    51,622.38
    Total L2 Elapsed Cycles          cycle    1,003,216
    Average SM Active Cycles         cycle    80,556.09
    Total SM Elapsed Cycles          cycle    2,310,400
    Average SMSP Active Cycles       cycle    59,216.66
    Total SMSP Elapsed Cycles        cycle    9,241,600
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       99,137
    Memory Throughput                 %        75.85
    DRAM Throughput                   %        75.85
    Duration                         us        43.42
    L1/TEX Cache Throughput           %        15.83
    L2 Cache Throughput               %        47.78
    SM Active Cycles              cycle       97,821
    Compute (SM) Throughput           %        31.74
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.36
    Achieved Active Warps Per SM           warp        35.21
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.64%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      295,916
    Total DRAM Elapsed Cycles        cycle    1,560,576
    Average L1 Active Cycles         cycle       97,821
    Total L1 Elapsed Cycles          cycle    3,319,616
    Average L2 Active Cycles         cycle    73,345.69
    Total L2 Elapsed Cycles          cycle    1,317,680
    Average SM Active Cycles         cycle       97,821
    Total SM Elapsed Cycles          cycle    3,319,616
    Average SMSP Active Cycles       cycle    88,799.97
    Total SMSP Elapsed Cycles        cycle   13,278,464
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       70,167
    Memory Throughput                 %        79.75
    DRAM Throughput                   %        79.75
    Duration                         us        30.69
    L1/TEX Cache Throughput           %        13.87
    L2 Cache Throughput               %        27.21
    SM Active Cycles              cycle   117,954.21
    Compute (SM) Throughput           %        19.20
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        39.59
    Achieved Active Warps Per SM           warp        19.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 60.41%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (39.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      219,676
    Total DRAM Elapsed Cycles        cycle    1,101,824
    Average L1 Active Cycles         cycle   117,954.21
    Total L1 Elapsed Cycles          cycle    2,826,388
    Average L2 Active Cycles         cycle    58,865.94
    Total L2 Elapsed Cycles          cycle      931,232
    Average SM Active Cycles         cycle   117,954.21
    Total SM Elapsed Cycles          cycle    2,826,388
    Average SMSP Active Cycles       cycle    67,627.56
    Total SMSP Elapsed Cycles        cycle   11,305,552
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       89,635
    Memory Throughput                 %        80.30
    DRAM Throughput                   %        80.30
    Duration                         us        39.36
    L1/TEX Cache Throughput           %        16.91
    L2 Cache Throughput               %        48.43
    SM Active Cycles              cycle    91,566.88
    Compute (SM) Throughput           %        30.99
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.73
    Achieved Active Warps Per SM           warp        33.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.27%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      283,904
    Total DRAM Elapsed Cycles        cycle    1,414,144
    Average L1 Active Cycles         cycle    91,566.88
    Total L1 Elapsed Cycles          cycle    3,399,834
    Average L2 Active Cycles         cycle    88,192.50
    Total L2 Elapsed Cycles          cycle    1,193,488
    Average SM Active Cycles         cycle    91,566.88
    Total SM Elapsed Cycles          cycle    3,399,834
    Average SMSP Active Cycles       cycle   106,763.16
    Total SMSP Elapsed Cycles        cycle   13,599,336
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       67,748
    Memory Throughput                 %        92.47
    DRAM Throughput                   %        92.47
    Duration                         us        29.70
    L1/TEX Cache Throughput           %        20.84
    L2 Cache Throughput               %        21.80
    SM Active Cycles              cycle    37,000.74
    Compute (SM) Throughput           %        20.08
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       134.04
    Achieved Active Warps Per SM           warp        64.34
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      246,184
    Total DRAM Elapsed Cycles        cycle    1,064,960
    Average L1 Active Cycles         cycle    37,000.74
    Total L1 Elapsed Cycles          cycle    2,701,076
    Average L2 Active Cycles         cycle    58,637.50
    Total L2 Elapsed Cycles          cycle      900,112
    Average SM Active Cycles         cycle    37,000.74
    Total SM Elapsed Cycles          cycle    2,701,076
    Average SMSP Active Cycles       cycle    68,170.15
    Total SMSP Elapsed Cycles        cycle   10,804,304
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      101,086
    Memory Throughput                 %        73.54
    DRAM Throughput                   %        73.54
    Duration                         us        44.26
    L1/TEX Cache Throughput           %        16.23
    L2 Cache Throughput               %        45.03
    SM Active Cycles              cycle    95,413.35
    Compute (SM) Throughput           %        30.28
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.90
    Achieved Active Warps Per SM           warp        36.43
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.1%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      292,360
    Total DRAM Elapsed Cycles        cycle    1,590,272
    Average L1 Active Cycles         cycle    95,413.35
    Total L1 Elapsed Cycles          cycle    3,479,368
    Average L2 Active Cycles         cycle    83,573.38
    Total L2 Elapsed Cycles          cycle    1,343,504
    Average SM Active Cycles         cycle    95,413.35
    Total SM Elapsed Cycles          cycle    3,479,368
    Average SMSP Active Cycles       cycle   101,186.56
    Total SMSP Elapsed Cycles        cycle   13,917,472
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       67,870
    Memory Throughput                 %        92.35
    DRAM Throughput                   %        92.35
    Duration                         us        29.73
    L1/TEX Cache Throughput           %        13.92
    L2 Cache Throughput               %        21.77
    SM Active Cycles              cycle   145,070.74
    Compute (SM) Throughput           %        19.25
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        33.53
    Achieved Active Warps Per SM           warp        16.09
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.47%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (33.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      246,828
    Total DRAM Elapsed Cycles        cycle    1,069,056
    Average L1 Active Cycles         cycle   145,070.74
    Total L1 Elapsed Cycles          cycle    2,817,578
    Average L2 Active Cycles         cycle    57,497.19
    Total L2 Elapsed Cycles          cycle      901,536
    Average SM Active Cycles         cycle   145,070.74
    Total SM Elapsed Cycles          cycle    2,817,578
    Average SMSP Active Cycles       cycle    66,936.46
    Total SMSP Elapsed Cycles        cycle   11,270,312
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      107,315
    Memory Throughput                 %        76.97
    DRAM Throughput                   %        76.97
    Duration                         us        46.94
    L1/TEX Cache Throughput           %        26.06
    L2 Cache Throughput               %        46.68
    SM Active Cycles              cycle    59,438.03
    Compute (SM) Throughput           %        27.75
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       131.76
    Achieved Active Warps Per SM           warp        63.24
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      324,320
    Total DRAM Elapsed Cycles        cycle    1,685,504
    Average L1 Active Cycles         cycle    59,438.03
    Total L1 Elapsed Cycles          cycle    3,795,340
    Average L2 Active Cycles         cycle    79,117.88
    Total L2 Elapsed Cycles          cycle    1,425,152
    Average SM Active Cycles         cycle    59,438.03
    Total SM Elapsed Cycles          cycle    3,795,340
    Average SMSP Active Cycles       cycle    95,696.93
    Total SMSP Elapsed Cycles        cycle   15,181,360
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       70,891
    Memory Throughput                 %        91.97
    DRAM Throughput                   %        91.97
    Duration                         us        31.04
    L1/TEX Cache Throughput           %        16.82
    L2 Cache Throughput               %        21.29
    SM Active Cycles              cycle    72,627.53
    Compute (SM) Throughput           %        23.26
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.23
    Achieved Active Warps Per SM           warp        33.71
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.77%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      256,396
    Total DRAM Elapsed Cycles        cycle    1,115,136
    Average L1 Active Cycles         cycle    72,627.53
    Total L1 Elapsed Cycles          cycle    2,332,406
    Average L2 Active Cycles         cycle    55,403.81
    Total L2 Elapsed Cycles          cycle      941,072
    Average SM Active Cycles         cycle    72,627.53
    Total SM Elapsed Cycles          cycle    2,332,406
    Average SMSP Active Cycles       cycle    64,014.04
    Total SMSP Elapsed Cycles        cycle    9,329,624
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      103,018
    Memory Throughput                 %        75.26
    DRAM Throughput                   %        75.26
    Duration                         us        45.12
    L1/TEX Cache Throughput           %        18.63
    L2 Cache Throughput               %        45.57
    SM Active Cycles              cycle    83,129.38
    Compute (SM) Throughput           %        31.83
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        89.26
    Achieved Active Warps Per SM           warp        42.84
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.74%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (89.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      304,792
    Total DRAM Elapsed Cycles        cycle    1,619,968
    Average L1 Active Cycles         cycle    83,129.38
    Total L1 Elapsed Cycles          cycle    3,308,224
    Average L2 Active Cycles         cycle    75,850.56
    Total L2 Elapsed Cycles          cycle    1,368,784
    Average SM Active Cycles         cycle    83,129.38
    Total SM Elapsed Cycles          cycle    3,308,224
    Average SMSP Active Cycles       cycle    91,497.67
    Total SMSP Elapsed Cycles        cycle   13,232,896
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       67,887
    Memory Throughput                 %        92.54
    DRAM Throughput                   %        92.54
    Duration                         us        29.76
    L1/TEX Cache Throughput           %        15.11
    L2 Cache Throughput               %        21.75
    SM Active Cycles              cycle    59,053.79
    Compute (SM) Throughput           %        20.89
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        85.91
    Achieved Active Warps Per SM           warp        41.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 14.09%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (85.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      247,328
    Total DRAM Elapsed Cycles        cycle    1,069,056
    Average L1 Active Cycles         cycle    59,053.79
    Total L1 Elapsed Cycles          cycle    2,596,016
    Average L2 Active Cycles         cycle       58,029
    Total L2 Elapsed Cycles          cycle      902,256
    Average SM Active Cycles         cycle    59,053.79
    Total SM Elapsed Cycles          cycle    2,596,016
    Average SMSP Active Cycles       cycle    66,912.11
    Total SMSP Elapsed Cycles        cycle   10,384,064
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.70
    SM Frequency                    Ghz         2.24
    Elapsed Cycles                cycle        3,305
    Memory Throughput                 %         1.06
    DRAM Throughput                   %         1.06
    Duration                         us         1.47
    L1/TEX Cache Throughput           %         3.55
    L2 Cache Throughput               %         0.61
    SM Active Cycles              cycle       337.91
    Compute (SM) Throughput           %         0.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.58
    Achieved Active Warps Per SM           warp         7.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.42%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (14.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          136
    Total DRAM Elapsed Cycles        cycle       51,200
    Average L1 Active Cycles         cycle       337.91
    Total L1 Elapsed Cycles          cycle      115,388
    Average L2 Active Cycles         cycle       392.19
    Total L2 Elapsed Cycles          cycle       43,616
    Average SM Active Cycles         cycle       337.91
    Total SM Elapsed Cycles          cycle      115,388
    Average SMSP Active Cycles       cycle       303.27
    Total SMSP Elapsed Cycles        cycle      461,552
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.059%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.89% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.544%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.23% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.059%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.89% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.44%                                                                                           
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 44.76% above the average, while the minimum instance value is 89.29% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       95,300
    Memory Throughput                 %        72.61
    DRAM Throughput                   %        72.61
    Duration                         us        41.82
    L1/TEX Cache Throughput           %        19.16
    L2 Cache Throughput               %        46.44
    SM Active Cycles              cycle    80,813.53
    Compute (SM) Throughput           %        25.37
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        85.55
    Achieved Active Warps Per SM           warp        41.06
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 14.45%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (85.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      272,856
    Total DRAM Elapsed Cycles        cycle    1,503,232
    Average L1 Active Cycles         cycle    80,813.53
    Total L1 Elapsed Cycles          cycle    4,149,992
    Average L2 Active Cycles         cycle    83,068.81
    Total L2 Elapsed Cycles          cycle    1,268,160
    Average SM Active Cycles         cycle    80,813.53
    Total SM Elapsed Cycles          cycle    4,149,992
    Average SMSP Active Cycles       cycle   100,995.82
    Total SMSP Elapsed Cycles        cycle   16,599,968
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       67,387
    Memory Throughput                 %        91.98
    DRAM Throughput                   %        91.98
    Duration                         us        29.57
    L1/TEX Cache Throughput           %        15.82
    L2 Cache Throughput               %        21.90
    SM Active Cycles              cycle    55,940.82
    Compute (SM) Throughput           %        21.88
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.23
    Achieved Active Warps Per SM           warp        41.87
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.77%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      243,944
    Total DRAM Elapsed Cycles        cycle    1,060,864
    Average L1 Active Cycles         cycle    55,940.82
    Total L1 Elapsed Cycles          cycle    2,478,718
    Average L2 Active Cycles         cycle    60,630.56
    Total L2 Elapsed Cycles          cycle      895,744
    Average SM Active Cycles         cycle    55,940.82
    Total SM Elapsed Cycles          cycle    2,478,718
    Average SMSP Active Cycles       cycle    69,714.90
    Total SMSP Elapsed Cycles        cycle    9,914,872
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.27
    Elapsed Cycles                cycle       93,753
    Memory Throughput                 %        72.15
    DRAM Throughput                   %        72.15
    Duration                         us        41.18
    L1/TEX Cache Throughput           %        16.66
    L2 Cache Throughput               %        47.36
    SM Active Cycles              cycle    92,975.88
    Compute (SM) Throughput           %        32.98
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.91
    Achieved Active Warps Per SM           warp        35.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.09%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      266,724
    Total DRAM Elapsed Cycles        cycle    1,478,656
    Average L1 Active Cycles         cycle    92,975.88
    Total L1 Elapsed Cycles          cycle    3,192,430
    Average L2 Active Cycles         cycle    76,091.75
    Total L2 Elapsed Cycles          cycle    1,248,144
    Average SM Active Cycles         cycle    92,975.88
    Total SM Elapsed Cycles          cycle    3,192,430
    Average SMSP Active Cycles       cycle    91,715.46
    Total SMSP Elapsed Cycles        cycle   12,769,720
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.27
    Elapsed Cycles                cycle       56,530
    Memory Throughput                 %        89.48
    DRAM Throughput                   %        89.48
    Duration                         us        24.90
    L1/TEX Cache Throughput           %        15.63
    L2 Cache Throughput               %        26.06
    SM Active Cycles              cycle    58,166.62
    Compute (SM) Throughput           %        21.60
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        67.62
    Achieved Active Warps Per SM           warp        32.46
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 32.38%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (67.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      199,860
    Total DRAM Elapsed Cycles        cycle      893,440
    Average L1 Active Cycles         cycle    58,166.62
    Total L1 Elapsed Cycles          cycle    2,510,454
    Average L2 Active Cycles         cycle       57,255
    Total L2 Elapsed Cycles          cycle      752,832
    Average SM Active Cycles         cycle    58,166.62
    Total SM Elapsed Cycles          cycle    2,510,454
    Average SMSP Active Cycles       cycle    66,892.81
    Total SMSP Elapsed Cycles        cycle   10,041,816
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.68
    SM Frequency                    Ghz         2.26
    Elapsed Cycles                cycle        3,400
    Memory Throughput                 %         1.87
    DRAM Throughput                   %         1.87
    Duration                         us         1.50
    L1/TEX Cache Throughput           %         3.50
    L2 Cache Throughput               %         0.56
    SM Active Cycles              cycle       343.32
    Compute (SM) Throughput           %         0.41
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.18
    Achieved Active Warps Per SM           warp         7.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.82%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          244
    Total DRAM Elapsed Cycles        cycle       52,224
    Average L1 Active Cycles         cycle       343.32
    Total L1 Elapsed Cycles          cycle      122,502
    Average L2 Active Cycles         cycle       401.25
    Total L2 Elapsed Cycles          cycle       44,848
    Average SM Active Cycles         cycle       343.32
    Total SM Elapsed Cycles          cycle      122,502
    Average SMSP Active Cycles       cycle       315.58
    Total SMSP Elapsed Cycles        cycle      490,008
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.752%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.86% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.396%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.03% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.752%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.86% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.894%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 48.16% above the average, while the minimum instance value is 89.53% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.27
    Elapsed Cycles                cycle       91,521
    Memory Throughput                 %        76.31
    DRAM Throughput                   %        76.31
    Duration                         us        40.19
    L1/TEX Cache Throughput           %        14.87
    L2 Cache Throughput               %        47.50
    SM Active Cycles              cycle   104,129.32
    Compute (SM) Throughput           %        29.46
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        63.33
    Achieved Active Warps Per SM           warp        30.40
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 36.67%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (63.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      275,256
    Total DRAM Elapsed Cycles        cycle    1,442,816
    Average L1 Active Cycles         cycle   104,129.32
    Total L1 Elapsed Cycles          cycle    3,572,992
    Average L2 Active Cycles         cycle    84,419.38
    Total L2 Elapsed Cycles          cycle    1,217,920
    Average SM Active Cycles         cycle   104,129.32
    Total SM Elapsed Cycles          cycle    3,572,992
    Average SMSP Active Cycles       cycle   101,871.80
    Total SMSP Elapsed Cycles        cycle   14,291,968
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       76,923
    Memory Throughput                 %        90.94
    DRAM Throughput                   %        90.94
    Duration                         us        33.63
    L1/TEX Cache Throughput           %        15.74
    L2 Cache Throughput               %        21.15
    SM Active Cycles              cycle    75,412.97
    Compute (SM) Throughput           %        21.75
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.64
    Achieved Active Warps Per SM           warp        34.39
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.36%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      274,720
    Total DRAM Elapsed Cycles        cycle    1,208,320
    Average L1 Active Cycles         cycle    75,412.97
    Total L1 Elapsed Cycles          cycle    2,492,494
    Average L2 Active Cycles         cycle    58,839.06
    Total L2 Elapsed Cycles          cycle    1,020,752
    Average SM Active Cycles         cycle    75,412.97
    Total SM Elapsed Cycles          cycle    2,492,494
    Average SMSP Active Cycles       cycle    68,647.95
    Total SMSP Elapsed Cycles        cycle    9,969,976
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.78
    SM Frequency                    Ghz         2.22
    Elapsed Cycles                cycle        3,622
    Memory Throughput                 %         1.94
    DRAM Throughput                   %         1.84
    Duration                         us         1.63
    L1/TEX Cache Throughput           %         3.60
    L2 Cache Throughput               %         1.94
    SM Active Cycles              cycle       332.97
    Compute (SM) Throughput           %         0.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.08
    Achieved Active Warps Per SM           warp         7.72
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.92%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (16.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          264
    Total DRAM Elapsed Cycles        cycle       57,344
    Average L1 Active Cycles         cycle       332.97
    Total L1 Elapsed Cycles          cycle      115,432
    Average L2 Active Cycles         cycle       389.69
    Total L2 Elapsed Cycles          cycle       47,808
    Average SM Active Cycles         cycle       332.97
    Total SM Elapsed Cycles          cycle      115,432
    Average SMSP Active Cycles       cycle       300.65
    Total SMSP Elapsed Cycles        cycle      461,728
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.953%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.89% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.487%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.25% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.953%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.89% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.802%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 59.83% above the average, while the minimum instance value is 89.22% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       96,313
    Memory Throughput                 %        75.38
    DRAM Throughput                   %        75.38
    Duration                         us        42.24
    L1/TEX Cache Throughput           %        15.79
    L2 Cache Throughput               %        46.85
    SM Active Cycles              cycle   103,513.82
    Compute (SM) Throughput           %        32.61
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        67.72
    Achieved Active Warps Per SM           warp        32.51
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 32.28%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (67.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      285,996
    Total DRAM Elapsed Cycles        cycle    1,517,568
    Average L1 Active Cycles         cycle   103,513.82
    Total L1 Elapsed Cycles          cycle    3,227,640
    Average L2 Active Cycles         cycle    77,201.94
    Total L2 Elapsed Cycles          cycle    1,280,848
    Average SM Active Cycles         cycle   103,513.82
    Total SM Elapsed Cycles          cycle    3,227,640
    Average SMSP Active Cycles       cycle    93,477.48
    Total SMSP Elapsed Cycles        cycle   12,910,560
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.96
    SM Frequency                    Ghz         2.26
    Elapsed Cycles                cycle       65,578
    Memory Throughput                 %        92.45
    DRAM Throughput                   %        92.45
    Duration                         us        28.96
    L1/TEX Cache Throughput           %        16.16
    L2 Cache Throughput               %        22.45
    SM Active Cycles              cycle    60,058.47
    Compute (SM) Throughput           %        22.34
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.27
    Achieved Active Warps Per SM           warp        38.05
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.73%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      239,976
    Total DRAM Elapsed Cycles        cycle    1,038,336
    Average L1 Active Cycles         cycle    60,058.47
    Total L1 Elapsed Cycles          cycle    2,426,742
    Average L2 Active Cycles         cycle    60,808.50
    Total L2 Elapsed Cycles          cycle      873,840
    Average SM Active Cycles         cycle    60,058.47
    Total SM Elapsed Cycles          cycle    2,426,742
    Average SMSP Active Cycles       cycle    70,265.79
    Total SMSP Elapsed Cycles        cycle    9,706,968
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.70
    SM Frequency                    Ghz         2.25
    Elapsed Cycles                cycle        4,101
    Memory Throughput                 %        12.17
    DRAM Throughput                   %        12.17
    Duration                         us         1.82
    L1/TEX Cache Throughput           %         3.54
    L2 Cache Throughput               %         1.07
    SM Active Cycles              cycle       339.21
    Compute (SM) Throughput           %         0.42
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.36
    Achieved Active Warps Per SM           warp         7.85
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.64%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (16.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        1,932
    Total DRAM Elapsed Cycles        cycle       63,488
    Average L1 Active Cycles         cycle       339.21
    Total L1 Elapsed Cycles          cycle      118,876
    Average L2 Active Cycles         cycle       421.50
    Total L2 Elapsed Cycles          cycle       54,080
    Average SM Active Cycles         cycle       339.21
    Total SM Elapsed Cycles          cycle      118,876
    Average SMSP Active Cycles       cycle       312.07
    Total SMSP Elapsed Cycles        cycle      475,504
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.874%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.86% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.524%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.10% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.874%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.86% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.545%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 68.52% above the average, while the minimum instance value is 90.04% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       95,903
    Memory Throughput                 %        72.08
    DRAM Throughput                   %        72.08
    Duration                         us        42.08
    L1/TEX Cache Throughput           %        18.27
    L2 Cache Throughput               %        47.22
    SM Active Cycles              cycle    84,760.47
    Compute (SM) Throughput           %        29.53
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        81.50
    Achieved Active Warps Per SM           warp        39.12
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 18.5%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (81.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      272,360
    Total DRAM Elapsed Cycles        cycle    1,511,424
    Average L1 Active Cycles         cycle    84,760.47
    Total L1 Elapsed Cycles          cycle    3,564,098
    Average L2 Active Cycles         cycle    82,582.56
    Total L2 Elapsed Cycles          cycle    1,275,776
    Average SM Active Cycles         cycle    84,760.47
    Total SM Elapsed Cycles          cycle    3,564,098
    Average SMSP Active Cycles       cycle   100,420.48
    Total SMSP Elapsed Cycles        cycle   14,256,392
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       69,766
    Memory Throughput                 %        80.86
    DRAM Throughput                   %        80.86
    Duration                         us        30.50
    L1/TEX Cache Throughput           %        15.77
    L2 Cache Throughput               %        23.32
    SM Active Cycles              cycle    76,183.50
    Compute (SM) Throughput           %        21.80
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        60.38
    Achieved Active Warps Per SM           warp        28.98
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 39.62%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (60.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      221,492
    Total DRAM Elapsed Cycles        cycle    1,095,680
    Average L1 Active Cycles         cycle    76,183.50
    Total L1 Elapsed Cycles          cycle    2,486,998
    Average L2 Active Cycles         cycle    59,776.94
    Total L2 Elapsed Cycles          cycle      925,232
    Average SM Active Cycles         cycle    76,183.50
    Total SM Elapsed Cycles          cycle    2,486,998
    Average SMSP Active Cycles       cycle    69,733.81
    Total SMSP Elapsed Cycles        cycle    9,947,992
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.73
    SM Frequency                    Ghz         2.24
    Elapsed Cycles                cycle        3,951
    Memory Throughput                 %         1.95
    DRAM Throughput                   %         1.95
    Duration                         us         1.76
    L1/TEX Cache Throughput           %         3.65
    L2 Cache Throughput               %         0.48
    SM Active Cycles              cycle       328.38
    Compute (SM) Throughput           %         0.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.98
    Achieved Active Warps Per SM           warp         7.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.02%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          300
    Total DRAM Elapsed Cycles        cycle       61,440
    Average L1 Active Cycles         cycle       328.38
    Total L1 Elapsed Cycles          cycle      114,398
    Average L2 Active Cycles         cycle       401.56
    Total L2 Elapsed Cycles          cycle       52,080
    Average SM Active Cycles         cycle       328.38
    Total SM Elapsed Cycles          cycle      114,398
    Average SMSP Active Cycles       cycle       292.80
    Total SMSP Elapsed Cycles        cycle      457,592
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.924%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.94% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.373%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.24% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.924%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.94% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.494%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 60.75% above the average, while the minimum instance value is 89.54% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       96,407
    Memory Throughput                 %        70.17
    DRAM Throughput                   %        70.17
    Duration                         us        42.30
    L1/TEX Cache Throughput           %        16.86
    L2 Cache Throughput               %        47.10
    SM Active Cycles              cycle    91,852.94
    Compute (SM) Throughput           %        32.38
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.90
    Achieved Active Warps Per SM           warp        35.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.1%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      266,572
    Total DRAM Elapsed Cycles        cycle    1,519,616
    Average L1 Active Cycles         cycle    91,852.94
    Total L1 Elapsed Cycles          cycle    3,249,708
    Average L2 Active Cycles         cycle    76,928.44
    Total L2 Elapsed Cycles          cycle    1,282,672
    Average SM Active Cycles         cycle    91,852.94
    Total SM Elapsed Cycles          cycle    3,249,708
    Average SMSP Active Cycles       cycle    92,784.76
    Total SMSP Elapsed Cycles        cycle   12,998,832
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       76,693
    Memory Throughput                 %        92.41
    DRAM Throughput                   %        92.41
    Duration                         us        33.57
    L1/TEX Cache Throughput           %        17.59
    L2 Cache Throughput               %        19.27
    SM Active Cycles              cycle    63,512.76
    Compute (SM) Throughput           %        24.31
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        83.73
    Achieved Active Warps Per SM           warp        40.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 16.27%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (83.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      278,448
    Total DRAM Elapsed Cycles        cycle    1,205,248
    Average L1 Active Cycles         cycle    63,512.76
    Total L1 Elapsed Cycles          cycle    2,229,376
    Average L2 Active Cycles         cycle    51,266.12
    Total L2 Elapsed Cycles          cycle    1,018,272
    Average SM Active Cycles         cycle    63,512.76
    Total SM Elapsed Cycles          cycle    2,229,376
    Average SMSP Active Cycles       cycle    61,199.92
    Total SMSP Elapsed Cycles        cycle    8,917,504
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       95,732
    Memory Throughput                 %        74.57
    DRAM Throughput                   %        74.57
    Duration                         us        41.98
    L1/TEX Cache Throughput           %        16.47
    L2 Cache Throughput               %        47.72
    SM Active Cycles              cycle    94,056.29
    Compute (SM) Throughput           %        32.98
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.24
    Achieved Active Warps Per SM           warp        35.15
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.76%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      281,396
    Total DRAM Elapsed Cycles        cycle    1,509,376
    Average L1 Active Cycles         cycle    94,056.29
    Total L1 Elapsed Cycles          cycle    3,190,302
    Average L2 Active Cycles         cycle    76,662.88
    Total L2 Elapsed Cycles          cycle    1,273,184
    Average SM Active Cycles         cycle    94,056.29
    Total SM Elapsed Cycles          cycle    3,190,302
    Average SMSP Active Cycles       cycle    92,460.44
    Total SMSP Elapsed Cycles        cycle   12,761,208
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       66,516
    Memory Throughput                 %        92.50
    DRAM Throughput                   %        92.50
    Duration                         us        29.18
    L1/TEX Cache Throughput           %        17.52
    L2 Cache Throughput               %        22.21
    SM Active Cycles              cycle    60,919.18
    Compute (SM) Throughput           %        24.20
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.43
    Achieved Active Warps Per SM           warp        37.64
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.57%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      242,240
    Total DRAM Elapsed Cycles        cycle    1,047,552
    Average L1 Active Cycles         cycle    60,919.18
    Total L1 Elapsed Cycles          cycle    2,239,452
    Average L2 Active Cycles         cycle    29,242.44
    Total L2 Elapsed Cycles          cycle      884,144
    Average SM Active Cycles         cycle    60,919.18
    Total SM Elapsed Cycles          cycle    2,239,452
    Average SMSP Active Cycles       cycle    35,317.97
    Total SMSP Elapsed Cycles        cycle    8,957,808
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle      107,485
    Memory Throughput                 %        76.34
    DRAM Throughput                   %        76.34
    Duration                         us        46.98
    L1/TEX Cache Throughput           %        25.37
    L2 Cache Throughput               %        46.65
    SM Active Cycles              cycle    61,041.62
    Compute (SM) Throughput           %        28.81
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       127.67
    Achieved Active Warps Per SM           warp        61.28
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      322,260
    Total DRAM Elapsed Cycles        cycle    1,688,576
    Average L1 Active Cycles         cycle    61,041.62
    Total L1 Elapsed Cycles          cycle    3,650,956
    Average L2 Active Cycles         cycle    76,796.81
    Total L2 Elapsed Cycles          cycle    1,427,424
    Average SM Active Cycles         cycle    61,041.62
    Total SM Elapsed Cycles          cycle    3,650,956
    Average SMSP Active Cycles       cycle    92,634.75
    Total SMSP Elapsed Cycles        cycle   14,603,824
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       67,206
    Memory Throughput                 %        91.50
    DRAM Throughput                   %        91.50
    Duration                         us        29.34
    L1/TEX Cache Throughput           %        17.23
    L2 Cache Throughput               %        22.03
    SM Active Cycles              cycle    63,424.85
    Compute (SM) Throughput           %        23.83
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.74
    Achieved Active Warps Per SM           warp        36.35
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.26%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      241,040
    Total DRAM Elapsed Cycles        cycle    1,053,696
    Average L1 Active Cycles         cycle    63,424.85
    Total L1 Elapsed Cycles          cycle    2,274,704
    Average L2 Active Cycles         cycle    53,500.75
    Total L2 Elapsed Cycles          cycle      890,944
    Average SM Active Cycles         cycle    63,424.85
    Total SM Elapsed Cycles          cycle    2,274,704
    Average SMSP Active Cycles       cycle    62,047.85
    Total SMSP Elapsed Cycles        cycle    9,098,816
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.26
    Elapsed Cycles                cycle       95,398
    Memory Throughput                 %        71.36
    DRAM Throughput                   %        71.36
    Duration                         us        42.11
    L1/TEX Cache Throughput           %        26.64
    L2 Cache Throughput               %        47.61
    SM Active Cycles              cycle    58,145.85
    Compute (SM) Throughput           %        31.95
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       118.10
    Achieved Active Warps Per SM           warp        56.69
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      270,020
    Total DRAM Elapsed Cycles        cycle    1,513,472
    Average L1 Active Cycles         cycle    58,145.85
    Total L1 Elapsed Cycles          cycle    3,291,836
    Average L2 Active Cycles         cycle    75,604.31
    Total L2 Elapsed Cycles          cycle    1,272,896
    Average SM Active Cycles         cycle    58,145.85
    Total SM Elapsed Cycles          cycle    3,291,836
    Average SMSP Active Cycles       cycle    91,146.97
    Total SMSP Elapsed Cycles        cycle   13,167,344
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.26
    Elapsed Cycles                cycle       67,046
    Memory Throughput                 %        91.73
    DRAM Throughput                   %        91.73
    Duration                         us        29.57
    L1/TEX Cache Throughput           %        22.06
    L2 Cache Throughput               %        21.99
    SM Active Cycles              cycle    34,948.41
    Compute (SM) Throughput           %        19.20
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       142.94
    Achieved Active Warps Per SM           warp        68.61
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      243,520
    Total DRAM Elapsed Cycles        cycle    1,061,888
    Average L1 Active Cycles         cycle    34,948.41
    Total L1 Elapsed Cycles          cycle    2,823,518
    Average L2 Active Cycles         cycle    61,212.81
    Total L2 Elapsed Cycles          cycle      892,976
    Average SM Active Cycles         cycle    34,948.41
    Total SM Elapsed Cycles          cycle    2,823,518
    Average SMSP Active Cycles       cycle    70,882.02
    Total SMSP Elapsed Cycles        cycle   11,294,072
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.26
    Elapsed Cycles                cycle       97,724
    Memory Throughput                 %        73.08
    DRAM Throughput                   %        73.08
    Duration                         us        43.10
    L1/TEX Cache Throughput           %        26.21
    L2 Cache Throughput               %        47.98
    SM Active Cycles              cycle    59,095.18
    Compute (SM) Throughput           %        32.38
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       120.33
    Achieved Active Warps Per SM           warp        57.76
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      283,064
    Total DRAM Elapsed Cycles        cycle    1,549,312
    Average L1 Active Cycles         cycle    59,095.18
    Total L1 Elapsed Cycles          cycle    3,248,754
    Average L2 Active Cycles         cycle    77,195.12
    Total L2 Elapsed Cycles          cycle    1,302,928
    Average SM Active Cycles         cycle    59,095.18
    Total SM Elapsed Cycles          cycle    3,248,754
    Average SMSP Active Cycles       cycle    92,807.55
    Total SMSP Elapsed Cycles        cycle   12,995,016
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       69,722
    Memory Throughput                 %        91.50
    DRAM Throughput                   %        91.50
    Duration                         us        30.56
    L1/TEX Cache Throughput           %        21.60
    L2 Cache Throughput               %        21.21
    SM Active Cycles              cycle    35,689.68
    Compute (SM) Throughput           %        21.81
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       141.03
    Achieved Active Warps Per SM           warp        67.70
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      251,104
    Total DRAM Elapsed Cycles        cycle    1,097,728
    Average L1 Active Cycles         cycle    35,689.68
    Total L1 Elapsed Cycles          cycle    2,486,758
    Average L2 Active Cycles         cycle    64,040.56
    Total L2 Elapsed Cycles          cycle      925,984
    Average SM Active Cycles         cycle    35,689.68
    Total SM Elapsed Cycles          cycle    2,486,758
    Average SMSP Active Cycles       cycle    74,997.73
    Total SMSP Elapsed Cycles        cycle    9,947,032
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.27
    Elapsed Cycles                cycle      108,579
    Memory Throughput                 %        75.11
    DRAM Throughput                   %        75.11
    Duration                         us        47.74
    L1/TEX Cache Throughput           %        26.24
    L2 Cache Throughput               %        44.56
    SM Active Cycles              cycle    59,025.65
    Compute (SM) Throughput           %        32.65
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       130.20
    Achieved Active Warps Per SM           warp        62.50
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      321,860
    Total DRAM Elapsed Cycles        cycle    1,714,176
    Average L1 Active Cycles         cycle    59,025.65
    Total L1 Elapsed Cycles          cycle    3,223,136
    Average L2 Active Cycles         cycle    77,350.62
    Total L2 Elapsed Cycles          cycle    1,446,208
    Average SM Active Cycles         cycle    59,025.65
    Total SM Elapsed Cycles          cycle    3,223,136
    Average SMSP Active Cycles       cycle    93,035.98
    Total SMSP Elapsed Cycles        cycle   12,892,544
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.27
    Elapsed Cycles                cycle       71,234
    Memory Throughput                 %        92.23
    DRAM Throughput                   %        92.23
    Duration                         us        31.39
    L1/TEX Cache Throughput           %        22.08
    L2 Cache Throughput               %        20.69
    SM Active Cycles              cycle    34,925.59
    Compute (SM) Throughput           %        22.20
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       146.02
    Achieved Active Warps Per SM           warp        70.09
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      259,948
    Total DRAM Elapsed Cycles        cycle    1,127,424
    Average L1 Active Cycles         cycle    34,925.59
    Total L1 Elapsed Cycles          cycle    2,442,348
    Average L2 Active Cycles         cycle    53,348.44
    Total L2 Elapsed Cycles          cycle      948,528
    Average SM Active Cycles         cycle    34,925.59
    Total SM Elapsed Cycles          cycle    2,442,348
    Average SMSP Active Cycles       cycle    61,642.60
    Total SMSP Elapsed Cycles        cycle    9,769,392
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.27
    Elapsed Cycles                cycle      104,192
    Memory Throughput                 %        76.61
    DRAM Throughput                   %        76.61
    Duration                         us        45.89
    L1/TEX Cache Throughput           %        25.69
    L2 Cache Throughput               %        46.31
    SM Active Cycles              cycle    60,281.24
    Compute (SM) Throughput           %        32.76
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       125.75
    Achieved Active Warps Per SM           warp        60.36
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      315,932
    Total DRAM Elapsed Cycles        cycle    1,649,664
    Average L1 Active Cycles         cycle    60,281.24
    Total L1 Elapsed Cycles          cycle    3,211,698
    Average L2 Active Cycles         cycle    70,642.94
    Total L2 Elapsed Cycles          cycle    1,388,720
    Average SM Active Cycles         cycle    60,281.24
    Total SM Elapsed Cycles          cycle    3,211,698
    Average SMSP Active Cycles       cycle    84,997.24
    Total SMSP Elapsed Cycles        cycle   12,846,792
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       65,966
    Memory Throughput                 %        92.39
    DRAM Throughput                   %        92.39
    Duration                         us        28.96
    L1/TEX Cache Throughput           %        15.60
    L2 Cache Throughput               %        22.49
    SM Active Cycles              cycle    61,119.12
    Compute (SM) Throughput           %        21.55
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.90
    Achieved Active Warps Per SM           warp        37.39
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.1%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      240,528
    Total DRAM Elapsed Cycles        cycle    1,041,408
    Average L1 Active Cycles         cycle    61,119.12
    Total L1 Elapsed Cycles          cycle    2,515,894
    Average L2 Active Cycles         cycle    29,092.44
    Total L2 Elapsed Cycles          cycle      877,216
    Average SM Active Cycles         cycle    61,119.12
    Total SM Elapsed Cycles          cycle    2,515,894
    Average SMSP Active Cycles       cycle    35,312.63
    Total SMSP Elapsed Cycles        cycle   10,063,576
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.50
    SM Frequency                    Ghz         2.21
    Elapsed Cycles                cycle        3,405
    Memory Throughput                 %         1.99
    DRAM Throughput                   %         1.99
    Duration                         us         1.54
    L1/TEX Cache Throughput           %         3.45
    L2 Cache Throughput               %         0.58
    SM Active Cycles              cycle       347.47
    Compute (SM) Throughput           %         0.43
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.69
    Achieved Active Warps Per SM           warp         7.05
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.31%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (14.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          260
    Total DRAM Elapsed Cycles        cycle       52,224
    Average L1 Active Cycles         cycle       347.47
    Total L1 Elapsed Cycles          cycle      116,280
    Average L2 Active Cycles         cycle       461.38
    Total L2 Elapsed Cycles          cycle       44,864
    Average SM Active Cycles         cycle       347.47
    Total SM Elapsed Cycles          cycle      116,280
    Average SMSP Active Cycles       cycle       312.74
    Total SMSP Elapsed Cycles        cycle      465,120
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.201%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.87% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.688%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.13% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.201%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.87% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.181%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 37.57% above the average, while the minimum instance value is 90.90% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       92,328
    Memory Throughput                 %        79.92
    DRAM Throughput                   %        79.92
    Duration                         us        40.48
    L1/TEX Cache Throughput           %        25.92
    L2 Cache Throughput               %        46.76
    SM Active Cycles              cycle    59,746.56
    Compute (SM) Throughput           %        29.63
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       110.68
    Achieved Active Warps Per SM           warp        53.13
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      290,728
    Total DRAM Elapsed Cycles        cycle    1,455,104
    Average L1 Active Cycles         cycle    59,746.56
    Total L1 Elapsed Cycles          cycle    3,550,934
    Average L2 Active Cycles         cycle    84,508.81
    Total L2 Elapsed Cycles          cycle    1,227,472
    Average SM Active Cycles         cycle    59,746.56
    Total SM Elapsed Cycles          cycle    3,550,934
    Average SMSP Active Cycles       cycle   102,239.47
    Total SMSP Elapsed Cycles        cycle   14,203,736
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       72,696
    Memory Throughput                 %        92.24
    DRAM Throughput                   %        92.24
    Duration                         us        31.84
    L1/TEX Cache Throughput           %        16.42
    L2 Cache Throughput               %        20.32
    SM Active Cycles              cycle    62,553.47
    Compute (SM) Throughput           %        22.71
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.55
    Achieved Active Warps Per SM           warp        39.63
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 17.45%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      263,516
    Total DRAM Elapsed Cycles        cycle    1,142,784
    Average L1 Active Cycles         cycle    62,553.47
    Total L1 Elapsed Cycles          cycle    2,387,576
    Average L2 Active Cycles         cycle    63,612.94
    Total L2 Elapsed Cycles          cycle      965,264
    Average SM Active Cycles         cycle    62,553.47
    Total SM Elapsed Cycles          cycle    2,387,576
    Average SMSP Active Cycles       cycle    74,371.88
    Total SMSP Elapsed Cycles        cycle    9,550,304
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.70
    SM Frequency                    Ghz         2.26
    Elapsed Cycles                cycle        3,328
    Memory Throughput                 %         2.44
    DRAM Throughput                   %         2.44
    Duration                         us         1.47
    L1/TEX Cache Throughput           %         3.57
    L2 Cache Throughput               %         0.59
    SM Active Cycles              cycle       336.29
    Compute (SM) Throughput           %         0.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.90
    Achieved Active Warps Per SM           warp         7.15
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.1%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (14.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          312
    Total DRAM Elapsed Cycles        cycle       51,200
    Average L1 Active Cycles         cycle       336.29
    Total L1 Elapsed Cycles          cycle      114,658
    Average L2 Active Cycles         cycle       413.94
    Total L2 Elapsed Cycles          cycle       43,936
    Average SM Active Cycles         cycle       336.29
    Total SM Elapsed Cycles          cycle      114,658
    Average SMSP Active Cycles       cycle       301.57
    Total SMSP Elapsed Cycles        cycle      458,632
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.071%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.91% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.545%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.19% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.071%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.91% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.143%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 60.65% above the average, while the minimum instance value is 89.85% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       99,332
    Memory Throughput                 %        74.86
    DRAM Throughput                   %        74.86
    Duration                         us        43.52
    L1/TEX Cache Throughput           %        25.80
    L2 Cache Throughput               %        47.60
    SM Active Cycles              cycle    60,029.50
    Compute (SM) Throughput           %        24.34
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       119.86
    Achieved Active Warps Per SM           warp        57.53
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      292,428
    Total DRAM Elapsed Cycles        cycle    1,562,624
    Average L1 Active Cycles         cycle    60,029.50
    Total L1 Elapsed Cycles          cycle    4,323,550
    Average L2 Active Cycles         cycle    85,143.19
    Total L2 Elapsed Cycles          cycle    1,319,088
    Average SM Active Cycles         cycle    60,029.50
    Total SM Elapsed Cycles          cycle    4,323,550
    Average SMSP Active Cycles       cycle   102,840.81
    Total SMSP Elapsed Cycles        cycle   17,294,200
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       72,719
    Memory Throughput                 %        91.44
    DRAM Throughput                   %        91.44
    Duration                         us        31.84
    L1/TEX Cache Throughput           %        15.74
    L2 Cache Throughput               %        20.45
    SM Active Cycles              cycle    72,811.71
    Compute (SM) Throughput           %        21.77
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.72
    Achieved Active Warps Per SM           warp        33.47
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 30.28%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      261,252
    Total DRAM Elapsed Cycles        cycle    1,142,784
    Average L1 Active Cycles         cycle    72,811.71
    Total L1 Elapsed Cycles          cycle    2,492,314
    Average L2 Active Cycles         cycle    56,345.19
    Total L2 Elapsed Cycles          cycle      965,744
    Average SM Active Cycles         cycle    72,811.71
    Total SM Elapsed Cycles          cycle    2,492,314
    Average SMSP Active Cycles       cycle    65,441.71
    Total SMSP Elapsed Cycles        cycle    9,969,256
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.30
    Elapsed Cycles                cycle      104,770
    Memory Throughput                 %        78.17
    DRAM Throughput                   %        78.17
    Duration                         us        45.60
    L1/TEX Cache Throughput           %        26.72
    L2 Cache Throughput               %        46.57
    SM Active Cycles              cycle    57,965.03
    Compute (SM) Throughput           %        31.60
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       132.42
    Achieved Active Warps Per SM           warp        63.56
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      319,976
    Total DRAM Elapsed Cycles        cycle    1,637,376
    Average L1 Active Cycles         cycle    57,965.03
    Total L1 Elapsed Cycles          cycle    3,331,768
    Average L2 Active Cycles         cycle    79,561.06
    Total L2 Elapsed Cycles          cycle    1,386,560
    Average SM Active Cycles         cycle    57,965.03
    Total SM Elapsed Cycles          cycle    3,331,768
    Average SMSP Active Cycles       cycle    97,109.56
    Total SMSP Elapsed Cycles        cycle   13,327,072
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       66,927
    Memory Throughput                 %        92.27
    DRAM Throughput                   %        92.27
    Duration                         us        29.38
    L1/TEX Cache Throughput           %        21.56
    L2 Cache Throughput               %        22.06
    SM Active Cycles              cycle    35,761.26
    Compute (SM) Throughput           %        23.47
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       135.06
    Achieved Active Warps Per SM           warp        64.83
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      243,296
    Total DRAM Elapsed Cycles        cycle    1,054,720
    Average L1 Active Cycles         cycle    35,761.26
    Total L1 Elapsed Cycles          cycle    2,311,150
    Average L2 Active Cycles         cycle    56,312.25
    Total L2 Elapsed Cycles          cycle      889,616
    Average SM Active Cycles         cycle    35,761.26
    Total SM Elapsed Cycles          cycle    2,311,150
    Average SMSP Active Cycles       cycle    65,447.67
    Total SMSP Elapsed Cycles        cycle    9,244,600
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.85
    SM Frequency                    Ghz         2.26
    Elapsed Cycles                cycle        3,394
    Memory Throughput                 %         1.74
    DRAM Throughput                   %         1.74
    Duration                         us         1.50
    L1/TEX Cache Throughput           %         3.57
    L2 Cache Throughput               %         0.57
    SM Active Cycles              cycle       336.44
    Compute (SM) Throughput           %         0.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.29
    Achieved Active Warps Per SM           warp         7.34
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.71%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          232
    Total DRAM Elapsed Cycles        cycle       53,248
    Average L1 Active Cycles         cycle       336.44
    Total L1 Elapsed Cycles          cycle      116,008
    Average L2 Active Cycles         cycle          423
    Total L2 Elapsed Cycles          cycle       44,800
    Average SM Active Cycles         cycle       336.44
    Total SM Elapsed Cycles          cycle      116,008
    Average SMSP Active Cycles       cycle       309.57
    Total SMSP Elapsed Cycles        cycle      464,032
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.988%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.87% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.639%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.17% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.988%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.87% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.956%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 46.05% above the average, while the minimum instance value is 90.07% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       97,429
    Memory Throughput                 %        77.45
    DRAM Throughput                   %        77.45
    Duration                         us        42.72
    L1/TEX Cache Throughput           %        17.26
    L2 Cache Throughput               %        47.93
    SM Active Cycles              cycle    89,753.94
    Compute (SM) Throughput           %        31.43
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.70
    Achieved Active Warps Per SM           warp        37.78
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.3%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      297,208
    Total DRAM Elapsed Cycles        cycle    1,534,976
    Average L1 Active Cycles         cycle    89,753.94
    Total L1 Elapsed Cycles          cycle    3,350,280
    Average L2 Active Cycles         cycle   100,888.75
    Total L2 Elapsed Cycles          cycle    1,296,144
    Average SM Active Cycles         cycle    89,753.94
    Total SM Elapsed Cycles          cycle    3,350,280
    Average SMSP Active Cycles       cycle   122,397.75
    Total SMSP Elapsed Cycles        cycle   13,401,120
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       73,264
    Memory Throughput                 %        92.06
    DRAM Throughput                   %        92.06
    Duration                         us        32.06
    L1/TEX Cache Throughput           %        21.86
    L2 Cache Throughput               %        20.20
    SM Active Cycles              cycle    35,271.91
    Compute (SM) Throughput           %        22.29
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       149.36
    Achieved Active Warps Per SM           warp        71.69
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      265,120
    Total DRAM Elapsed Cycles        cycle    1,152,000
    Average L1 Active Cycles         cycle    35,271.91
    Total L1 Elapsed Cycles          cycle    2,435,790
    Average L2 Active Cycles         cycle    63,239.69
    Total L2 Elapsed Cycles          cycle      972,640
    Average SM Active Cycles         cycle    35,271.91
    Total SM Elapsed Cycles          cycle    2,435,790
    Average SMSP Active Cycles       cycle   126,568.50
    Total SMSP Elapsed Cycles        cycle    9,743,160
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 38.95%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 37.44% above the average, while the minimum instance value is 8.98% below the       
          average.                                                                                                      

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.57
    SM Frequency                    Ghz         2.22
    Elapsed Cycles                cycle        3,984
    Memory Throughput                 %         1.95
    DRAM Throughput                   %         1.95
    Duration                         us         1.79
    L1/TEX Cache Throughput           %         3.65
    L2 Cache Throughput               %         0.48
    SM Active Cycles              cycle       328.76
    Compute (SM) Throughput           %         0.40
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.23
    Achieved Active Warps Per SM           warp         7.31
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.77%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          300
    Total DRAM Elapsed Cycles        cycle       61,440
    Average L1 Active Cycles         cycle       328.76
    Total L1 Elapsed Cycles          cycle      117,554
    Average L2 Active Cycles         cycle       347.62
    Total L2 Elapsed Cycles          cycle       52,592
    Average SM Active Cycles         cycle       328.76
    Total SM Elapsed Cycles          cycle      117,554
    Average SMSP Active Cycles       cycle       293.73
    Total SMSP Elapsed Cycles        cycle      470,216
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.75%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.98% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.229%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.32% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.75%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.98% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.758%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.90% above the average, while the minimum instance value is 87.92% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle      101,137
    Memory Throughput                 %        73.27
    DRAM Throughput                   %        73.27
    Duration                         us        44.10
    L1/TEX Cache Throughput           %        17.41
    L2 Cache Throughput               %        46.98
    SM Active Cycles              cycle    88,960.32
    Compute (SM) Throughput           %        32.00
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        81.76
    Achieved Active Warps Per SM           warp        39.25
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 18.24%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (81.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      290,372
    Total DRAM Elapsed Cycles        cycle    1,585,152
    Average L1 Active Cycles         cycle    88,960.32
    Total L1 Elapsed Cycles          cycle    3,292,308
    Average L2 Active Cycles         cycle    90,895.81
    Total L2 Elapsed Cycles          cycle    1,340,400
    Average SM Active Cycles         cycle    88,960.32
    Total SM Elapsed Cycles          cycle    3,292,308
    Average SMSP Active Cycles       cycle   113,201.71
    Total SMSP Elapsed Cycles        cycle   13,169,232
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       77,875
    Memory Throughput                 %        91.57
    DRAM Throughput                   %        91.57
    Duration                         us        34.02
    L1/TEX Cache Throughput           %        21.92
    L2 Cache Throughput               %        19.00
    SM Active Cycles              cycle    35,171.47
    Compute (SM) Throughput           %        23.79
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       153.43
    Achieved Active Warps Per SM           warp        73.65
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      279,676
    Total DRAM Elapsed Cycles        cycle    1,221,632
    Average L1 Active Cycles         cycle    35,171.47
    Total L1 Elapsed Cycles          cycle    2,283,306
    Average L2 Active Cycles         cycle    56,412.81
    Total L2 Elapsed Cycles          cycle    1,032,912
    Average SM Active Cycles         cycle    35,171.47
    Total SM Elapsed Cycles          cycle    2,283,306
    Average SMSP Active Cycles       cycle    65,517.88
    Total SMSP Elapsed Cycles        cycle    9,133,224
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       98,778
    Memory Throughput                 %        76.23
    DRAM Throughput                   %        76.23
    Duration                         us        43.33
    L1/TEX Cache Throughput           %        25.41
    L2 Cache Throughput               %        46.20
    SM Active Cycles              cycle    60,958.24
    Compute (SM) Throughput           %        32.49
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       117.73
    Achieved Active Warps Per SM           warp        56.51
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      296,612
    Total DRAM Elapsed Cycles        cycle    1,556,480
    Average L1 Active Cycles         cycle    60,958.24
    Total L1 Elapsed Cycles          cycle    3,246,334
    Average L2 Active Cycles         cycle    77,376.75
    Total L2 Elapsed Cycles          cycle    1,313,952
    Average SM Active Cycles         cycle    60,958.24
    Total SM Elapsed Cycles          cycle    3,246,334
    Average SMSP Active Cycles       cycle    91,757.88
    Total SMSP Elapsed Cycles        cycle   12,985,336
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       74,823
    Memory Throughput                 %        91.20
    DRAM Throughput                   %        91.20
    Duration                         us        32.70
    L1/TEX Cache Throughput           %        21.67
    L2 Cache Throughput               %        19.82
    SM Active Cycles              cycle    35,583.74
    Compute (SM) Throughput           %        21.54
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       148.06
    Achieved Active Warps Per SM           warp        71.07
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      267,796
    Total DRAM Elapsed Cycles        cycle    1,174,528
    Average L1 Active Cycles         cycle    35,583.74
    Total L1 Elapsed Cycles          cycle    2,523,382
    Average L2 Active Cycles         cycle    60,610.69
    Total L2 Elapsed Cycles          cycle      991,984
    Average SM Active Cycles         cycle    35,583.74
    Total SM Elapsed Cycles          cycle    2,523,382
    Average SMSP Active Cycles       cycle    70,571.07
    Total SMSP Elapsed Cycles        cycle   10,093,528
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.70
    SM Frequency                    Ghz         2.23
    Elapsed Cycles                cycle        4,071
    Memory Throughput                 %         2.12
    DRAM Throughput                   %         2.12
    Duration                         us         1.82
    L1/TEX Cache Throughput           %         3.54
    L2 Cache Throughput               %         0.48
    SM Active Cycles              cycle       339.35
    Compute (SM) Throughput           %         0.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.22
    Achieved Active Warps Per SM           warp         7.78
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.78%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (16.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          336
    Total DRAM Elapsed Cycles        cycle       63,488
    Average L1 Active Cycles         cycle       339.35
    Total L1 Elapsed Cycles          cycle      115,620
    Average L2 Active Cycles         cycle       418.31
    Total L2 Elapsed Cycles          cycle       53,712
    Average SM Active Cycles         cycle       339.35
    Total SM Elapsed Cycles          cycle      115,620
    Average SMSP Active Cycles       cycle       310.39
    Total SMSP Elapsed Cycles        cycle      462,480
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.075%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.90% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.677%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.15% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.075%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.90% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.321%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 50.73% above the average, while the minimum instance value is 89.96% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      100,020
    Memory Throughput                 %        74.62
    DRAM Throughput                   %        74.62
    Duration                         us        43.87
    L1/TEX Cache Throughput           %        25.97
    L2 Cache Throughput               %        46.75
    SM Active Cycles              cycle    59,638.65
    Compute (SM) Throughput           %        32.02
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       119.85
    Achieved Active Warps Per SM           warp        57.53
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      294,000
    Total DRAM Elapsed Cycles        cycle    1,575,936
    Average L1 Active Cycles         cycle    59,638.65
    Total L1 Elapsed Cycles          cycle    3,296,366
    Average L2 Active Cycles         cycle       78,282
    Total L2 Elapsed Cycles          cycle    1,330,320
    Average SM Active Cycles         cycle    59,638.65
    Total SM Elapsed Cycles          cycle    3,296,366
    Average SMSP Active Cycles       cycle    94,592.29
    Total SMSP Elapsed Cycles        cycle   13,185,464
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       69,425
    Memory Throughput                 %        92.42
    DRAM Throughput                   %        92.42
    Duration                         us        30.46
    L1/TEX Cache Throughput           %        16.99
    L2 Cache Throughput               %        21.29
    SM Active Cycles              cycle    64,395.74
    Compute (SM) Throughput           %        23.56
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.57
    Achieved Active Warps Per SM           warp        37.71
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.43%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      252,684
    Total DRAM Elapsed Cycles        cycle    1,093,632
    Average L1 Active Cycles         cycle    64,395.74
    Total L1 Elapsed Cycles          cycle    2,309,112
    Average L2 Active Cycles         cycle    54,865.31
    Total L2 Elapsed Cycles          cycle      922,368
    Average SM Active Cycles         cycle    64,395.74
    Total SM Elapsed Cycles          cycle    2,309,112
    Average SMSP Active Cycles       cycle    63,655.24
    Total SMSP Elapsed Cycles        cycle    9,236,448
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      100,365
    Memory Throughput                 %        75.15
    DRAM Throughput                   %        75.15
    Duration                         us           44
    L1/TEX Cache Throughput           %        15.71
    L2 Cache Throughput               %        46.87
    SM Active Cycles              cycle   103,157.03
    Compute (SM) Throughput           %        32.60
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.76
    Achieved Active Warps Per SM           warp        33.48
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 30.24%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      297,060
    Total DRAM Elapsed Cycles        cycle    1,581,056
    Average L1 Active Cycles         cycle   103,157.03
    Total L1 Elapsed Cycles          cycle    3,240,862
    Average L2 Active Cycles         cycle    79,124.19
    Total L2 Elapsed Cycles          cycle    1,334,656
    Average SM Active Cycles         cycle   103,157.03
    Total SM Elapsed Cycles          cycle    3,240,862
    Average SMSP Active Cycles       cycle    95,463.07
    Total SMSP Elapsed Cycles        cycle   12,963,448
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       69,619
    Memory Throughput                 %        91.66
    DRAM Throughput                   %        91.66
    Duration                         us        30.50
    L1/TEX Cache Throughput           %        16.87
    L2 Cache Throughput               %        21.22
    SM Active Cycles              cycle    64,565.68
    Compute (SM) Throughput           %        23.41
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.47
    Achieved Active Warps Per SM           warp        35.75
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.53%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      251,080
    Total DRAM Elapsed Cycles        cycle    1,095,680
    Average L1 Active Cycles         cycle    64,565.68
    Total L1 Elapsed Cycles          cycle    2,325,636
    Average L2 Active Cycles         cycle    55,251.12
    Total L2 Elapsed Cycles          cycle      924,848
    Average SM Active Cycles         cycle    64,565.68
    Total SM Elapsed Cycles          cycle    2,325,636
    Average SMSP Active Cycles       cycle    63,829.07
    Total SMSP Elapsed Cycles        cycle    9,302,544
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       97,469
    Memory Throughput                 %        76.39
    DRAM Throughput                   %        76.39
    Duration                         us        42.72
    L1/TEX Cache Throughput           %        25.78
    L2 Cache Throughput               %        47.26
    SM Active Cycles              cycle    60,087.29
    Compute (SM) Throughput           %        31.27
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       117.94
    Achieved Active Warps Per SM           warp        56.61
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      293,320
    Total DRAM Elapsed Cycles        cycle    1,536,000
    Average L1 Active Cycles         cycle    60,087.29
    Total L1 Elapsed Cycles          cycle    3,381,314
    Average L2 Active Cycles         cycle    79,221.75
    Total L2 Elapsed Cycles          cycle    1,296,512
    Average SM Active Cycles         cycle    60,087.29
    Total SM Elapsed Cycles          cycle    3,381,314
    Average SMSP Active Cycles       cycle    95,742.75
    Total SMSP Elapsed Cycles        cycle   13,525,256
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       68,297
    Memory Throughput                 %        92.59
    DRAM Throughput                   %        92.59
    Duration                         us        29.95
    L1/TEX Cache Throughput           %        21.75
    L2 Cache Throughput               %        21.62
    SM Active Cycles              cycle    35,448.97
    Compute (SM) Throughput           %        17.69
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       140.11
    Achieved Active Warps Per SM           warp        67.25
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      248,876
    Total DRAM Elapsed Cycles        cycle    1,075,200
    Average L1 Active Cycles         cycle    35,448.97
    Total L1 Elapsed Cycles          cycle    3,080,610
    Average L2 Active Cycles         cycle    65,470.19
    Total L2 Elapsed Cycles          cycle      907,808
    Average SM Active Cycles         cycle    35,448.97
    Total SM Elapsed Cycles          cycle    3,080,610
    Average SMSP Active Cycles       cycle    75,865.88
    Total SMSP Elapsed Cycles        cycle   12,322,440
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      100,573
    Memory Throughput                 %        74.47
    DRAM Throughput                   %        74.47
    Duration                         us        44.10
    L1/TEX Cache Throughput           %        16.55
    L2 Cache Throughput               %        44.99
    SM Active Cycles              cycle       93,577
    Compute (SM) Throughput           %        27.80
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.56
    Achieved Active Warps Per SM           warp        36.75
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.44%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      294,912
    Total DRAM Elapsed Cycles        cycle    1,584,128
    Average L1 Active Cycles         cycle       93,577
    Total L1 Elapsed Cycles          cycle    3,809,054
    Average L2 Active Cycles         cycle    90,891.62
    Total L2 Elapsed Cycles          cycle    1,337,184
    Average SM Active Cycles         cycle       93,577
    Total SM Elapsed Cycles          cycle    3,809,054
    Average SMSP Active Cycles       cycle   109,959.28
    Total SMSP Elapsed Cycles        cycle   15,236,216
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       68,640
    Memory Throughput                 %        91.89
    DRAM Throughput                   %        91.89
    Duration                         us        30.11
    L1/TEX Cache Throughput           %        21.77
    L2 Cache Throughput               %        21.53
    SM Active Cycles              cycle    35,412.76
    Compute (SM) Throughput           %        23.48
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       138.49
    Achieved Active Warps Per SM           warp        66.48
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      248,404
    Total DRAM Elapsed Cycles        cycle    1,081,344
    Average L1 Active Cycles         cycle    35,412.76
    Total L1 Elapsed Cycles          cycle    2,321,880
    Average L2 Active Cycles         cycle    29,219.69
    Total L2 Elapsed Cycles          cycle      911,776
    Average SM Active Cycles         cycle    35,412.76
    Total SM Elapsed Cycles          cycle    2,321,880
    Average SMSP Active Cycles       cycle    35,329.45
    Total SMSP Elapsed Cycles        cycle    9,287,520
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      108,522
    Memory Throughput                 %        75.78
    DRAM Throughput                   %        75.78
    Duration                         us        47.46
    L1/TEX Cache Throughput           %        26.12
    L2 Cache Throughput               %        45.83
    SM Active Cycles              cycle    59,283.56
    Compute (SM) Throughput           %        25.15
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       132.42
    Achieved Active Warps Per SM           warp        63.56
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      323,100
    Total DRAM Elapsed Cycles        cycle    1,705,472
    Average L1 Active Cycles         cycle    59,283.56
    Total L1 Elapsed Cycles          cycle    4,211,736
    Average L2 Active Cycles         cycle    80,502.81
    Total L2 Elapsed Cycles          cycle    1,440,880
    Average SM Active Cycles         cycle    59,283.56
    Total SM Elapsed Cycles          cycle    4,211,736
    Average SMSP Active Cycles       cycle    98,009.83
    Total SMSP Elapsed Cycles        cycle   16,846,944
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.27
    Elapsed Cycles                cycle       62,290
    Memory Throughput                 %        92.77
    DRAM Throughput                   %        92.77
    Duration                         us        27.36
    L1/TEX Cache Throughput           %        21.86
    L2 Cache Throughput               %        23.70
    SM Active Cycles              cycle    35,276.71
    Compute (SM) Throughput           %        21.52
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       126.40
    Achieved Active Warps Per SM           warp        60.67
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      227,744
    Total DRAM Elapsed Cycles        cycle      982,016
    Average L1 Active Cycles         cycle    35,276.71
    Total L1 Elapsed Cycles          cycle    2,535,572
    Average L2 Active Cycles         cycle    57,402.25
    Total L2 Elapsed Cycles          cycle      827,824
    Average SM Active Cycles         cycle    35,276.71
    Total SM Elapsed Cycles          cycle    2,535,572
    Average SMSP Active Cycles       cycle    66,768.91
    Total SMSP Elapsed Cycles        cycle   10,142,288
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.87
    SM Frequency                    Ghz         2.27
    Elapsed Cycles                cycle        3,337
    Memory Throughput                 %         1.01
    DRAM Throughput                   %         1.01
    Duration                         us         1.47
    L1/TEX Cache Throughput           %         3.61
    L2 Cache Throughput               %         0.81
    SM Active Cycles              cycle       332.68
    Compute (SM) Throughput           %         0.42
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.16
    Achieved Active Warps Per SM           warp         7.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          132
    Total DRAM Elapsed Cycles        cycle       52,224
    Average L1 Active Cycles         cycle       332.68
    Total L1 Elapsed Cycles          cycle      119,602
    Average L2 Active Cycles         cycle     1,124.31
    Total L2 Elapsed Cycles          cycle       44,000
    Average SM Active Cycles         cycle       332.68
    Total SM Elapsed Cycles          cycle      119,602
    Average SMSP Active Cycles       cycle       301.48
    Total SMSP Elapsed Cycles        cycle      478,408
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.702%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.87% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.272%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.18% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.702%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.87% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.84%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 26.52% above the average, while the minimum instance value is 24.49% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       93,376
    Memory Throughput                 %        79.73
    DRAM Throughput                   %        79.73
    Duration                         us        40.93
    L1/TEX Cache Throughput           %        25.59
    L2 Cache Throughput               %        46.94
    SM Active Cycles              cycle    60,516.32
    Compute (SM) Throughput           %        25.41
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       109.98
    Achieved Active Warps Per SM           warp        52.79
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      293,116
    Total DRAM Elapsed Cycles        cycle    1,470,464
    Average L1 Active Cycles         cycle    60,516.32
    Total L1 Elapsed Cycles          cycle    4,173,066
    Average L2 Active Cycles         cycle    80,908.12
    Total L2 Elapsed Cycles          cycle    1,241,392
    Average SM Active Cycles         cycle    60,516.32
    Total SM Elapsed Cycles          cycle    4,173,066
    Average SMSP Active Cycles       cycle    98,034.21
    Total SMSP Elapsed Cycles        cycle   16,692,264
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       72,408
    Memory Throughput                 %        80.38
    DRAM Throughput                   %        80.38
    Duration                         us        31.62
    L1/TEX Cache Throughput           %        21.84
    L2 Cache Throughput               %        26.97
    SM Active Cycles              cycle    35,303.85
    Compute (SM) Throughput           %        22.86
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       135.02
    Achieved Active Warps Per SM           warp        64.81
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      228,188
    Total DRAM Elapsed Cycles        cycle    1,135,616
    Average L1 Active Cycles         cycle    35,303.85
    Total L1 Elapsed Cycles          cycle    2,389,092
    Average L2 Active Cycles         cycle    60,049.69
    Total L2 Elapsed Cycles          cycle      959,392
    Average SM Active Cycles         cycle    35,303.85
    Total SM Elapsed Cycles          cycle    2,389,092
    Average SMSP Active Cycles       cycle    70,316.32
    Total SMSP Elapsed Cycles        cycle    9,556,368
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      100,436
    Memory Throughput                 %        74.60
    DRAM Throughput                   %        74.60
    Duration                         us           44
    L1/TEX Cache Throughput           %        16.74
    L2 Cache Throughput               %        46.14
    SM Active Cycles              cycle    92,497.35
    Compute (SM) Throughput           %        30.97
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.49
    Achieved Active Warps Per SM           warp        37.20
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.51%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      294,860
    Total DRAM Elapsed Cycles        cycle    1,581,056
    Average L1 Active Cycles         cycle    92,497.35
    Total L1 Elapsed Cycles          cycle    3,428,166
    Average L2 Active Cycles         cycle    80,591.56
    Total L2 Elapsed Cycles          cycle    1,335,616
    Average SM Active Cycles         cycle    92,497.35
    Total SM Elapsed Cycles          cycle    3,428,166
    Average SMSP Active Cycles       cycle    97,476.67
    Total SMSP Elapsed Cycles        cycle   13,712,664
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.26
    Elapsed Cycles                cycle       69,335
    Memory Throughput                 %        92.29
    DRAM Throughput                   %        92.29
    Duration                         us        30.59
    L1/TEX Cache Throughput           %        16.80
    L2 Cache Throughput               %        21.24
    SM Active Cycles              cycle    67,684.76
    Compute (SM) Throughput           %        23.42
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.07
    Achieved Active Warps Per SM           warp        35.55
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.93%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      253,272
    Total DRAM Elapsed Cycles        cycle    1,097,728
    Average L1 Active Cycles         cycle    67,684.76
    Total L1 Elapsed Cycles          cycle    2,334,936
    Average L2 Active Cycles         cycle    58,447.31
    Total L2 Elapsed Cycles          cycle      924,176
    Average SM Active Cycles         cycle    67,684.76
    Total SM Elapsed Cycles          cycle    2,334,936
    Average SMSP Active Cycles       cycle    67,965.48
    Total SMSP Elapsed Cycles        cycle    9,339,744
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.27
    Elapsed Cycles                cycle      100,291
    Memory Throughput                 %        72.98
    DRAM Throughput                   %        72.98
    Duration                         us        44.22
    L1/TEX Cache Throughput           %        26.66
    L2 Cache Throughput               %        46.49
    SM Active Cycles              cycle    58,095.50
    Compute (SM) Throughput           %        30.72
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       124.10
    Achieved Active Warps Per SM           warp        59.57
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      289,784
    Total DRAM Elapsed Cycles        cycle    1,588,224
    Average L1 Active Cycles         cycle    58,095.50
    Total L1 Elapsed Cycles          cycle    3,462,458
    Average L2 Active Cycles         cycle    78,327.19
    Total L2 Elapsed Cycles          cycle    1,336,848
    Average SM Active Cycles         cycle    58,095.50
    Total SM Elapsed Cycles          cycle    3,462,458
    Average SMSP Active Cycles       cycle    94,334.50
    Total SMSP Elapsed Cycles        cycle   13,849,832
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       72,940
    Memory Throughput                 %        91.42
    DRAM Throughput                   %        91.42
    Duration                         us        31.97
    L1/TEX Cache Throughput           %        15.24
    L2 Cache Throughput               %        20.32
    SM Active Cycles              cycle    75,834.97
    Compute (SM) Throughput           %        21.26
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        65.90
    Achieved Active Warps Per SM           warp        31.63
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 34.1%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (65.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      262,120
    Total DRAM Elapsed Cycles        cycle    1,146,880
    Average L1 Active Cycles         cycle    75,834.97
    Total L1 Elapsed Cycles          cycle    2,573,352
    Average L2 Active Cycles         cycle    60,382.38
    Total L2 Elapsed Cycles          cycle      968,832
    Average SM Active Cycles         cycle    75,834.97
    Total SM Elapsed Cycles          cycle    2,573,352
    Average SMSP Active Cycles       cycle    69,607.26
    Total SMSP Elapsed Cycles        cycle   10,293,408
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.68
    SM Frequency                    Ghz         2.25
    Elapsed Cycles                cycle        3,393
    Memory Throughput                 %         1.93
    DRAM Throughput                   %         1.93
    Duration                         us         1.50
    L1/TEX Cache Throughput           %         3.48
    L2 Cache Throughput               %         0.57
    SM Active Cycles              cycle       344.91
    Compute (SM) Throughput           %         0.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.09
    Achieved Active Warps Per SM           warp         7.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.91%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          252
    Total DRAM Elapsed Cycles        cycle       52,224
    Average L1 Active Cycles         cycle       344.91
    Total L1 Elapsed Cycles          cycle      114,564
    Average L2 Active Cycles         cycle       377.50
    Total L2 Elapsed Cycles          cycle       44,736
    Average SM Active Cycles         cycle       344.91
    Total SM Elapsed Cycles          cycle      114,564
    Average SMSP Active Cycles       cycle       307.31
    Total SMSP Elapsed Cycles        cycle      458,256
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.257%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.89% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.67%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.14% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.257%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.89% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.058%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 52.28% above the average, while the minimum instance value is 88.87% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       95,342
    Memory Throughput                 %        79.46
    DRAM Throughput                   %        79.46
    Duration                         us        41.76
    L1/TEX Cache Throughput           %        25.83
    L2 Cache Throughput               %        47.47
    SM Active Cycles              cycle    59,951.85
    Compute (SM) Throughput           %        30.61
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       114.25
    Achieved Active Warps Per SM           warp        54.84
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      298,228
    Total DRAM Elapsed Cycles        cycle    1,501,184
    Average L1 Active Cycles         cycle    59,951.85
    Total L1 Elapsed Cycles          cycle    3,477,538
    Average L2 Active Cycles         cycle    79,623.19
    Total L2 Elapsed Cycles          cycle    1,267,488
    Average SM Active Cycles         cycle    59,951.85
    Total SM Elapsed Cycles          cycle    3,477,538
    Average SMSP Active Cycles       cycle    96,360.96
    Total SMSP Elapsed Cycles        cycle   13,910,152
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       70,449
    Memory Throughput                 %        92.41
    DRAM Throughput                   %        92.41
    Duration                         us        30.88
    L1/TEX Cache Throughput           %        17.27
    L2 Cache Throughput               %        20.98
    SM Active Cycles              cycle    63,603.38
    Compute (SM) Throughput           %        24.12
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.09
    Achieved Active Warps Per SM           warp        37.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.91%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      255,964
    Total DRAM Elapsed Cycles        cycle    1,107,968
    Average L1 Active Cycles         cycle    63,603.38
    Total L1 Elapsed Cycles          cycle    2,270,618
    Average L2 Active Cycles         cycle    58,707.38
    Total L2 Elapsed Cycles          cycle      935,168
    Average SM Active Cycles         cycle    63,603.38
    Total SM Elapsed Cycles          cycle    2,270,618
    Average SMSP Active Cycles       cycle    67,968.46
    Total SMSP Elapsed Cycles        cycle    9,082,472
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle      111,806
    Memory Throughput                 %        78.02
    DRAM Throughput                   %        78.02
    Duration                         us        48.83
    L1/TEX Cache Throughput           %        15.59
    L2 Cache Throughput               %        46.78
    SM Active Cycles              cycle   108,280.59
    Compute (SM) Throughput           %        32.60
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.80
    Achieved Active Warps Per SM           warp        35.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.2%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      342,344
    Total DRAM Elapsed Cycles        cycle    1,755,136
    Average L1 Active Cycles         cycle   108,280.59
    Total L1 Elapsed Cycles          cycle    3,269,824
    Average L2 Active Cycles         cycle    76,301.75
    Total L2 Elapsed Cycles          cycle    1,484,032
    Average SM Active Cycles         cycle   108,280.59
    Total SM Elapsed Cycles          cycle    3,269,824
    Average SMSP Active Cycles       cycle    92,590.54
    Total SMSP Elapsed Cycles        cycle   13,079,296
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       69,728
    Memory Throughput                 %        92.29
    DRAM Throughput                   %        92.29
    Duration                         us        30.56
    L1/TEX Cache Throughput           %        14.77
    L2 Cache Throughput               %        21.19
    SM Active Cycles              cycle    66,145.26
    Compute (SM) Throughput           %        20.65
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.94
    Achieved Active Warps Per SM           warp        36.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.06%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      253,028
    Total DRAM Elapsed Cycles        cycle    1,096,704
    Average L1 Active Cycles         cycle    66,145.26
    Total L1 Elapsed Cycles          cycle    2,655,124
    Average L2 Active Cycles         cycle    59,710.88
    Total L2 Elapsed Cycles          cycle      926,192
    Average SM Active Cycles         cycle    66,145.26
    Total SM Elapsed Cycles          cycle    2,655,124
    Average SMSP Active Cycles       cycle    69,480.79
    Total SMSP Elapsed Cycles        cycle   10,620,496
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.67
    SM Frequency                    Ghz         2.21
    Elapsed Cycles                cycle        3,391
    Memory Throughput                 %         1.98
    DRAM Throughput                   %         1.98
    Duration                         us         1.54
    L1/TEX Cache Throughput           %         3.59
    L2 Cache Throughput               %         0.58
    SM Active Cycles              cycle          334
    Compute (SM) Throughput           %         0.41
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.11
    Achieved Active Warps Per SM           warp         7.25
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.89%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          264
    Total DRAM Elapsed Cycles        cycle       53,248
    Average L1 Active Cycles         cycle          334
    Total L1 Elapsed Cycles          cycle      118,644
    Average L2 Active Cycles         cycle       390.12
    Total L2 Elapsed Cycles          cycle       44,752
    Average SM Active Cycles         cycle          334
    Total SM Elapsed Cycles          cycle      118,644
    Average SMSP Active Cycles       cycle       303.74
    Total SMSP Elapsed Cycles        cycle      474,576
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.787%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.91% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.363%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.10% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.787%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.91% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.348%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 45.51% above the average, while the minimum instance value is 89.23% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      106,910
    Memory Throughput                 %        77.68
    DRAM Throughput                   %        77.68
    Duration                         us        46.78
    L1/TEX Cache Throughput           %        15.65
    L2 Cache Throughput               %        46.30
    SM Active Cycles              cycle    98,935.09
    Compute (SM) Throughput           %        27.71
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.69
    Achieved Active Warps Per SM           warp        37.77
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.31%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      326,136
    Total DRAM Elapsed Cycles        cycle    1,679,360
    Average L1 Active Cycles         cycle    98,935.09
    Total L1 Elapsed Cycles          cycle    3,851,450
    Average L2 Active Cycles         cycle    80,230.62
    Total L2 Elapsed Cycles          cycle    1,420,416
    Average SM Active Cycles         cycle    98,935.09
    Total SM Elapsed Cycles          cycle    3,851,450
    Average SMSP Active Cycles       cycle    96,833.85
    Total SMSP Elapsed Cycles        cycle   15,405,800
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       83,435
    Memory Throughput                 %        91.45
    DRAM Throughput                   %        91.45
    Duration                         us        36.42
    L1/TEX Cache Throughput           %        14.31
    L2 Cache Throughput               %        20.89
    SM Active Cycles              cycle    62,229.15
    Compute (SM) Throughput           %        20.03
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        91.04
    Achieved Active Warps Per SM           warp        43.70
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      299,196
    Total DRAM Elapsed Cycles        cycle    1,308,672
    Average L1 Active Cycles         cycle    62,229.15
    Total L1 Elapsed Cycles          cycle    2,739,738
    Average L2 Active Cycles         cycle    59,588.88
    Total L2 Elapsed Cycles          cycle    1,106,256
    Average SM Active Cycles         cycle    62,229.15
    Total SM Elapsed Cycles          cycle    2,739,738
    Average SMSP Active Cycles       cycle    69,438.22
    Total SMSP Elapsed Cycles        cycle   10,958,952
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      100,578
    Memory Throughput                 %        73.96
    DRAM Throughput                   %        73.96
    Duration                         us        44.10
    L1/TEX Cache Throughput           %        25.47
    L2 Cache Throughput               %        46.23
    SM Active Cycles              cycle    60,817.53
    Compute (SM) Throughput           %        31.61
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       119.20
    Achieved Active Warps Per SM           warp        57.22
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      292,912
    Total DRAM Elapsed Cycles        cycle    1,584,128
    Average L1 Active Cycles         cycle    60,817.53
    Total L1 Elapsed Cycles          cycle    3,379,632
    Average L2 Active Cycles         cycle       82,129
    Total L2 Elapsed Cycles          cycle    1,337,600
    Average SM Active Cycles         cycle    60,817.53
    Total SM Elapsed Cycles          cycle    3,379,632
    Average SMSP Active Cycles       cycle    99,694.27
    Total SMSP Elapsed Cycles        cycle   13,518,528
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       67,689
    Memory Throughput                 %        91.92
    DRAM Throughput                   %        91.92
    Duration                         us        29.70
    L1/TEX Cache Throughput           %        16.99
    L2 Cache Throughput               %        21.81
    SM Active Cycles              cycle    59,696.97
    Compute (SM) Throughput           %        23.80
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        81.66
    Achieved Active Warps Per SM           warp        39.20
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 18.34%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (81.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      244,972
    Total DRAM Elapsed Cycles        cycle    1,065,984
    Average L1 Active Cycles         cycle    59,696.97
    Total L1 Elapsed Cycles          cycle    2,307,998
    Average L2 Active Cycles         cycle    60,503.56
    Total L2 Elapsed Cycles          cycle      899,552
    Average SM Active Cycles         cycle    59,696.97
    Total SM Elapsed Cycles          cycle    2,307,998
    Average SMSP Active Cycles       cycle    70,499.41
    Total SMSP Elapsed Cycles        cycle    9,231,992
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.73
    SM Frequency                    Ghz         2.25
    Elapsed Cycles                cycle        3,960
    Memory Throughput                 %         1.82
    DRAM Throughput                   %         1.82
    Duration                         us         1.76
    L1/TEX Cache Throughput           %         3.48
    L2 Cache Throughput               %         0.49
    SM Active Cycles              cycle          345
    Compute (SM) Throughput           %         0.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.75
    Achieved Active Warps Per SM           warp         7.08
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.25%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (14.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          280
    Total DRAM Elapsed Cycles        cycle       61,440
    Average L1 Active Cycles         cycle          345
    Total L1 Elapsed Cycles          cycle      114,794
    Average L2 Active Cycles         cycle       409.69
    Total L2 Elapsed Cycles          cycle       52,240
    Average SM Active Cycles         cycle          345
    Total SM Elapsed Cycles          cycle      114,794
    Average SMSP Active Cycles       cycle       311.18
    Total SMSP Elapsed Cycles        cycle      459,176
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.241%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.86% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.742%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.15% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.241%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.86% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.612%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 52.69% above the average, while the minimum instance value is 89.75% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       99,656
    Memory Throughput                 %        74.17
    DRAM Throughput                   %        74.17
    Duration                         us        43.71
    L1/TEX Cache Throughput           %        16.39
    L2 Cache Throughput               %        47.54
    SM Active Cycles              cycle    94,478.24
    Compute (SM) Throughput           %        32.20
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.98
    Achieved Active Warps Per SM           warp        36.47
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.02%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      291,284
    Total DRAM Elapsed Cycles        cycle    1,570,816
    Average L1 Active Cycles         cycle    94,478.24
    Total L1 Elapsed Cycles          cycle    3,323,266
    Average L2 Active Cycles         cycle    49,150.12
    Total L2 Elapsed Cycles          cycle    1,325,488
    Average SM Active Cycles         cycle    94,478.24
    Total SM Elapsed Cycles          cycle    3,323,266
    Average SMSP Active Cycles       cycle    59,120.79
    Total SMSP Elapsed Cycles        cycle   13,293,064
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       66,732
    Memory Throughput                 %        92.53
    DRAM Throughput                   %        92.53
    Duration                         us        29.31
    L1/TEX Cache Throughput           %        17.09
    L2 Cache Throughput               %        22.12
    SM Active Cycles              cycle    65,155.82
    Compute (SM) Throughput           %        23.97
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.17
    Achieved Active Warps Per SM           warp        36.56
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.83%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      243,280
    Total DRAM Elapsed Cycles        cycle    1,051,648
    Average L1 Active Cycles         cycle    65,155.82
    Total L1 Elapsed Cycles          cycle    2,294,366
    Average L2 Active Cycles         cycle    54,817.50
    Total L2 Elapsed Cycles          cycle      887,568
    Average SM Active Cycles         cycle    65,155.82
    Total SM Elapsed Cycles          cycle    2,294,366
    Average SMSP Active Cycles       cycle    63,174.58
    Total SMSP Elapsed Cycles        cycle    9,177,464
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       97,647
    Memory Throughput                 %        75.77
    DRAM Throughput                   %        75.77
    Duration                         us        42.85
    L1/TEX Cache Throughput           %        15.99
    L2 Cache Throughput               %        46.67
    SM Active Cycles              cycle    96,879.21
    Compute (SM) Throughput           %        32.13
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.58
    Achieved Active Warps Per SM           warp        34.84
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.42%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      291,720
    Total DRAM Elapsed Cycles        cycle    1,540,096
    Average L1 Active Cycles         cycle    96,879.21
    Total L1 Elapsed Cycles          cycle    3,335,184
    Average L2 Active Cycles         cycle    49,943.56
    Total L2 Elapsed Cycles          cycle    1,299,072
    Average SM Active Cycles         cycle    96,879.21
    Total SM Elapsed Cycles          cycle    3,335,184
    Average SMSP Active Cycles       cycle    60,053.26
    Total SMSP Elapsed Cycles        cycle   13,340,736
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       69,660
    Memory Throughput                 %        91.53
    DRAM Throughput                   %        91.53
    Duration                         us        30.56
    L1/TEX Cache Throughput           %        21.88
    L2 Cache Throughput               %        21.20
    SM Active Cycles              cycle    35,233.59
    Compute (SM) Throughput           %        21.94
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       140.23
    Achieved Active Warps Per SM           warp        67.31
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      251,180
    Total DRAM Elapsed Cycles        cycle    1,097,728
    Average L1 Active Cycles         cycle    35,233.59
    Total L1 Elapsed Cycles          cycle    2,508,198
    Average L2 Active Cycles         cycle    55,348.69
    Total L2 Elapsed Cycles          cycle      925,888
    Average SM Active Cycles         cycle    35,233.59
    Total SM Elapsed Cycles          cycle    2,508,198
    Average SMSP Active Cycles       cycle    64,358.05
    Total SMSP Elapsed Cycles        cycle   10,032,792
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       98,547
    Memory Throughput                 %        74.26
    DRAM Throughput                   %        74.26
    Duration                         us        43.23
    L1/TEX Cache Throughput           %        16.06
    L2 Cache Throughput               %        46.64
    SM Active Cycles              cycle    96,462.68
    Compute (SM) Throughput           %        31.85
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.64
    Achieved Active Warps Per SM           warp        34.87
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.36%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      288,212
    Total DRAM Elapsed Cycles        cycle    1,552,384
    Average L1 Active Cycles         cycle    96,462.68
    Total L1 Elapsed Cycles          cycle    3,370,310
    Average L2 Active Cycles         cycle    79,012.50
    Total L2 Elapsed Cycles          cycle    1,310,560
    Average SM Active Cycles         cycle    96,462.68
    Total SM Elapsed Cycles          cycle    3,370,310
    Average SMSP Active Cycles       cycle    95,192.09
    Total SMSP Elapsed Cycles        cycle   13,481,240
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       69,419
    Memory Throughput                 %        91.66
    DRAM Throughput                   %        91.66
    Duration                         us        30.46
    L1/TEX Cache Throughput           %        21.87
    L2 Cache Throughput               %        21.27
    SM Active Cycles              cycle    35,261.88
    Compute (SM) Throughput           %        23.69
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       140.20
    Achieved Active Warps Per SM           warp        67.29
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      250,840
    Total DRAM Elapsed Cycles        cycle    1,094,656
    Average L1 Active Cycles         cycle    35,261.88
    Total L1 Elapsed Cycles          cycle    2,325,524
    Average L2 Active Cycles         cycle    54,638.50
    Total L2 Elapsed Cycles          cycle      923,296
    Average SM Active Cycles         cycle    35,261.88
    Total SM Elapsed Cycles          cycle    2,325,524
    Average SMSP Active Cycles       cycle    63,220.70
    Total SMSP Elapsed Cycles        cycle    9,302,096
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.85
    SM Frequency                    Ghz         2.24
    Elapsed Cycles                cycle        3,371
    Memory Throughput                 %         2.22
    DRAM Throughput                   %         2.22
    Duration                         us         1.50
    L1/TEX Cache Throughput           %         3.60
    L2 Cache Throughput               %         0.62
    SM Active Cycles              cycle       332.94
    Compute (SM) Throughput           %         0.33
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.16
    Achieved Active Warps Per SM           warp         7.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          296
    Total DRAM Elapsed Cycles        cycle       53,248
    Average L1 Active Cycles         cycle       332.94
    Total L1 Elapsed Cycles          cycle      151,166
    Average L2 Active Cycles         cycle       526.56
    Total L2 Elapsed Cycles          cycle       44,432
    Average SM Active Cycles         cycle       332.94
    Total SM Elapsed Cycles          cycle      151,166
    Average SMSP Active Cycles       cycle       302.16
    Total SMSP Elapsed Cycles        cycle      604,664
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.307%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.87% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.307%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.87% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.63%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 56.05% above the average, while the minimum instance value is 85.57% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      100,696
    Memory Throughput                 %        74.90
    DRAM Throughput                   %        74.90
    Duration                         us        44.13
    L1/TEX Cache Throughput           %        26.45
    L2 Cache Throughput               %        46.85
    SM Active Cycles              cycle    58,548.94
    Compute (SM) Throughput           %        33.58
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       123.81
    Achieved Active Warps Per SM           warp        59.43
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      296,800
    Total DRAM Elapsed Cycles        cycle    1,585,152
    Average L1 Active Cycles         cycle    58,548.94
    Total L1 Elapsed Cycles          cycle    3,200,902
    Average L2 Active Cycles         cycle    76,957.12
    Total L2 Elapsed Cycles          cycle    1,338,496
    Average SM Active Cycles         cycle    58,548.94
    Total SM Elapsed Cycles          cycle    3,200,902
    Average SMSP Active Cycles       cycle    93,323.16
    Total SMSP Elapsed Cycles        cycle   12,803,608
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       67,740
    Memory Throughput                 %        92.37
    DRAM Throughput                   %        92.37
    Duration                         us        29.70
    L1/TEX Cache Throughput           %        21.84
    L2 Cache Throughput               %        21.81
    SM Active Cycles              cycle    35,301.09
    Compute (SM) Throughput           %        24.38
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       135.94
    Achieved Active Warps Per SM           warp        65.25
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      246,168
    Total DRAM Elapsed Cycles        cycle    1,065,984
    Average L1 Active Cycles         cycle    35,301.09
    Total L1 Elapsed Cycles          cycle    2,262,510
    Average L2 Active Cycles         cycle    54,251.69
    Total L2 Elapsed Cycles          cycle      899,984
    Average SM Active Cycles         cycle    35,301.09
    Total SM Elapsed Cycles          cycle    2,262,510
    Average SMSP Active Cycles       cycle    61,755.70
    Total SMSP Elapsed Cycles        cycle    9,050,040
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.67
    SM Frequency                    Ghz         2.23
    Elapsed Cycles                cycle        3,419
    Memory Throughput                 %         1.95
    DRAM Throughput                   %         1.95
    Duration                         us         1.54
    L1/TEX Cache Throughput           %         3.49
    L2 Cache Throughput               %         0.55
    SM Active Cycles              cycle       343.38
    Compute (SM) Throughput           %         0.43
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.43
    Achieved Active Warps Per SM           warp         7.40
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.57%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          260
    Total DRAM Elapsed Cycles        cycle       53,248
    Average L1 Active Cycles         cycle       343.38
    Total L1 Elapsed Cycles          cycle      115,048
    Average L2 Active Cycles         cycle       406.44
    Total L2 Elapsed Cycles          cycle       45,104
    Average SM Active Cycles         cycle       343.38
    Total SM Elapsed Cycles          cycle      115,048
    Average SMSP Active Cycles       cycle       307.10
    Total SMSP Elapsed Cycles        cycle      460,192
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.2%                                                                                            
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.95% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.635%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.11% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.2%                                                                                            
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.95% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.687%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 46.38% above the average, while the minimum instance value is 89.67% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      103,061
    Memory Throughput                 %        77.44
    DRAM Throughput                   %        77.44
    Duration                         us        45.15
    L1/TEX Cache Throughput           %        16.15
    L2 Cache Throughput               %        47.42
    SM Active Cycles              cycle    95,875.59
    Compute (SM) Throughput           %        30.76
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.65
    Achieved Active Warps Per SM           warp        37.75
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.35%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      314,008
    Total DRAM Elapsed Cycles        cycle    1,622,016
    Average L1 Active Cycles         cycle    95,875.59
    Total L1 Elapsed Cycles          cycle    3,497,476
    Average L2 Active Cycles         cycle    89,816.50
    Total L2 Elapsed Cycles          cycle    1,370,064
    Average SM Active Cycles         cycle    95,875.59
    Total SM Elapsed Cycles          cycle    3,497,476
    Average SMSP Active Cycles       cycle   108,684.65
    Total SMSP Elapsed Cycles        cycle   13,989,904
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       76,550
    Memory Throughput                 %        92.08
    DRAM Throughput                   %        92.08
    Duration                         us        33.47
    L1/TEX Cache Throughput           %        16.77
    L2 Cache Throughput               %        20.32
    SM Active Cycles              cycle    63,938.68
    Compute (SM) Throughput           %        23.60
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        84.67
    Achieved Active Warps Per SM           warp        40.64
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 15.33%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (84.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      276,748
    Total DRAM Elapsed Cycles        cycle    1,202,176
    Average L1 Active Cycles         cycle    63,938.68
    Total L1 Elapsed Cycles          cycle    2,338,812
    Average L2 Active Cycles         cycle    55,623.62
    Total L2 Elapsed Cycles          cycle    1,015,664
    Average SM Active Cycles         cycle    63,938.68
    Total SM Elapsed Cycles          cycle    2,338,812
    Average SMSP Active Cycles       cycle    64,738.18
    Total SMSP Elapsed Cycles        cycle    9,355,248
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.68
    SM Frequency                    Ghz         2.24
    Elapsed Cycles                cycle        3,363
    Memory Throughput                 %         1.78
    DRAM Throughput                   %         1.78
    Duration                         us         1.50
    L1/TEX Cache Throughput           %         3.57
    L2 Cache Throughput               %         0.56
    SM Active Cycles              cycle       336.09
    Compute (SM) Throughput           %         0.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.62
    Achieved Active Warps Per SM           warp         7.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.38%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (14.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          232
    Total DRAM Elapsed Cycles        cycle       52,224
    Average L1 Active Cycles         cycle       336.09
    Total L1 Elapsed Cycles          cycle      115,842
    Average L2 Active Cycles         cycle       440.69
    Total L2 Elapsed Cycles          cycle       44,336
    Average SM Active Cycles         cycle       336.09
    Total SM Elapsed Cycles          cycle      115,842
    Average SMSP Active Cycles       cycle       302.65
    Total SMSP Elapsed Cycles        cycle      463,368
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.996%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.93% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.502%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.19% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.996%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.93% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.774%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 55.17% above the average, while the minimum instance value is 90.47% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      104,497
    Memory Throughput                 %        75.59
    DRAM Throughput                   %        75.59
    Duration                         us        45.76
    L1/TEX Cache Throughput           %        16.16
    L2 Cache Throughput               %        45.41
    SM Active Cycles              cycle    95,869.74
    Compute (SM) Throughput           %        29.60
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.01
    Achieved Active Warps Per SM           warp        37.93
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.99%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      310,768
    Total DRAM Elapsed Cycles        cycle    1,644,544
    Average L1 Active Cycles         cycle    95,869.74
    Total L1 Elapsed Cycles          cycle    3,638,256
    Average L2 Active Cycles         cycle    79,990.06
    Total L2 Elapsed Cycles          cycle    1,389,440
    Average SM Active Cycles         cycle    95,869.74
    Total SM Elapsed Cycles          cycle    3,638,256
    Average SMSP Active Cycles       cycle    97,006.40
    Total SMSP Elapsed Cycles        cycle   14,553,024
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       72,407
    Memory Throughput                 %        91.95
    DRAM Throughput                   %        91.95
    Duration                         us        31.71
    L1/TEX Cache Throughput           %        21.83
    L2 Cache Throughput               %        20.41
    SM Active Cycles              cycle    35,324.94
    Compute (SM) Throughput           %        20.34
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       145.06
    Achieved Active Warps Per SM           warp        69.63
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      261,748
    Total DRAM Elapsed Cycles        cycle    1,138,688
    Average L1 Active Cycles         cycle    35,324.94
    Total L1 Elapsed Cycles          cycle    2,717,166
    Average L2 Active Cycles         cycle    58,928.69
    Total L2 Elapsed Cycles          cycle      961,760
    Average SM Active Cycles         cycle    35,324.94
    Total SM Elapsed Cycles          cycle    2,717,166
    Average SMSP Active Cycles       cycle    68,571.65
    Total SMSP Elapsed Cycles        cycle   10,868,664
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.68
    SM Frequency                    Ghz         2.24
    Elapsed Cycles                cycle        3,373
    Memory Throughput                 %         2.48
    DRAM Throughput                   %         2.48
    Duration                         us         1.50
    L1/TEX Cache Throughput           %         3.28
    L2 Cache Throughput               %         0.59
    SM Active Cycles              cycle       365.79
    Compute (SM) Throughput           %         0.42
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.13
    Achieved Active Warps Per SM           warp         6.78
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.87%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (14.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          324
    Total DRAM Elapsed Cycles        cycle       52,224
    Average L1 Active Cycles         cycle       365.79
    Total L1 Elapsed Cycles          cycle      121,094
    Average L2 Active Cycles         cycle       418.69
    Total L2 Elapsed Cycles          cycle       44,432
    Average SM Active Cycles         cycle       365.79
    Total SM Elapsed Cycles          cycle      121,094
    Average SMSP Active Cycles       cycle       313.42
    Total SMSP Elapsed Cycles        cycle      484,376
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.277%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.85% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.441%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.19% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.277%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.85% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.616%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 57.15% above the average, while the minimum instance value is 89.97% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      101,327
    Memory Throughput                 %        77.05
    DRAM Throughput                   %        77.05
    Duration                         us        44.42
    L1/TEX Cache Throughput           %        25.97
    L2 Cache Throughput               %        47.49
    SM Active Cycles              cycle    59,646.76
    Compute (SM) Throughput           %        30.50
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       123.38
    Achieved Active Warps Per SM           warp        59.22
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      307,120
    Total DRAM Elapsed Cycles        cycle    1,594,368
    Average L1 Active Cycles         cycle    59,646.76
    Total L1 Elapsed Cycles          cycle    3,535,190
    Average L2 Active Cycles         cycle    84,314.19
    Total L2 Elapsed Cycles          cycle    1,346,544
    Average SM Active Cycles         cycle    59,646.76
    Total SM Elapsed Cycles          cycle    3,535,190
    Average SMSP Active Cycles       cycle   102,396.14
    Total SMSP Elapsed Cycles        cycle   14,140,760
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       72,787
    Memory Throughput                 %        80.66
    DRAM Throughput                   %        80.66
    Duration                         us        31.81
    L1/TEX Cache Throughput           %        14.51
    L2 Cache Throughput               %        26.34
    SM Active Cycles              cycle    65,226.56
    Compute (SM) Throughput           %        20.47
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.43
    Achieved Active Warps Per SM           warp        34.77
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.57%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      230,452
    Total DRAM Elapsed Cycles        cycle    1,142,784
    Average L1 Active Cycles         cycle    65,226.56
    Total L1 Elapsed Cycles          cycle    2,702,164
    Average L2 Active Cycles         cycle    57,544.56
    Total L2 Elapsed Cycles          cycle      965,184
    Average SM Active Cycles         cycle    65,226.56
    Total SM Elapsed Cycles          cycle    2,702,164
    Average SMSP Active Cycles       cycle    67,022.98
    Total SMSP Elapsed Cycles        cycle   10,808,656
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       99,579
    Memory Throughput                 %        74.12
    DRAM Throughput                   %        74.12
    Duration                         us        43.68
    L1/TEX Cache Throughput           %        25.09
    L2 Cache Throughput               %        46.26
    SM Active Cycles              cycle    61,735.94
    Compute (SM) Throughput           %        32.37
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       115.20
    Achieved Active Warps Per SM           warp        55.29
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      290,896
    Total DRAM Elapsed Cycles        cycle    1,569,792
    Average L1 Active Cycles         cycle    61,735.94
    Total L1 Elapsed Cycles          cycle    3,334,960
    Average L2 Active Cycles         cycle       77,775
    Total L2 Elapsed Cycles          cycle    1,324,576
    Average SM Active Cycles         cycle    61,735.94
    Total SM Elapsed Cycles          cycle    3,334,960
    Average SMSP Active Cycles       cycle    93,996.51
    Total SMSP Elapsed Cycles        cycle   13,339,840
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.96
    SM Frequency                    Ghz         2.25
    Elapsed Cycles                cycle       46,697
    Memory Throughput                 %        90.47
    DRAM Throughput                   %        90.47
    Duration                         us        20.70
    L1/TEX Cache Throughput           %        15.79
    L2 Cache Throughput               %        31.43
    SM Active Cycles              cycle    65,280.24
    Compute (SM) Throughput           %        22.28
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        51.64
    Achieved Active Warps Per SM           warp        24.79
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 48.36%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (51.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      167,916
    Total DRAM Elapsed Cycles        cycle      742,400
    Average L1 Active Cycles         cycle    65,280.24
    Total L1 Elapsed Cycles          cycle    2,484,356
    Average L2 Active Cycles         cycle    58,969.94
    Total L2 Elapsed Cycles          cycle      624,192
    Average SM Active Cycles         cycle    65,280.24
    Total SM Elapsed Cycles          cycle    2,484,356
    Average SMSP Active Cycles       cycle    68,859.88
    Total SMSP Elapsed Cycles        cycle    9,937,424
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.68
    SM Frequency                    Ghz         2.25
    Elapsed Cycles                cycle        3,384
    Memory Throughput                 %         2.33
    DRAM Throughput                   %         2.33
    Duration                         us         1.50
    L1/TEX Cache Throughput           %         3.42
    L2 Cache Throughput               %         1.90
    SM Active Cycles              cycle       350.38
    Compute (SM) Throughput           %         0.34
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.13
    Achieved Active Warps Per SM           warp         6.78
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.87%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (14.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          304
    Total DRAM Elapsed Cycles        cycle       52,224
    Average L1 Active Cycles         cycle       350.38
    Total L1 Elapsed Cycles          cycle      129,346
    Average L2 Active Cycles         cycle       360.12
    Total L2 Elapsed Cycles          cycle       44,624
    Average SM Active Cycles         cycle       350.38
    Total SM Elapsed Cycles          cycle      129,346
    Average SMSP Active Cycles       cycle       293.24
    Total SMSP Elapsed Cycles        cycle      517,384
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.525%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.85% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.663%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.46% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.525%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.85% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.298%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 48.77% above the average, while the minimum instance value is 88.34% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      105,423
    Memory Throughput                 %        78.06
    DRAM Throughput                   %        78.06
    Duration                         us        46.14
    L1/TEX Cache Throughput           %        26.32
    L2 Cache Throughput               %        46.71
    SM Active Cycles              cycle    58,853.53
    Compute (SM) Throughput           %        28.21
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       131.12
    Achieved Active Warps Per SM           warp        62.94
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      323,740
    Total DRAM Elapsed Cycles        cycle    1,658,880
    Average L1 Active Cycles         cycle    58,853.53
    Total L1 Elapsed Cycles          cycle    3,831,584
    Average L2 Active Cycles         cycle    82,840.19
    Total L2 Elapsed Cycles          cycle    1,400,944
    Average SM Active Cycles         cycle    58,853.53
    Total SM Elapsed Cycles          cycle    3,831,584
    Average SMSP Active Cycles       cycle   100,116.08
    Total SMSP Elapsed Cycles        cycle   15,326,336
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.27
    Elapsed Cycles                cycle       65,683
    Memory Throughput                 %        92.69
    DRAM Throughput                   %        92.69
    Duration                         us        28.86
    L1/TEX Cache Throughput           %        24.87
    L2 Cache Throughput               %        22.99
    SM Active Cycles              cycle    62,216.26
    Compute (SM) Throughput           %        35.13
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.58
    Achieved Active Warps Per SM           warp        36.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.42%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      240,128
    Total DRAM Elapsed Cycles        cycle    1,036,288
    Average L1 Active Cycles         cycle    62,216.26
    Total L1 Elapsed Cycles          cycle    1,577,020
    Average L2 Active Cycles         cycle       66,460
    Total L2 Elapsed Cycles          cycle      873,888
    Average SM Active Cycles         cycle    62,216.26
    Total SM Elapsed Cycles          cycle    1,577,020
    Average SMSP Active Cycles       cycle    77,797.82
    Total SMSP Elapsed Cycles        cycle    6,308,080
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      106,527
    Memory Throughput                 %        76.91
    DRAM Throughput                   %        76.91
    Duration                         us        46.66
    L1/TEX Cache Throughput           %        15.04
    L2 Cache Throughput               %        46.65
    SM Active Cycles              cycle   102,960.91
    Compute (SM) Throughput           %        29.49
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.59
    Achieved Active Warps Per SM           warp        36.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.41%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      322,320
    Total DRAM Elapsed Cycles        cycle    1,676,288
    Average L1 Active Cycles         cycle   102,960.91
    Total L1 Elapsed Cycles          cycle    3,668,550
    Average L2 Active Cycles         cycle    86,184.44
    Total L2 Elapsed Cycles          cycle    1,415,376
    Average SM Active Cycles         cycle   102,960.91
    Total SM Elapsed Cycles          cycle    3,668,550
    Average SMSP Active Cycles       cycle   103,951.25
    Total SMSP Elapsed Cycles        cycle   14,674,200
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       70,470
    Memory Throughput                 %        92.40
    DRAM Throughput                   %        92.40
    Duration                         us        30.88
    L1/TEX Cache Throughput           %        21.76
    L2 Cache Throughput               %        21.01
    SM Active Cycles              cycle    35,430.97
    Compute (SM) Throughput           %        22.41
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       143.50
    Achieved Active Warps Per SM           warp        68.88
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      256,168
    Total DRAM Elapsed Cycles        cycle    1,108,992
    Average L1 Active Cycles         cycle    35,430.97
    Total L1 Elapsed Cycles          cycle    2,474,824
    Average L2 Active Cycles         cycle    54,865.12
    Total L2 Elapsed Cycles          cycle      935,584
    Average SM Active Cycles         cycle    35,430.97
    Total SM Elapsed Cycles          cycle    2,474,824
    Average SMSP Active Cycles       cycle    63,679.37
    Total SMSP Elapsed Cycles        cycle    9,899,296
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      100,243
    Memory Throughput                 %        76.03
    DRAM Throughput                   %        76.03
    Duration                         us        43.97
    L1/TEX Cache Throughput           %        15.05
    L2 Cache Throughput               %        47.96
    SM Active Cycles              cycle   102,941.91
    Compute (SM) Throughput           %        31.23
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.37
    Achieved Active Warps Per SM           warp        34.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.63%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      300,136
    Total DRAM Elapsed Cycles        cycle    1,579,008
    Average L1 Active Cycles         cycle   102,941.91
    Total L1 Elapsed Cycles          cycle    3,469,468
    Average L2 Active Cycles         cycle    49,985.12
    Total L2 Elapsed Cycles          cycle    1,332,608
    Average SM Active Cycles         cycle   102,941.91
    Total SM Elapsed Cycles          cycle    3,469,468
    Average SMSP Active Cycles       cycle    60,216.09
    Total SMSP Elapsed Cycles        cycle   13,877,872
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       74,329
    Memory Throughput                 %        91.63
    DRAM Throughput                   %        91.63
    Duration                         us        32.54
    L1/TEX Cache Throughput           %        21.76
    L2 Cache Throughput               %        19.88
    SM Active Cycles              cycle    35,433.97
    Compute (SM) Throughput           %        19.77
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       148.75
    Achieved Active Warps Per SM           warp        71.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      267,404
    Total DRAM Elapsed Cycles        cycle    1,167,360
    Average L1 Active Cycles         cycle    35,433.97
    Total L1 Elapsed Cycles          cycle    2,808,090
    Average L2 Active Cycles         cycle    59,082.75
    Total L2 Elapsed Cycles          cycle      986,944
    Average SM Active Cycles         cycle    35,433.97
    Total SM Elapsed Cycles          cycle    2,808,090
    Average SMSP Active Cycles       cycle    68,777.09
    Total SMSP Elapsed Cycles        cycle   11,232,360
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       98,341
    Memory Throughput                 %        75.20
    DRAM Throughput                   %        75.20
    Duration                         us        43.14
    L1/TEX Cache Throughput           %        16.23
    L2 Cache Throughput               %        47.21
    SM Active Cycles              cycle    95,410.50
    Compute (SM) Throughput           %        28.20
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.76
    Achieved Active Warps Per SM           warp        35.89
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.24%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      291,452
    Total DRAM Elapsed Cycles        cycle    1,550,336
    Average L1 Active Cycles         cycle    95,410.50
    Total L1 Elapsed Cycles          cycle    3,848,196
    Average L2 Active Cycles         cycle    85,766.44
    Total L2 Elapsed Cycles          cycle    1,308,320
    Average SM Active Cycles         cycle    95,410.50
    Total SM Elapsed Cycles          cycle    3,848,196
    Average SMSP Active Cycles       cycle   103,925.29
    Total SMSP Elapsed Cycles        cycle   15,392,784
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       72,776
    Memory Throughput                 %        79.54
    DRAM Throughput                   %        79.54
    Duration                         us        31.81
    L1/TEX Cache Throughput           %        15.78
    L2 Cache Throughput               %        26.28
    SM Active Cycles              cycle    62,535.18
    Compute (SM) Throughput           %        22.36
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.61
    Achieved Active Warps Per SM           warp        36.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.39%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      227,252
    Total DRAM Elapsed Cycles        cycle    1,142,784
    Average L1 Active Cycles         cycle    62,535.18
    Total L1 Elapsed Cycles          cycle    2,485,948
    Average L2 Active Cycles         cycle    58,320.06
    Total L2 Elapsed Cycles          cycle      964,928
    Average SM Active Cycles         cycle    62,535.18
    Total SM Elapsed Cycles          cycle    2,485,948
    Average SMSP Active Cycles       cycle    67,575.49
    Total SMSP Elapsed Cycles        cycle    9,943,792
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.30
    Elapsed Cycles                cycle      112,371
    Memory Throughput                 %        76.85
    DRAM Throughput                   %        76.85
    Duration                         us        48.90
    L1/TEX Cache Throughput           %        16.05
    L2 Cache Throughput               %        46.61
    SM Active Cycles              cycle    96,471.85
    Compute (SM) Throughput           %        29.90
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        85.07
    Achieved Active Warps Per SM           warp        40.83
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 14.93%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (85.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      337,612
    Total DRAM Elapsed Cycles        cycle    1,757,184
    Average L1 Active Cycles         cycle    96,471.85
    Total L1 Elapsed Cycles          cycle    3,635,662
    Average L2 Active Cycles         cycle    86,032.06
    Total L2 Elapsed Cycles          cycle    1,487,328
    Average SM Active Cycles         cycle    96,471.85
    Total SM Elapsed Cycles          cycle    3,635,662
    Average SMSP Active Cycles       cycle   103,981.43
    Total SMSP Elapsed Cycles        cycle   14,542,648
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.27
    Elapsed Cycles                cycle       72,828
    Memory Throughput                 %        92.08
    DRAM Throughput                   %        92.08
    Duration                         us        32.10
    L1/TEX Cache Throughput           %        15.86
    L2 Cache Throughput               %        20.22
    SM Active Cycles              cycle    69,275.71
    Compute (SM) Throughput           %        22.50
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.14
    Achieved Active Warps Per SM           warp        36.55
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.86%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      265,432
    Total DRAM Elapsed Cycles        cycle    1,153,024
    Average L1 Active Cycles         cycle    69,275.71
    Total L1 Elapsed Cycles          cycle    2,473,098
    Average L2 Active Cycles         cycle    60,271.62
    Total L2 Elapsed Cycles          cycle      970,656
    Average SM Active Cycles         cycle    69,275.71
    Total SM Elapsed Cycles          cycle    2,473,098
    Average SMSP Active Cycles       cycle    69,816.38
    Total SMSP Elapsed Cycles        cycle    9,892,392
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.71
    SM Frequency                    Ghz         2.22
    Elapsed Cycles                cycle        3,981
    Memory Throughput                 %         1.77
    DRAM Throughput                   %         1.77
    Duration                         us         1.79
    L1/TEX Cache Throughput           %         3.50
    L2 Cache Throughput               %         0.47
    SM Active Cycles              cycle       342.38
    Compute (SM) Throughput           %         0.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.20
    Achieved Active Warps Per SM           warp         7.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.8%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          276
    Total DRAM Elapsed Cycles        cycle       62,464
    Average L1 Active Cycles         cycle       342.38
    Total L1 Elapsed Cycles          cycle      115,884
    Average L2 Active Cycles         cycle       408.38
    Total L2 Elapsed Cycles          cycle       52,560
    Average SM Active Cycles         cycle       342.38
    Total SM Elapsed Cycles          cycle      115,884
    Average SMSP Active Cycles       cycle       312.57
    Total SMSP Elapsed Cycles        cycle      463,536
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.121%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.89% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.704%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.10% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.121%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.89% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.617%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 45.18% above the average, while the minimum instance value is 89.72% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.26
    Elapsed Cycles                cycle       99,522
    Memory Throughput                 %        74.42
    DRAM Throughput                   %        74.42
    Duration                         us        43.90
    L1/TEX Cache Throughput           %        25.66
    L2 Cache Throughput               %        46.76
    SM Active Cycles              cycle    60,368.71
    Compute (SM) Throughput           %        31.86
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       118.30
    Achieved Active Warps Per SM           warp        56.78
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      293,388
    Total DRAM Elapsed Cycles        cycle    1,576,960
    Average L1 Active Cycles         cycle    60,368.71
    Total L1 Elapsed Cycles          cycle    3,416,324
    Average L2 Active Cycles         cycle    77,514.94
    Total L2 Elapsed Cycles          cycle    1,327,184
    Average SM Active Cycles         cycle    60,368.71
    Total SM Elapsed Cycles          cycle    3,416,324
    Average SMSP Active Cycles       cycle    93,445.06
    Total SMSP Elapsed Cycles        cycle   13,665,296
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.27
    Elapsed Cycles                cycle       72,819
    Memory Throughput                 %        92.10
    DRAM Throughput                   %        92.10
    Duration                         us        32.13
    L1/TEX Cache Throughput           %        16.15
    L2 Cache Throughput               %        20.26
    SM Active Cycles              cycle    68,484.32
    Compute (SM) Throughput           %        22.93
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.72
    Achieved Active Warps Per SM           warp        37.30
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.28%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      265,492
    Total DRAM Elapsed Cycles        cycle    1,153,024
    Average L1 Active Cycles         cycle    68,484.32
    Total L1 Elapsed Cycles          cycle    2,428,322
    Average L2 Active Cycles         cycle    58,679.81
    Total L2 Elapsed Cycles          cycle      969,968
    Average SM Active Cycles         cycle    68,484.32
    Total SM Elapsed Cycles          cycle    2,428,322
    Average SMSP Active Cycles       cycle    68,050.82
    Total SMSP Elapsed Cycles        cycle    9,713,288
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.63
    SM Frequency                    Ghz         2.24
    Elapsed Cycles                cycle        3,660
    Memory Throughput                 %        11.39
    DRAM Throughput                   %        11.39
    Duration                         us         1.63
    L1/TEX Cache Throughput           %         3.66
    L2 Cache Throughput               %         1.12
    SM Active Cycles              cycle       327.76
    Compute (SM) Throughput           %         0.43
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.38
    Achieved Active Warps Per SM           warp         7.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.62%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        1,604
    Total DRAM Elapsed Cycles        cycle       56,320
    Average L1 Active Cycles         cycle       327.76
    Total L1 Elapsed Cycles          cycle      116,408
    Average L2 Active Cycles         cycle       339.56
    Total L2 Elapsed Cycles          cycle       48,304
    Average SM Active Cycles         cycle       327.76
    Total SM Elapsed Cycles          cycle      116,408
    Average SMSP Active Cycles       cycle       303.77
    Total SMSP Elapsed Cycles        cycle      465,632
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.784%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.87% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.492%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.17% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.784%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.87% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.202%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 55.14% above the average, while the minimum instance value is 87.63% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       97,772
    Memory Throughput                 %        74.80
    DRAM Throughput                   %        74.80
    Duration                         us        42.85
    L1/TEX Cache Throughput           %        25.70
    L2 Cache Throughput               %        46.59
    SM Active Cycles              cycle       60,268
    Compute (SM) Throughput           %        30.16
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       118.10
    Achieved Active Warps Per SM           warp        56.69
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      288,000
    Total DRAM Elapsed Cycles        cycle    1,540,096
    Average L1 Active Cycles         cycle       60,268
    Total L1 Elapsed Cycles          cycle    3,611,862
    Average L2 Active Cycles         cycle    88,312.06
    Total L2 Elapsed Cycles          cycle    1,299,584
    Average SM Active Cycles         cycle       60,268
    Total SM Elapsed Cycles          cycle    3,611,862
    Average SMSP Active Cycles       cycle   106,809.47
    Total SMSP Elapsed Cycles        cycle   14,447,448
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       87,410
    Memory Throughput                 %        91.62
    DRAM Throughput                   %        91.62
    Duration                         us        38.08
    L1/TEX Cache Throughput           %        21.63
    L2 Cache Throughput               %        26.41
    SM Active Cycles              cycle    35,637.32
    Compute (SM) Throughput           %        21.51
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       172.25
    Achieved Active Warps Per SM           warp        82.68
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      313,360
    Total DRAM Elapsed Cycles        cycle    1,368,064
    Average L1 Active Cycles         cycle    35,637.32
    Total L1 Elapsed Cycles          cycle    2,590,340
    Average L2 Active Cycles         cycle    58,201.81
    Total L2 Elapsed Cycles          cycle    1,157,680
    Average SM Active Cycles         cycle    35,637.32
    Total SM Elapsed Cycles          cycle    2,590,340
    Average SMSP Active Cycles       cycle    67,924.83
    Total SMSP Elapsed Cycles        cycle   10,361,360
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      109,023
    Memory Throughput                 %        75.76
    DRAM Throughput                   %        75.76
    Duration                         us        47.71
    L1/TEX Cache Throughput           %        14.25
    L2 Cache Throughput               %        45.87
    SM Active Cycles              cycle   108,687.53
    Compute (SM) Throughput           %        30.11
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.17
    Achieved Active Warps Per SM           warp        35.12
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.83%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      324,848
    Total DRAM Elapsed Cycles        cycle    1,715,200
    Average L1 Active Cycles         cycle   108,687.53
    Total L1 Elapsed Cycles          cycle    3,619,968
    Average L2 Active Cycles         cycle    84,351.62
    Total L2 Elapsed Cycles          cycle    1,448,512
    Average SM Active Cycles         cycle   108,687.53
    Total SM Elapsed Cycles          cycle    3,619,968
    Average SMSP Active Cycles       cycle   102,283.51
    Total SMSP Elapsed Cycles        cycle   14,479,872
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       68,597
    Memory Throughput                 %        92.70
    DRAM Throughput                   %        92.70
    Duration                         us        30.11
    L1/TEX Cache Throughput           %        17.66
    L2 Cache Throughput               %        21.54
    SM Active Cycles              cycle    63,371.97
    Compute (SM) Throughput           %        25.10
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.16
    Achieved Active Warps Per SM           warp        38.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      250,612
    Total DRAM Elapsed Cycles        cycle    1,081,344
    Average L1 Active Cycles         cycle    63,371.97
    Total L1 Elapsed Cycles          cycle    2,222,140
    Average L2 Active Cycles         cycle    29,148.31
    Total L2 Elapsed Cycles          cycle      911,600
    Average SM Active Cycles         cycle    63,371.97
    Total SM Elapsed Cycles          cycle    2,222,140
    Average SMSP Active Cycles       cycle    35,281.01
    Total SMSP Elapsed Cycles        cycle    8,888,560
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       99,279
    Memory Throughput                 %        74.98
    DRAM Throughput                   %        74.98
    Duration                         us        43.52
    L1/TEX Cache Throughput           %        25.92
    L2 Cache Throughput               %        47.03
    SM Active Cycles              cycle    59,746.44
    Compute (SM) Throughput           %        32.24
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       119.59
    Achieved Active Warps Per SM           warp        57.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      293,108
    Total DRAM Elapsed Cycles        cycle    1,563,648
    Average L1 Active Cycles         cycle    59,746.44
    Total L1 Elapsed Cycles          cycle    3,384,126
    Average L2 Active Cycles         cycle    78,182.50
    Total L2 Elapsed Cycles          cycle    1,319,456
    Average SM Active Cycles         cycle    59,746.44
    Total SM Elapsed Cycles          cycle    3,384,126
    Average SMSP Active Cycles       cycle    94,334.07
    Total SMSP Elapsed Cycles        cycle   13,536,504
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       70,143
    Memory Throughput                 %        92.46
    DRAM Throughput                   %        92.46
    Duration                         us        30.78
    L1/TEX Cache Throughput           %        16.01
    L2 Cache Throughput               %        21.19
    SM Active Cycles              cycle    68,562.26
    Compute (SM) Throughput           %        22.78
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.81
    Achieved Active Warps Per SM           warp        34.47
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.19%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      255,384
    Total DRAM Elapsed Cycles        cycle    1,104,896
    Average L1 Active Cycles         cycle    68,562.26
    Total L1 Elapsed Cycles          cycle    2,450,348
    Average L2 Active Cycles         cycle    59,761.94
    Total L2 Elapsed Cycles          cycle      932,560
    Average SM Active Cycles         cycle    68,562.26
    Total SM Elapsed Cycles          cycle    2,450,348
    Average SMSP Active Cycles       cycle    68,514.54
    Total SMSP Elapsed Cycles        cycle    9,801,392
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      104,621
    Memory Throughput                 %        75.61
    DRAM Throughput                   %        75.61
    Duration                         us        45.86
    L1/TEX Cache Throughput           %        14.85
    L2 Cache Throughput               %        47.64
    SM Active Cycles              cycle   104,294.50
    Compute (SM) Throughput           %        31.02
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.67
    Achieved Active Warps Per SM           warp        35.36
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.33%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      311,236
    Total DRAM Elapsed Cycles        cycle    1,646,592
    Average L1 Active Cycles         cycle   104,294.50
    Total L1 Elapsed Cycles          cycle    3,520,336
    Average L2 Active Cycles         cycle    78,009.25
    Total L2 Elapsed Cycles          cycle    1,390,496
    Average SM Active Cycles         cycle   104,294.50
    Total SM Elapsed Cycles          cycle    3,520,336
    Average SMSP Active Cycles       cycle    94,007.32
    Total SMSP Elapsed Cycles        cycle   14,081,344
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       69,257
    Memory Throughput                 %        91.92
    DRAM Throughput                   %        91.92
    Duration                         us        30.37
    L1/TEX Cache Throughput           %        16.01
    L2 Cache Throughput               %        21.33
    SM Active Cycles              cycle    67,420.44
    Compute (SM) Throughput           %        22.79
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.62
    Achieved Active Warps Per SM           warp        35.34
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.38%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      250,616
    Total DRAM Elapsed Cycles        cycle    1,090,560
    Average L1 Active Cycles         cycle    67,420.44
    Total L1 Elapsed Cycles          cycle    2,450,206
    Average L2 Active Cycles         cycle    60,042.31
    Total L2 Elapsed Cycles          cycle      920,224
    Average SM Active Cycles         cycle    67,420.44
    Total SM Elapsed Cycles          cycle    2,450,206
    Average SMSP Active Cycles       cycle    69,787.91
    Total SMSP Elapsed Cycles        cycle    9,800,824
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.68
    SM Frequency                    Ghz         2.23
    Elapsed Cycles                cycle        3,361
    Memory Throughput                 %         1.41
    DRAM Throughput                   %         1.41
    Duration                         us         1.50
    L1/TEX Cache Throughput           %         3.52
    L2 Cache Throughput               %         0.56
    SM Active Cycles              cycle       340.88
    Compute (SM) Throughput           %         0.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.01
    Achieved Active Warps Per SM           warp         7.20
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.99%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          184
    Total DRAM Elapsed Cycles        cycle       52,224
    Average L1 Active Cycles         cycle       340.88
    Total L1 Elapsed Cycles          cycle      115,984
    Average L2 Active Cycles         cycle       417.06
    Total L2 Elapsed Cycles          cycle       44,320
    Average SM Active Cycles         cycle       340.88
    Total SM Elapsed Cycles          cycle      115,984
    Average SMSP Active Cycles       cycle       312.98
    Total SMSP Elapsed Cycles        cycle      463,936
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.086%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.91% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.71%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.13% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.086%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.91% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.906%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 59.15% above the average, while the minimum instance value is 89.93% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      107,068
    Memory Throughput                 %        76.47
    DRAM Throughput                   %        76.47
    Duration                         us        46.91
    L1/TEX Cache Throughput           %        14.95
    L2 Cache Throughput               %        45.65
    SM Active Cycles              cycle   103,563.59
    Compute (SM) Throughput           %        29.73
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.08
    Achieved Active Warps Per SM           warp        36.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.92%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      322,216
    Total DRAM Elapsed Cycles        cycle    1,685,504
    Average L1 Active Cycles         cycle   103,563.59
    Total L1 Elapsed Cycles          cycle    3,675,492
    Average L2 Active Cycles         cycle    86,274.25
    Total L2 Elapsed Cycles          cycle    1,423,648
    Average SM Active Cycles         cycle   103,563.59
    Total SM Elapsed Cycles          cycle    3,675,492
    Average SMSP Active Cycles       cycle   104,170.43
    Total SMSP Elapsed Cycles        cycle   14,701,968
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       71,805
    Memory Throughput                 %        92.19
    DRAM Throughput                   %        92.19
    Duration                         us        31.46
    L1/TEX Cache Throughput           %        21.76
    L2 Cache Throughput               %        20.59
    SM Active Cycles              cycle    35,433.15
    Compute (SM) Throughput           %        20.43
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       148.66
    Achieved Active Warps Per SM           warp        71.36
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      260,544
    Total DRAM Elapsed Cycles        cycle    1,130,496
    Average L1 Active Cycles         cycle    35,433.15
    Total L1 Elapsed Cycles          cycle    2,735,426
    Average L2 Active Cycles         cycle    56,611.81
    Total L2 Elapsed Cycles          cycle      954,016
    Average SM Active Cycles         cycle    35,433.15
    Total SM Elapsed Cycles          cycle    2,735,426
    Average SMSP Active Cycles       cycle    65,845.15
    Total SMSP Elapsed Cycles        cycle   10,941,704
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle      109,100
    Memory Throughput                 %        76.65
    DRAM Throughput                   %        76.65
    Duration                         us        47.52
    L1/TEX Cache Throughput           %        25.12
    L2 Cache Throughput               %        46.71
    SM Active Cycles              cycle    61,666.38
    Compute (SM) Throughput           %        30.77
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       127.91
    Achieved Active Warps Per SM           warp        61.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      326,888
    Total DRAM Elapsed Cycles        cycle    1,705,984
    Average L1 Active Cycles         cycle    61,666.38
    Total L1 Elapsed Cycles          cycle    3,555,354
    Average L2 Active Cycles         cycle    80,891.62
    Total L2 Elapsed Cycles          cycle    1,444,720
    Average SM Active Cycles         cycle    61,666.38
    Total SM Elapsed Cycles          cycle    3,555,354
    Average SMSP Active Cycles       cycle    98,141.18
    Total SMSP Elapsed Cycles        cycle   14,221,416
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       72,035
    Memory Throughput                 %        92.21
    DRAM Throughput                   %        92.21
    Duration                         us        31.55
    L1/TEX Cache Throughput           %        16.77
    L2 Cache Throughput               %        20.52
    SM Active Cycles              cycle    69,405.29
    Compute (SM) Throughput           %        23.91
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.96
    Achieved Active Warps Per SM           warp        35.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.04%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      261,556
    Total DRAM Elapsed Cycles        cycle    1,134,592
    Average L1 Active Cycles         cycle    69,405.29
    Total L1 Elapsed Cycles          cycle    2,339,034
    Average L2 Active Cycles         cycle    58,589.06
    Total L2 Elapsed Cycles          cycle      957,072
    Average SM Active Cycles         cycle    69,405.29
    Total SM Elapsed Cycles          cycle    2,339,034
    Average SMSP Active Cycles       cycle    68,410.70
    Total SMSP Elapsed Cycles        cycle    9,356,136
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      113,553
    Memory Throughput                 %        77.93
    DRAM Throughput                   %        77.93
    Duration                         us        49.66
    L1/TEX Cache Throughput           %        26.10
    L2 Cache Throughput               %        46.66
    SM Active Cycles              cycle    59,329.62
    Compute (SM) Throughput           %        32.20
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       139.12
    Achieved Active Warps Per SM           warp        66.78
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      347,744
    Total DRAM Elapsed Cycles        cycle    1,784,832
    Average L1 Active Cycles         cycle    59,329.62
    Total L1 Elapsed Cycles          cycle    3,401,048
    Average L2 Active Cycles         cycle    78,620.50
    Total L2 Elapsed Cycles          cycle    1,507,760
    Average SM Active Cycles         cycle    59,329.62
    Total SM Elapsed Cycles          cycle    3,401,048
    Average SMSP Active Cycles       cycle    95,247.47
    Total SMSP Elapsed Cycles        cycle   13,604,192
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.27
    Elapsed Cycles                cycle       67,481
    Memory Throughput                 %        92.13
    DRAM Throughput                   %        92.13
    Duration                         us        29.63
    L1/TEX Cache Throughput           %        15.62
    L2 Cache Throughput               %        22.01
    SM Active Cycles              cycle    69,896.94
    Compute (SM) Throughput           %        22.29
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        68.11
    Achieved Active Warps Per SM           warp        32.69
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.89%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (68.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      244,808
    Total DRAM Elapsed Cycles        cycle    1,062,912
    Average L1 Active Cycles         cycle    69,896.94
    Total L1 Elapsed Cycles          cycle    2,510,948
    Average L2 Active Cycles         cycle    63,051.69
    Total L2 Elapsed Cycles          cycle      897,120
    Average SM Active Cycles         cycle    69,896.94
    Total SM Elapsed Cycles          cycle    2,510,948
    Average SMSP Active Cycles       cycle    72,937.11
    Total SMSP Elapsed Cycles        cycle   10,043,792
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      101,885
    Memory Throughput                 %        76.69
    DRAM Throughput                   %        76.69
    Duration                         us        44.67
    L1/TEX Cache Throughput           %        14.18
    L2 Cache Throughput               %        47.19
    SM Active Cycles              cycle   116,986.56
    Compute (SM) Throughput           %        30.17
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        63.07
    Achieved Active Warps Per SM           warp        30.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 36.93%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (63.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      307,456
    Total DRAM Elapsed Cycles        cycle    1,603,584
    Average L1 Active Cycles         cycle   116,986.56
    Total L1 Elapsed Cycles          cycle    3,632,872
    Average L2 Active Cycles         cycle    48,776.94
    Total L2 Elapsed Cycles          cycle    1,354,736
    Average SM Active Cycles         cycle   116,986.56
    Total SM Elapsed Cycles          cycle    3,632,872
    Average SMSP Active Cycles       cycle    58,712.18
    Total SMSP Elapsed Cycles        cycle   14,531,488
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       68,172
    Memory Throughput                 %        92.20
    DRAM Throughput                   %        92.20
    Duration                         us        29.89
    L1/TEX Cache Throughput           %        21.85
    L2 Cache Throughput               %        21.66
    SM Active Cycles              cycle    35,288.26
    Compute (SM) Throughput           %        22.89
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       138.51
    Achieved Active Warps Per SM           warp        66.48
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      247,364
    Total DRAM Elapsed Cycles        cycle    1,073,152
    Average L1 Active Cycles         cycle    35,288.26
    Total L1 Elapsed Cycles          cycle    2,447,906
    Average L2 Active Cycles         cycle    56,802.81
    Total L2 Elapsed Cycles          cycle      906,160
    Average SM Active Cycles         cycle    35,288.26
    Total SM Elapsed Cycles          cycle    2,447,906
    Average SMSP Active Cycles       cycle    66,662.84
    Total SMSP Elapsed Cycles        cycle    9,791,624
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       97,662
    Memory Throughput                 %        77.69
    DRAM Throughput                   %        77.69
    Duration                         us        42.82
    L1/TEX Cache Throughput           %        25.76
    L2 Cache Throughput               %        47.23
    SM Active Cycles              cycle    60,114.24
    Compute (SM) Throughput           %        28.66
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       116.62
    Achieved Active Warps Per SM           warp        55.98
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      298,732
    Total DRAM Elapsed Cycles        cycle    1,538,048
    Average L1 Active Cycles         cycle    60,114.24
    Total L1 Elapsed Cycles          cycle    3,830,038
    Average L2 Active Cycles         cycle    81,235.81
    Total L2 Elapsed Cycles          cycle    1,298,256
    Average SM Active Cycles         cycle    60,114.24
    Total SM Elapsed Cycles          cycle    3,830,038
    Average SMSP Active Cycles       cycle    98,365.64
    Total SMSP Elapsed Cycles        cycle   15,320,152
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.27
    Elapsed Cycles                cycle       66,172
    Memory Throughput                 %        92.55
    DRAM Throughput                   %        92.55
    Duration                         us        29.06
    L1/TEX Cache Throughput           %        12.05
    L2 Cache Throughput               %        22.33
    SM Active Cycles              cycle    63,987.38
    Compute (SM) Throughput           %        14.40
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.37
    Achieved Active Warps Per SM           warp        35.22
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.63%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      241,200
    Total DRAM Elapsed Cycles        cycle    1,042,432
    Average L1 Active Cycles         cycle    63,987.38
    Total L1 Elapsed Cycles          cycle    3,893,846
    Average L2 Active Cycles         cycle    61,861.69
    Total L2 Elapsed Cycles          cycle      879,040
    Average SM Active Cycles         cycle    63,987.38
    Total SM Elapsed Cycles          cycle    3,893,846
    Average SMSP Active Cycles       cycle    72,248.79
    Total SMSP Elapsed Cycles        cycle   15,575,384
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       98,760
    Memory Throughput                 %        74.45
    DRAM Throughput                   %        74.45
    Duration                         us        43.36
    L1/TEX Cache Throughput           %        19.65
    L2 Cache Throughput               %        47.74
    SM Active Cycles              cycle    78,819.82
    Compute (SM) Throughput           %        32.46
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        91.56
    Achieved Active Warps Per SM           warp        43.95
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      290,064
    Total DRAM Elapsed Cycles        cycle    1,558,528
    Average L1 Active Cycles         cycle    78,819.82
    Total L1 Elapsed Cycles          cycle    3,386,886
    Average L2 Active Cycles         cycle    78,889.75
    Total L2 Elapsed Cycles          cycle    1,313,936
    Average SM Active Cycles         cycle    78,819.82
    Total SM Elapsed Cycles          cycle    3,386,886
    Average SMSP Active Cycles       cycle    95,514.64
    Total SMSP Elapsed Cycles        cycle   13,547,544
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       70,871
    Memory Throughput                 %        92.77
    DRAM Throughput                   %        92.77
    Duration                         us        31.07
    L1/TEX Cache Throughput           %        16.46
    L2 Cache Throughput               %        20.87
    SM Active Cycles              cycle    65,592.12
    Compute (SM) Throughput           %        23.54
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.91
    Achieved Active Warps Per SM           warp        37.88
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.09%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      258,632
    Total DRAM Elapsed Cycles        cycle    1,115,136
    Average L1 Active Cycles         cycle    65,592.12
    Total L1 Elapsed Cycles          cycle    2,384,216
    Average L2 Active Cycles         cycle    29,060.56
    Total L2 Elapsed Cycles          cycle      941,856
    Average SM Active Cycles         cycle    65,592.12
    Total SM Elapsed Cycles          cycle    2,384,216
    Average SMSP Active Cycles       cycle    35,205.79
    Total SMSP Elapsed Cycles        cycle    9,536,864
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.78
    SM Frequency                    Ghz         2.26
    Elapsed Cycles                cycle        3,685
    Memory Throughput                 %        11.19
    DRAM Throughput                   %        11.19
    Duration                         us         1.63
    L1/TEX Cache Throughput           %         3.65
    L2 Cache Throughput               %         1.12
    SM Active Cycles              cycle       328.38
    Compute (SM) Throughput           %         0.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.39
    Achieved Active Warps Per SM           warp         7.39
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.61%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        1,604
    Total DRAM Elapsed Cycles        cycle       57,344
    Average L1 Active Cycles         cycle       328.38
    Total L1 Elapsed Cycles          cycle      114,182
    Average L2 Active Cycles         cycle       451.75
    Total L2 Elapsed Cycles          cycle       48,656
    Average SM Active Cycles         cycle       328.38
    Total SM Elapsed Cycles          cycle      114,182
    Average SMSP Active Cycles       cycle       297.18
    Total SMSP Elapsed Cycles        cycle      456,728
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.932%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.89% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.48%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.23% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.932%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.89% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.371%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 56.35% above the average, while the minimum instance value is 90.70% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle      115,236
    Memory Throughput                 %        76.16
    DRAM Throughput                   %        76.16
    Duration                         us        50.34
    L1/TEX Cache Throughput           %        25.61
    L2 Cache Throughput               %        43.84
    SM Active Cycles              cycle    60,484.26
    Compute (SM) Throughput           %        32.01
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       137.28
    Achieved Active Warps Per SM           warp        65.89
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      344,312
    Total DRAM Elapsed Cycles        cycle    1,808,384
    Average L1 Active Cycles         cycle    60,484.26
    Total L1 Elapsed Cycles          cycle    3,439,086
    Average L2 Active Cycles         cycle    78,749.25
    Total L2 Elapsed Cycles          cycle    1,528,976
    Average SM Active Cycles         cycle    60,484.26
    Total SM Elapsed Cycles          cycle    3,439,086
    Average SMSP Active Cycles       cycle    95,156.88
    Total SMSP Elapsed Cycles        cycle   13,756,344
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       76,203
    Memory Throughput                 %        91.78
    DRAM Throughput                   %        91.78
    Duration                         us        33.31
    L1/TEX Cache Throughput           %        16.91
    L2 Cache Throughput               %        19.81
    SM Active Cycles              cycle    62,732.03
    Compute (SM) Throughput           %        24.22
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.30
    Achieved Active Warps Per SM           warp        42.39
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.7%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      274,904
    Total DRAM Elapsed Cycles        cycle    1,198,080
    Average L1 Active Cycles         cycle    62,732.03
    Total L1 Elapsed Cycles          cycle    2,319,252
    Average L2 Active Cycles         cycle    55,308.38
    Total L2 Elapsed Cycles          cycle    1,010,624
    Average SM Active Cycles         cycle    62,732.03
    Total SM Elapsed Cycles          cycle    2,319,252
    Average SMSP Active Cycles       cycle    64,645.97
    Total SMSP Elapsed Cycles        cycle    9,277,008
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.71
    SM Frequency                    Ghz         2.24
    Elapsed Cycles                cycle        4,015
    Memory Throughput                 %         2.41
    DRAM Throughput                   %         2.41
    Duration                         us         1.79
    L1/TEX Cache Throughput           %         3.56
    L2 Cache Throughput               %         0.50
    SM Active Cycles              cycle       336.76
    Compute (SM) Throughput           %         0.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.15
    Achieved Active Warps Per SM           warp         7.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.85%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          376
    Total DRAM Elapsed Cycles        cycle       62,464
    Average L1 Active Cycles         cycle       336.76
    Total L1 Elapsed Cycles          cycle      115,664
    Average L2 Active Cycles         cycle       475.88
    Total L2 Elapsed Cycles          cycle       53,072
    Average SM Active Cycles         cycle       336.76
    Total SM Elapsed Cycles          cycle      115,664
    Average SMSP Active Cycles       cycle       306.30
    Total SMSP Elapsed Cycles        cycle      462,656
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.015%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.87% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.585%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.13% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.015%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.87% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.157%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 56.86% above the average, while the minimum instance value is 91.17% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      100,648
    Memory Throughput                 %        77.67
    DRAM Throughput                   %        77.67
    Duration                         us        44.16
    L1/TEX Cache Throughput           %        25.96
    L2 Cache Throughput               %        48.60
    SM Active Cycles              cycle    59,654.62
    Compute (SM) Throughput           %        31.58
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       123.65
    Achieved Active Warps Per SM           warp        59.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      307,984
    Total DRAM Elapsed Cycles        cycle    1,586,176
    Average L1 Active Cycles         cycle    59,654.62
    Total L1 Elapsed Cycles          cycle    3,489,046
    Average L2 Active Cycles         cycle    79,977.12
    Total L2 Elapsed Cycles          cycle    1,338,528
    Average SM Active Cycles         cycle    59,654.62
    Total SM Elapsed Cycles          cycle    3,489,046
    Average SMSP Active Cycles       cycle    96,791.18
    Total SMSP Elapsed Cycles        cycle   13,956,184
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       77,820
    Memory Throughput                 %        91.96
    DRAM Throughput                   %        91.96
    Duration                         us        34.05
    L1/TEX Cache Throughput           %        14.26
    L2 Cache Throughput               %        19.04
    SM Active Cycles              cycle    62,086.62
    Compute (SM) Throughput           %        20.44
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        86.09
    Achieved Active Warps Per SM           warp        41.32
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 13.91%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (86.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      281,092
    Total DRAM Elapsed Cycles        cycle    1,222,656
    Average L1 Active Cycles         cycle    62,086.62
    Total L1 Elapsed Cycles          cycle    2,749,670
    Average L2 Active Cycles         cycle    57,213.75
    Total L2 Elapsed Cycles          cycle    1,032,832
    Average SM Active Cycles         cycle    62,086.62
    Total SM Elapsed Cycles          cycle    2,749,670
    Average SMSP Active Cycles       cycle    66,236.89
    Total SMSP Elapsed Cycles        cycle   10,998,680
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.73
    SM Frequency                    Ghz         2.25
    Elapsed Cycles                cycle        3,966
    Memory Throughput                 %         2.40
    DRAM Throughput                   %         2.40
    Duration                         us         1.76
    L1/TEX Cache Throughput           %         3.68
    L2 Cache Throughput               %         0.49
    SM Active Cycles              cycle       326.38
    Compute (SM) Throughput           %         0.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.45
    Achieved Active Warps Per SM           warp         7.42
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.55%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          368
    Total DRAM Elapsed Cycles        cycle       61,440
    Average L1 Active Cycles         cycle       326.38
    Total L1 Elapsed Cycles          cycle      114,732
    Average L2 Active Cycles         cycle       392.81
    Total L2 Elapsed Cycles          cycle       52,288
    Average SM Active Cycles         cycle       326.38
    Total SM Elapsed Cycles          cycle      114,732
    Average SMSP Active Cycles       cycle       296.07
    Total SMSP Elapsed Cycles        cycle      458,928
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.861%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.94% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.427%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.26% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.861%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.94% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.197%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 43.24% above the average, while the minimum instance value is 89.31% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      104,830
    Memory Throughput                 %        77.03
    DRAM Throughput                   %        77.03
    Duration                         us        45.95
    L1/TEX Cache Throughput           %        14.70
    L2 Cache Throughput               %        46.15
    SM Active Cycles              cycle   105,358.82
    Compute (SM) Throughput           %        28.85
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.42
    Achieved Active Warps Per SM           warp        34.76
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.58%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      317,896
    Total DRAM Elapsed Cycles        cycle    1,650,688
    Average L1 Active Cycles         cycle   105,358.82
    Total L1 Elapsed Cycles          cycle    3,822,856
    Average L2 Active Cycles         cycle       85,140
    Total L2 Elapsed Cycles          cycle    1,393,488
    Average SM Active Cycles         cycle   105,358.82
    Total SM Elapsed Cycles          cycle    3,822,856
    Average SMSP Active Cycles       cycle   103,072.11
    Total SMSP Elapsed Cycles        cycle   15,291,424
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       73,604
    Memory Throughput                 %        92.04
    DRAM Throughput                   %        92.04
    Duration                         us        32.22
    L1/TEX Cache Throughput           %        21.94
    L2 Cache Throughput               %        20.06
    SM Active Cycles              cycle    35,149.79
    Compute (SM) Throughput           %        23.16
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       147.32
    Achieved Active Warps Per SM           warp        70.71
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      266,264
    Total DRAM Elapsed Cycles        cycle    1,157,120
    Average L1 Active Cycles         cycle    35,149.79
    Total L1 Elapsed Cycles          cycle    2,428,446
    Average L2 Active Cycles         cycle    62,215.75
    Total L2 Elapsed Cycles          cycle      978,000
    Average SM Active Cycles         cycle    35,149.79
    Total SM Elapsed Cycles          cycle    2,428,446
    Average SMSP Active Cycles       cycle    72,843.07
    Total SMSP Elapsed Cycles        cycle    9,713,784
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz            9
    SM Frequency                    Ghz         2.26
    Elapsed Cycles                cycle        3,466
    Memory Throughput                 %         1.91
    DRAM Throughput                   %         1.91
    Duration                         us         1.54
    L1/TEX Cache Throughput           %         3.53
    L2 Cache Throughput               %         0.55
    SM Active Cycles              cycle       339.50
    Compute (SM) Throughput           %         0.15
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.14
    Achieved Active Warps Per SM           warp         7.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.86%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          264
    Total DRAM Elapsed Cycles        cycle       55,296
    Average L1 Active Cycles         cycle       339.50
    Total L1 Elapsed Cycles          cycle      327,798
    Average L2 Active Cycles         cycle       399.31
    Total L2 Elapsed Cycles          cycle       45,728
    Average SM Active Cycles         cycle       339.50
    Total SM Elapsed Cycles          cycle      327,798
    Average SMSP Active Cycles       cycle       306.90
    Total SMSP Elapsed Cycles        cycle    1,311,192
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.201%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 44.39% above the average, while the minimum instance value is 64.69% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       99,964
    Memory Throughput                 %        76.60
    DRAM Throughput                   %        76.60
    Duration                         us        43.81
    L1/TEX Cache Throughput           %        15.65
    L2 Cache Throughput               %        46.63
    SM Active Cycles              cycle    98,986.71
    Compute (SM) Throughput           %        32.52
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.65
    Achieved Active Warps Per SM           warp        35.35
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.35%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      301,388
    Total DRAM Elapsed Cycles        cycle    1,573,888
    Average L1 Active Cycles         cycle    98,986.71
    Total L1 Elapsed Cycles          cycle    3,393,418
    Average L2 Active Cycles         cycle    83,761.12
    Total L2 Elapsed Cycles          cycle    1,329,024
    Average SM Active Cycles         cycle    98,986.71
    Total SM Elapsed Cycles          cycle    3,393,418
    Average SMSP Active Cycles       cycle   101,697.35
    Total SMSP Elapsed Cycles        cycle   13,573,672
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       68,838
    Memory Throughput                 %        92.43
    DRAM Throughput                   %        92.43
    Duration                         us        30.21
    L1/TEX Cache Throughput           %        16.44
    L2 Cache Throughput               %        21.44
    SM Active Cycles              cycle    89,831.32
    Compute (SM) Throughput           %        23.58
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        56.20
    Achieved Active Warps Per SM           warp        26.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.8%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (56.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      250,816
    Total DRAM Elapsed Cycles        cycle    1,085,440
    Average L1 Active Cycles         cycle    89,831.32
    Total L1 Elapsed Cycles          cycle    2,385,798
    Average L2 Active Cycles         cycle    54,683.44
    Total L2 Elapsed Cycles          cycle      915,440
    Average SM Active Cycles         cycle    89,831.32
    Total SM Elapsed Cycles          cycle    2,385,798
    Average SMSP Active Cycles       cycle       63,256
    Total SMSP Elapsed Cycles        cycle    9,543,192
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.50
    SM Frequency                    Ghz         2.21
    Elapsed Cycles                cycle        3,396
    Memory Throughput                 %         2.63
    DRAM Throughput                   %         2.63
    Duration                         us         1.54
    L1/TEX Cache Throughput           %         3.60
    L2 Cache Throughput               %         0.57
    SM Active Cycles              cycle       333.26
    Compute (SM) Throughput           %         0.42
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.43
    Achieved Active Warps Per SM           warp         7.41
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.57%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          344
    Total DRAM Elapsed Cycles        cycle       52,224
    Average L1 Active Cycles         cycle       333.26
    Total L1 Elapsed Cycles          cycle      119,384
    Average L2 Active Cycles         cycle       419.94
    Total L2 Elapsed Cycles          cycle       44,816
    Average SM Active Cycles         cycle       333.26
    Total SM Elapsed Cycles          cycle      119,384
    Average SMSP Active Cycles       cycle       301.71
    Total SMSP Elapsed Cycles        cycle      477,536
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.729%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.89% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.286%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.16% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.729%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.89% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.142%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 47.64% above the average, while the minimum instance value is 90.00% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      100,690
    Memory Throughput                 %        75.02
    DRAM Throughput                   %        75.02
    Duration                         us        44.16
    L1/TEX Cache Throughput           %        14.74
    L2 Cache Throughput               %        47.10
    SM Active Cycles              cycle   105,086.26
    Compute (SM) Throughput           %        29.34
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.47
    Achieved Active Warps Per SM           warp        33.35
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 30.53%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      297,668
    Total DRAM Elapsed Cycles        cycle    1,587,200
    Average L1 Active Cycles         cycle   105,086.26
    Total L1 Elapsed Cycles          cycle    3,762,608
    Average L2 Active Cycles         cycle    86,137.38
    Total L2 Elapsed Cycles          cycle    1,339,056
    Average SM Active Cycles         cycle   105,086.26
    Total SM Elapsed Cycles          cycle    3,762,608
    Average SMSP Active Cycles       cycle   104,281.31
    Total SMSP Elapsed Cycles        cycle   15,050,432
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.27
    Elapsed Cycles                cycle       66,297
    Memory Throughput                 %        92.44
    DRAM Throughput                   %        92.44
    Duration                         us        29.12
    L1/TEX Cache Throughput           %        16.71
    L2 Cache Throughput               %        22.29
    SM Active Cycles              cycle    66,275.29
    Compute (SM) Throughput           %        23.97
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.84
    Achieved Active Warps Per SM           warp        34.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.16%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      241,848
    Total DRAM Elapsed Cycles        cycle    1,046,528
    Average L1 Active Cycles         cycle    66,275.29
    Total L1 Elapsed Cycles          cycle    2,348,292
    Average L2 Active Cycles         cycle    29,344.25
    Total L2 Elapsed Cycles          cycle      881,616
    Average SM Active Cycles         cycle    66,275.29
    Total SM Elapsed Cycles          cycle    2,348,292
    Average SMSP Active Cycles       cycle    35,391.82
    Total SMSP Elapsed Cycles        cycle    9,393,168
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.27
    Elapsed Cycles                cycle       96,144
    Memory Throughput                 %        71.44
    DRAM Throughput                   %        71.44
    Duration                         us        42.40
    L1/TEX Cache Throughput           %        17.61
    L2 Cache Throughput               %        46.57
    SM Active Cycles              cycle    87,972.56
    Compute (SM) Throughput           %        26.45
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.90
    Achieved Active Warps Per SM           warp        37.39
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.1%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      272,144
    Total DRAM Elapsed Cycles        cycle    1,523,712
    Average L1 Active Cycles         cycle    87,972.56
    Total L1 Elapsed Cycles          cycle    4,174,382
    Average L2 Active Cycles         cycle    80,373.88
    Total L2 Elapsed Cycles          cycle    1,282,160
    Average SM Active Cycles         cycle    87,972.56
    Total SM Elapsed Cycles          cycle    4,174,382
    Average SMSP Active Cycles       cycle    97,151.38
    Total SMSP Elapsed Cycles        cycle   16,697,528
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       67,076
    Memory Throughput                 %        91.47
    DRAM Throughput                   %        91.47
    Duration                         us        29.44
    L1/TEX Cache Throughput           %        17.04
    L2 Cache Throughput               %        22.02
    SM Active Cycles              cycle    62,746.32
    Compute (SM) Throughput           %        24.47
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.64
    Achieved Active Warps Per SM           warp        36.31
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.36%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      241,900
    Total DRAM Elapsed Cycles        cycle    1,057,792
    Average L1 Active Cycles         cycle    62,746.32
    Total L1 Elapsed Cycles          cycle    2,302,474
    Average L2 Active Cycles         cycle    29,073.31
    Total L2 Elapsed Cycles          cycle      891,648
    Average SM Active Cycles         cycle    62,746.32
    Total SM Elapsed Cycles          cycle    2,302,474
    Average SMSP Active Cycles       cycle    35,118.22
    Total SMSP Elapsed Cycles        cycle    9,209,896
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       95,902
    Memory Throughput                 %        79.57
    DRAM Throughput                   %        79.57
    Duration                         us        42.11
    L1/TEX Cache Throughput           %        25.59
    L2 Cache Throughput               %        47.44
    SM Active Cycles              cycle    60,526.59
    Compute (SM) Throughput           %        31.65
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       113.58
    Achieved Active Warps Per SM           warp        54.52
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      300,860
    Total DRAM Elapsed Cycles        cycle    1,512,448
    Average L1 Active Cycles         cycle    60,526.59
    Total L1 Elapsed Cycles          cycle    3,491,616
    Average L2 Active Cycles         cycle    90,997.38
    Total L2 Elapsed Cycles          cycle    1,275,888
    Average SM Active Cycles         cycle    60,526.59
    Total SM Elapsed Cycles          cycle    3,491,616
    Average SMSP Active Cycles       cycle   110,150.60
    Total SMSP Elapsed Cycles        cycle   13,966,464
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       75,533
    Memory Throughput                 %        82.20
    DRAM Throughput                   %        82.20
    Duration                         us        33.06
    L1/TEX Cache Throughput           %        16.84
    L2 Cache Throughput               %        24.04
    SM Active Cycles              cycle    64,893.85
    Compute (SM) Throughput           %        24.20
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        81.61
    Achieved Active Warps Per SM           warp        39.17
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 18.39%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (81.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      244,316
    Total DRAM Elapsed Cycles        cycle    1,188,864
    Average L1 Active Cycles         cycle    64,893.85
    Total L1 Elapsed Cycles          cycle    2,329,164
    Average L2 Active Cycles         cycle    55,309.62
    Total L2 Elapsed Cycles          cycle    1,003,200
    Average SM Active Cycles         cycle    64,893.85
    Total SM Elapsed Cycles          cycle    2,329,164
    Average SMSP Active Cycles       cycle    63,460.16
    Total SMSP Elapsed Cycles        cycle    9,316,656
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.80
    SM Frequency                    Ghz         2.25
    Elapsed Cycles                cycle        3,607
    Memory Throughput                 %         2.36
    DRAM Throughput                   %         2.36
    Duration                         us         1.60
    L1/TEX Cache Throughput           %         3.55
    L2 Cache Throughput               %         0.52
    SM Active Cycles              cycle       338.26
    Compute (SM) Throughput           %         0.43
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.58
    Achieved Active Warps Per SM           warp         7.48
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.42%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          332
    Total DRAM Elapsed Cycles        cycle       56,320
    Average L1 Active Cycles         cycle       338.26
    Total L1 Elapsed Cycles          cycle      118,012
    Average L2 Active Cycles         cycle       450.50
    Total L2 Elapsed Cycles          cycle       47,616
    Average SM Active Cycles         cycle       338.26
    Total SM Elapsed Cycles          cycle      118,012
    Average SMSP Active Cycles       cycle       306.30
    Total SMSP Elapsed Cycles        cycle      472,048
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.906%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.86% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.462%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.23% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.906%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.86% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.971%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 46.05% above the average, while the minimum instance value is 90.68% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      103,907
    Memory Throughput                 %        76.62
    DRAM Throughput                   %        76.62
    Duration                         us        45.50
    L1/TEX Cache Throughput           %        16.28
    L2 Cache Throughput               %        47.24
    SM Active Cycles              cycle    95,158.38
    Compute (SM) Throughput           %        26.76
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        80.20
    Achieved Active Warps Per SM           warp        38.50
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 19.8%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (80.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      313,444
    Total DRAM Elapsed Cycles        cycle    1,636,352
    Average L1 Active Cycles         cycle    95,158.38
    Total L1 Elapsed Cycles          cycle    4,133,000
    Average L2 Active Cycles         cycle    80,980.69
    Total L2 Elapsed Cycles          cycle    1,380,976
    Average SM Active Cycles         cycle    95,158.38
    Total SM Elapsed Cycles          cycle    4,133,000
    Average SMSP Active Cycles       cycle    97,959.44
    Total SMSP Elapsed Cycles        cycle   16,532,000
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       70,667
    Memory Throughput                 %        92.18
    DRAM Throughput                   %        92.18
    Duration                         us        30.98
    L1/TEX Cache Throughput           %        16.85
    L2 Cache Throughput               %        20.95
    SM Active Cycles              cycle    66,246.53
    Compute (SM) Throughput           %        24.22
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.20
    Achieved Active Warps Per SM           warp        36.58
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.8%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      256,504
    Total DRAM Elapsed Cycles        cycle    1,113,088
    Average L1 Active Cycles         cycle    66,246.53
    Total L1 Elapsed Cycles          cycle    2,327,760
    Average L2 Active Cycles         cycle    55,644.69
    Total L2 Elapsed Cycles          cycle      938,496
    Average SM Active Cycles         cycle    66,246.53
    Total SM Elapsed Cycles          cycle    2,327,760
    Average SMSP Active Cycles       cycle    64,571.80
    Total SMSP Elapsed Cycles        cycle    9,311,040
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.27
    Elapsed Cycles                cycle       96,266
    Memory Throughput                 %        78.88
    DRAM Throughput                   %        78.88
    Duration                         us        42.30
    L1/TEX Cache Throughput           %        25.62
    L2 Cache Throughput               %        48.74
    SM Active Cycles              cycle    60,443.12
    Compute (SM) Throughput           %        33.60
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       117.01
    Achieved Active Warps Per SM           warp        56.16
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      299,664
    Total DRAM Elapsed Cycles        cycle    1,519,616
    Average L1 Active Cycles         cycle    60,443.12
    Total L1 Elapsed Cycles          cycle    3,293,740
    Average L2 Active Cycles         cycle    77,632.94
    Total L2 Elapsed Cycles          cycle    1,281,424
    Average SM Active Cycles         cycle    60,443.12
    Total SM Elapsed Cycles          cycle    3,293,740
    Average SMSP Active Cycles       cycle    93,875.83
    Total SMSP Elapsed Cycles        cycle   13,174,960
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       65,920
    Memory Throughput                 %        92.44
    DRAM Throughput                   %        92.44
    Duration                         us        28.93
    L1/TEX Cache Throughput           %        16.84
    L2 Cache Throughput               %        22.43
    SM Active Cycles              cycle    65,503.12
    Compute (SM) Throughput           %        24.24
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.35
    Achieved Active Warps Per SM           warp        35.21
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.65%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      240,444
    Total DRAM Elapsed Cycles        cycle    1,040,384
    Average L1 Active Cycles         cycle    65,503.12
    Total L1 Elapsed Cycles          cycle    2,328,360
    Average L2 Active Cycles         cycle    54,724.44
    Total L2 Elapsed Cycles          cycle      876,096
    Average SM Active Cycles         cycle    65,503.12
    Total SM Elapsed Cycles          cycle    2,328,360
    Average SMSP Active Cycles       cycle    63,623.64
    Total SMSP Elapsed Cycles        cycle    9,313,440
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      100,050
    Memory Throughput                 %        74.96
    DRAM Throughput                   %        74.96
    Duration                         us        43.90
    L1/TEX Cache Throughput           %        16.13
    L2 Cache Throughput               %        46.82
    SM Active Cycles              cycle    95,994.53
    Compute (SM) Throughput           %        33.54
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.14
    Achieved Active Warps Per SM           warp        36.55
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.86%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      295,516
    Total DRAM Elapsed Cycles        cycle    1,576,960
    Average L1 Active Cycles         cycle    95,994.53
    Total L1 Elapsed Cycles          cycle    3,302,598
    Average L2 Active Cycles         cycle    80,713.94
    Total L2 Elapsed Cycles          cycle    1,330,656
    Average SM Active Cycles         cycle    95,994.53
    Total SM Elapsed Cycles          cycle    3,302,598
    Average SMSP Active Cycles       cycle    97,810.85
    Total SMSP Elapsed Cycles        cycle   13,210,392
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       71,227
    Memory Throughput                 %        91.85
    DRAM Throughput                   %        91.85
    Duration                         us        31.26
    L1/TEX Cache Throughput           %        16.04
    L2 Cache Throughput               %        20.78
    SM Active Cycles              cycle    72,696.44
    Compute (SM) Throughput           %        23.11
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.27
    Achieved Active Warps Per SM           warp        33.73
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.73%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      257,708
    Total DRAM Elapsed Cycles        cycle    1,122,304
    Average L1 Active Cycles         cycle    72,696.44
    Total L1 Elapsed Cycles          cycle    2,444,554
    Average L2 Active Cycles         cycle    58,158.31
    Total L2 Elapsed Cycles          cycle      947,408
    Average SM Active Cycles         cycle    72,696.44
    Total SM Elapsed Cycles          cycle    2,444,554
    Average SMSP Active Cycles       cycle    67,843.20
    Total SMSP Elapsed Cycles        cycle    9,778,216
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.68
    SM Frequency                    Ghz         2.25
    Elapsed Cycles                cycle        3,393
    Memory Throughput                 %         2.02
    DRAM Throughput                   %         2.02
    Duration                         us         1.50
    L1/TEX Cache Throughput           %         3.59
    L2 Cache Throughput               %         0.58
    SM Active Cycles              cycle       334.50
    Compute (SM) Throughput           %         0.42
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.03
    Achieved Active Warps Per SM           warp         7.22
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.97%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          264
    Total DRAM Elapsed Cycles        cycle       52,224
    Average L1 Active Cycles         cycle       334.50
    Total L1 Elapsed Cycles          cycle      116,662
    Average L2 Active Cycles         cycle       399.81
    Total L2 Elapsed Cycles          cycle       44,720
    Average SM Active Cycles         cycle       334.50
    Total SM Elapsed Cycles          cycle      116,662
    Average SMSP Active Cycles       cycle       296.39
    Total SMSP Elapsed Cycles        cycle      466,648
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.911%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.89% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.317%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.13% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.911%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.89% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.502%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 45.46% above the average, while the minimum instance value is 89.50% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.25
    Elapsed Cycles                cycle      106,995
    Memory Throughput                 %        76.25
    DRAM Throughput                   %        76.25
    Duration                         us        47.42
    L1/TEX Cache Throughput           %        16.68
    L2 Cache Throughput               %        46.61
    SM Active Cycles              cycle    92,850.91
    Compute (SM) Throughput           %        30.03
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        84.55
    Achieved Active Warps Per SM           warp        40.58
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 15.45%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (84.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      324,796
    Total DRAM Elapsed Cycles        cycle    1,703,936
    Average L1 Active Cycles         cycle    92,850.91
    Total L1 Elapsed Cycles          cycle    3,692,212
    Average L2 Active Cycles         cycle    81,457.38
    Total L2 Elapsed Cycles          cycle    1,429,056
    Average SM Active Cycles         cycle    92,850.91
    Total SM Elapsed Cycles          cycle    3,692,212
    Average SMSP Active Cycles       cycle    97,505.97
    Total SMSP Elapsed Cycles        cycle   14,768,848
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       67,836
    Memory Throughput                 %        92.33
    DRAM Throughput                   %        92.33
    Duration                         us        29.76
    L1/TEX Cache Throughput           %        17.06
    L2 Cache Throughput               %        21.75
    SM Active Cycles              cycle    65,891.62
    Compute (SM) Throughput           %        24.58
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.87
    Achieved Active Warps Per SM           warp        34.98
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.13%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      246,772
    Total DRAM Elapsed Cycles        cycle    1,069,056
    Average L1 Active Cycles         cycle    65,891.62
    Total L1 Elapsed Cycles          cycle    2,299,082
    Average L2 Active Cycles         cycle    50,734.12
    Total L2 Elapsed Cycles          cycle      902,224
    Average SM Active Cycles         cycle    65,891.62
    Total SM Elapsed Cycles          cycle    2,299,082
    Average SMSP Active Cycles       cycle    58,502.35
    Total SMSP Elapsed Cycles        cycle    9,196,328
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.27
    Elapsed Cycles                cycle      101,048
    Memory Throughput                 %        73.60
    DRAM Throughput                   %        73.60
    Duration                         us        44.58
    L1/TEX Cache Throughput           %        26.13
    L2 Cache Throughput               %        46.65
    SM Active Cycles              cycle    59,272.41
    Compute (SM) Throughput           %        33.30
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       122.84
    Achieved Active Warps Per SM           warp        58.96
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      294,688
    Total DRAM Elapsed Cycles        cycle    1,601,536
    Average L1 Active Cycles         cycle    59,272.41
    Total L1 Elapsed Cycles          cycle    3,331,748
    Average L2 Active Cycles         cycle    77,745.31
    Total L2 Elapsed Cycles          cycle    1,348,112
    Average SM Active Cycles         cycle    59,272.41
    Total SM Elapsed Cycles          cycle    3,331,748
    Average SMSP Active Cycles       cycle    94,078.48
    Total SMSP Elapsed Cycles        cycle   13,326,992
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.26
    Elapsed Cycles                cycle       67,156
    Memory Throughput                 %        92.45
    DRAM Throughput                   %        92.45
    Duration                         us        29.66
    L1/TEX Cache Throughput           %        16.09
    L2 Cache Throughput               %        21.92
    SM Active Cycles              cycle    60,996.47
    Compute (SM) Throughput           %        23.20
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.45
    Achieved Active Warps Per SM           warp        38.14
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.55%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      246,144
    Total DRAM Elapsed Cycles        cycle    1,064,960
    Average L1 Active Cycles         cycle    60,996.47
    Total L1 Elapsed Cycles          cycle    2,437,018
    Average L2 Active Cycles         cycle    57,490.94
    Total L2 Elapsed Cycles          cycle      895,808
    Average SM Active Cycles         cycle    60,996.47
    Total SM Elapsed Cycles          cycle    2,437,018
    Average SMSP Active Cycles       cycle    66,677.12
    Total SMSP Elapsed Cycles        cycle    9,748,072
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       99,001
    Memory Throughput                 %        75.75
    DRAM Throughput                   %        75.75
    Duration                         us        43.42
    L1/TEX Cache Throughput           %        15.86
    L2 Cache Throughput               %        46.50
    SM Active Cycles              cycle    97,658.62
    Compute (SM) Throughput           %        31.86
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.56
    Achieved Active Warps Per SM           warp        35.31
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.44%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      295,548
    Total DRAM Elapsed Cycles        cycle    1,560,576
    Average L1 Active Cycles         cycle    97,658.62
    Total L1 Elapsed Cycles          cycle    3,485,074
    Average L2 Active Cycles         cycle    83,473.19
    Total L2 Elapsed Cycles          cycle    1,317,248
    Average SM Active Cycles         cycle    97,658.62
    Total SM Elapsed Cycles          cycle    3,485,074
    Average SMSP Active Cycles       cycle   101,138.87
    Total SMSP Elapsed Cycles        cycle   13,940,296
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       71,432
    Memory Throughput                 %        92.45
    DRAM Throughput                   %        92.45
    Duration                         us        31.33
    L1/TEX Cache Throughput           %        16.16
    L2 Cache Throughput               %        20.69
    SM Active Cycles              cycle    64,191.59
    Compute (SM) Throughput           %        23.31
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        81.04
    Achieved Active Warps Per SM           warp        38.90
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 18.96%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (81.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      259,872
    Total DRAM Elapsed Cycles        cycle    1,124,352
    Average L1 Active Cycles         cycle    64,191.59
    Total L1 Elapsed Cycles          cycle    2,427,392
    Average L2 Active Cycles         cycle    63,508.06
    Total L2 Elapsed Cycles          cycle      949,392
    Average SM Active Cycles         cycle    64,191.59
    Total SM Elapsed Cycles          cycle    2,427,392
    Average SMSP Active Cycles       cycle    74,533.15
    Total SMSP Elapsed Cycles        cycle    9,709,568
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.67
    SM Frequency                    Ghz         2.24
    Elapsed Cycles                cycle        3,448
    Memory Throughput                 %         2.22
    DRAM Throughput                   %         2.22
    Duration                         us         1.54
    L1/TEX Cache Throughput           %         3.54
    L2 Cache Throughput               %         0.56
    SM Active Cycles              cycle       338.91
    Compute (SM) Throughput           %         0.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.11
    Achieved Active Warps Per SM           warp         7.25
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.89%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          296
    Total DRAM Elapsed Cycles        cycle       53,248
    Average L1 Active Cycles         cycle       338.91
    Total L1 Elapsed Cycles          cycle      114,426
    Average L2 Active Cycles         cycle       402.94
    Total L2 Elapsed Cycles          cycle       45,376
    Average SM Active Cycles         cycle       338.91
    Total SM Elapsed Cycles          cycle      114,426
    Average SMSP Active Cycles       cycle       311.21
    Total SMSP Elapsed Cycles        cycle      457,704
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.138%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.88% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.762%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.12% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.138%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.88% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.847%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 55.23% above the average, while the minimum instance value is 89.58% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      111,848
    Memory Throughput                 %        78.53
    DRAM Throughput                   %        78.53
    Duration                         us        48.93
    L1/TEX Cache Throughput           %        15.98
    L2 Cache Throughput               %        47.23
    SM Active Cycles              cycle    96,891.18
    Compute (SM) Throughput           %        33.14
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        84.66
    Achieved Active Warps Per SM           warp        40.64
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 15.34%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (84.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      344,972
    Total DRAM Elapsed Cycles        cycle    1,757,184
    Average L1 Active Cycles         cycle    96,891.18
    Total L1 Elapsed Cycles          cycle    3,353,674
    Average L2 Active Cycles         cycle    74,595.06
    Total L2 Elapsed Cycles          cycle    1,485,184
    Average SM Active Cycles         cycle    96,891.18
    Total SM Elapsed Cycles          cycle    3,353,674
    Average SMSP Active Cycles       cycle    89,926.97
    Total SMSP Elapsed Cycles        cycle   13,414,696
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       72,921
    Memory Throughput                 %        79.61
    DRAM Throughput                   %        79.61
    Duration                         us        31.87
    L1/TEX Cache Throughput           %        17.66
    L2 Cache Throughput               %        25.48
    SM Active Cycles              cycle    43,665.65
    Compute (SM) Throughput           %        22.16
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       107.70
    Achieved Active Warps Per SM           warp        51.70
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      227,840
    Total DRAM Elapsed Cycles        cycle    1,144,832
    Average L1 Active Cycles         cycle    43,665.65
    Total L1 Elapsed Cycles          cycle    2,555,396
    Average L2 Active Cycles         cycle    58,558.19
    Total L2 Elapsed Cycles          cycle      966,848
    Average SM Active Cycles         cycle    43,665.65
    Total SM Elapsed Cycles          cycle    2,555,396
    Average SMSP Active Cycles       cycle    68,385.87
    Total SMSP Elapsed Cycles        cycle   10,221,584
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       98,006
    Memory Throughput                 %        75.52
    DRAM Throughput                   %        75.52
    Duration                         us        43.04
    L1/TEX Cache Throughput           %        15.39
    L2 Cache Throughput               %        47.27
    SM Active Cycles              cycle   102,604.15
    Compute (SM) Throughput           %        33.44
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.10
    Achieved Active Warps Per SM           warp        33.17
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 30.9%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      291,940
    Total DRAM Elapsed Cycles        cycle    1,546,240
    Average L1 Active Cycles         cycle   102,604.15
    Total L1 Elapsed Cycles          cycle    3,325,782
    Average L2 Active Cycles         cycle    79,636.06
    Total L2 Elapsed Cycles          cycle    1,304,128
    Average SM Active Cycles         cycle   102,604.15
    Total SM Elapsed Cycles          cycle    3,325,782
    Average SMSP Active Cycles       cycle    96,334.96
    Total SMSP Elapsed Cycles        cycle   13,303,128
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.96
    SM Frequency                    Ghz         2.27
    Elapsed Cycles                cycle       68,364
    Memory Throughput                 %        92.33
    DRAM Throughput                   %        92.33
    Duration                         us        30.05
    L1/TEX Cache Throughput           %        21.88
    L2 Cache Throughput               %        21.58
    SM Active Cycles              cycle    35,237.47
    Compute (SM) Throughput           %        24.58
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       138.47
    Achieved Active Warps Per SM           warp        66.47
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      248,660
    Total DRAM Elapsed Cycles        cycle    1,077,248
    Average L1 Active Cycles         cycle    35,237.47
    Total L1 Elapsed Cycles          cycle    2,304,848
    Average L2 Active Cycles         cycle    54,173.25
    Total L2 Elapsed Cycles          cycle      909,824
    Average SM Active Cycles         cycle    35,237.47
    Total SM Elapsed Cycles          cycle    2,304,848
    Average SMSP Active Cycles       cycle    63,102.22
    Total SMSP Elapsed Cycles        cycle    9,219,392
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.87
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle        3,366
    Memory Throughput                 %         3.22
    DRAM Throughput                   %         3.22
    Duration                         us         1.47
    L1/TEX Cache Throughput           %         3.60
    L2 Cache Throughput               %         0.56
    SM Active Cycles              cycle       333.24
    Compute (SM) Throughput           %         0.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.83
    Achieved Active Warps Per SM           warp         7.12
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.17%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (14.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          420
    Total DRAM Elapsed Cycles        cycle       52,224
    Average L1 Active Cycles         cycle       333.24
    Total L1 Elapsed Cycles          cycle      114,644
    Average L2 Active Cycles         cycle       391.88
    Total L2 Elapsed Cycles          cycle       44,416
    Average SM Active Cycles         cycle       333.24
    Total SM Elapsed Cycles          cycle      114,644
    Average SMSP Active Cycles       cycle       300.54
    Total SMSP Elapsed Cycles        cycle      458,576
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.007%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.90% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.53%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.26% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.007%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.90% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.765%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 55.01% above the average, while the minimum instance value is 89.28% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      104,401
    Memory Throughput                 %        77.11
    DRAM Throughput                   %        77.11
    Duration                         us        45.76
    L1/TEX Cache Throughput           %        15.99
    L2 Cache Throughput               %        47.24
    SM Active Cycles              cycle    96,839.79
    Compute (SM) Throughput           %        31.66
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.43
    Achieved Active Warps Per SM           warp        38.13
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.57%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      317,044
    Total DRAM Elapsed Cycles        cycle    1,644,544
    Average L1 Active Cycles         cycle    96,839.79
    Total L1 Elapsed Cycles          cycle    3,515,922
    Average L2 Active Cycles         cycle       84,386
    Total L2 Elapsed Cycles          cycle    1,387,936
    Average SM Active Cycles         cycle    96,839.79
    Total SM Elapsed Cycles          cycle    3,515,922
    Average SMSP Active Cycles       cycle   102,189.06
    Total SMSP Elapsed Cycles        cycle   14,063,688
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       68,693
    Memory Throughput                 %        92.35
    DRAM Throughput                   %        92.35
    Duration                         us        30.11
    L1/TEX Cache Throughput           %        16.23
    L2 Cache Throughput               %        21.50
    SM Active Cycles              cycle    63,417.53
    Compute (SM) Throughput           %        23.46
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.26
    Achieved Active Warps Per SM           warp        37.08
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.74%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      249,664
    Total DRAM Elapsed Cycles        cycle    1,081,344
    Average L1 Active Cycles         cycle    63,417.53
    Total L1 Elapsed Cycles          cycle    2,416,672
    Average L2 Active Cycles         cycle    53,591.06
    Total L2 Elapsed Cycles          cycle      912,848
    Average SM Active Cycles         cycle    63,417.53
    Total SM Elapsed Cycles          cycle    2,416,672
    Average SMSP Active Cycles       cycle    62,157.89
    Total SMSP Elapsed Cycles        cycle    9,666,688
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.67
    SM Frequency                    Ghz         2.25
    Elapsed Cycles                cycle        3,451
    Memory Throughput                 %         1.71
    DRAM Throughput                   %         1.71
    Duration                         us         1.54
    L1/TEX Cache Throughput           %         3.49
    L2 Cache Throughput               %         0.55
    SM Active Cycles              cycle       343.41
    Compute (SM) Throughput           %         0.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.16
    Achieved Active Warps Per SM           warp         7.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          228
    Total DRAM Elapsed Cycles        cycle       53,248
    Average L1 Active Cycles         cycle       343.41
    Total L1 Elapsed Cycles          cycle      113,960
    Average L2 Active Cycles         cycle       445.56
    Total L2 Elapsed Cycles          cycle       45,520
    Average SM Active Cycles         cycle       343.41
    Total SM Elapsed Cycles          cycle      113,960
    Average SMSP Active Cycles       cycle       308.12
    Total SMSP Elapsed Cycles        cycle      455,840
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.264%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.90% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.723%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.14% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.264%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.90% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.552%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 41.83% above the average, while the minimum instance value is 90.57% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.27
    Elapsed Cycles                cycle       98,631
    Memory Throughput                 %        74.63
    DRAM Throughput                   %        74.63
    Duration                         us        43.33
    L1/TEX Cache Throughput           %        25.99
    L2 Cache Throughput               %        46.67
    SM Active Cycles              cycle    59,583.91
    Compute (SM) Throughput           %        33.68
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       119.95
    Achieved Active Warps Per SM           warp        57.57
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      290,400
    Total DRAM Elapsed Cycles        cycle    1,556,480
    Average L1 Active Cycles         cycle    59,583.91
    Total L1 Elapsed Cycles          cycle    3,306,904
    Average L2 Active Cycles         cycle    78,510.31
    Total L2 Elapsed Cycles          cycle    1,313,024
    Average SM Active Cycles         cycle    59,583.91
    Total SM Elapsed Cycles          cycle    3,306,904
    Average SMSP Active Cycles       cycle    94,529.01
    Total SMSP Elapsed Cycles        cycle   13,227,616
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       73,371
    Memory Throughput                 %        92.43
    DRAM Throughput                   %        92.43
    Duration                         us        32.19
    L1/TEX Cache Throughput           %        19.55
    L2 Cache Throughput               %        20.17
    SM Active Cycles              cycle    39,433.26
    Compute (SM) Throughput           %        20.40
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       138.05
    Achieved Active Warps Per SM           warp        66.26
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      266,920
    Total DRAM Elapsed Cycles        cycle    1,155,072
    Average L1 Active Cycles         cycle    39,433.26
    Total L1 Elapsed Cycles          cycle    2,781,132
    Average L2 Active Cycles         cycle    59,148.38
    Total L2 Elapsed Cycles          cycle      975,472
    Average SM Active Cycles         cycle    39,433.26
    Total SM Elapsed Cycles          cycle    2,781,132
    Average SMSP Active Cycles       cycle    68,331.79
    Total SMSP Elapsed Cycles        cycle   11,124,528
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      100,445
    Memory Throughput                 %        74.93
    DRAM Throughput                   %        74.93
    Duration                         us        44.06
    L1/TEX Cache Throughput           %        17.40
    L2 Cache Throughput               %        47.29
    SM Active Cycles              cycle    89,012.65
    Compute (SM) Throughput           %        33.06
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.27
    Achieved Active Warps Per SM           warp        39.49
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 17.73%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      296,556
    Total DRAM Elapsed Cycles        cycle    1,583,104
    Average L1 Active Cycles         cycle    89,012.65
    Total L1 Elapsed Cycles          cycle    3,373,246
    Average L2 Active Cycles         cycle    80,635.38
    Total L2 Elapsed Cycles          cycle    1,336,400
    Average SM Active Cycles         cycle    89,012.65
    Total SM Elapsed Cycles          cycle    3,373,246
    Average SMSP Active Cycles       cycle    97,311.79
    Total SMSP Elapsed Cycles        cycle   13,492,984
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       68,936
    Memory Throughput                 %        91.46
    DRAM Throughput                   %        91.46
    Duration                         us        30.27
    L1/TEX Cache Throughput           %        16.83
    L2 Cache Throughput               %        21.42
    SM Active Cycles              cycle    59,805.59
    Compute (SM) Throughput           %        24.35
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.38
    Achieved Active Warps Per SM           warp        39.54
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 17.62%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      248,420
    Total DRAM Elapsed Cycles        cycle    1,086,464
    Average L1 Active Cycles         cycle    59,805.59
    Total L1 Elapsed Cycles          cycle    2,331,458
    Average L2 Active Cycles         cycle    52,838.31
    Total L2 Elapsed Cycles          cycle      916,784
    Average SM Active Cycles         cycle    59,805.59
    Total SM Elapsed Cycles          cycle    2,331,458
    Average SMSP Active Cycles       cycle    61,278.26
    Total SMSP Elapsed Cycles        cycle    9,325,832
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.26
    Elapsed Cycles                cycle       99,223
    Memory Throughput                 %        75.53
    DRAM Throughput                   %        75.53
    Duration                         us        43.78
    L1/TEX Cache Throughput           %        16.58
    L2 Cache Throughput               %        47.20
    SM Active Cycles              cycle    93,404.29
    Compute (SM) Throughput           %        31.75
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.47
    Achieved Active Warps Per SM           warp        37.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.53%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      297,180
    Total DRAM Elapsed Cycles        cycle    1,573,888
    Average L1 Active Cycles         cycle    93,404.29
    Total L1 Elapsed Cycles          cycle    3,514,880
    Average L2 Active Cycles         cycle    80,290.50
    Total L2 Elapsed Cycles          cycle    1,323,056
    Average SM Active Cycles         cycle    93,404.29
    Total SM Elapsed Cycles          cycle    3,514,880
    Average SMSP Active Cycles       cycle    96,377.56
    Total SMSP Elapsed Cycles        cycle   14,059,520
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.27
    Elapsed Cycles                cycle       69,078
    Memory Throughput                 %        92.08
    DRAM Throughput                   %        92.08
    Duration                         us        30.34
    L1/TEX Cache Throughput           %        16.62
    L2 Cache Throughput               %        21.37
    SM Active Cycles              cycle    63,785.47
    Compute (SM) Throughput           %        24.06
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.57
    Achieved Active Warps Per SM           warp        37.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.43%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      250,572
    Total DRAM Elapsed Cycles        cycle    1,088,512
    Average L1 Active Cycles         cycle    63,785.47
    Total L1 Elapsed Cycles          cycle    2,361,498
    Average L2 Active Cycles         cycle    29,113.62
    Total L2 Elapsed Cycles          cycle      918,288
    Average SM Active Cycles         cycle    63,785.47
    Total SM Elapsed Cycles          cycle    2,361,498
    Average SMSP Active Cycles       cycle    35,163.39
    Total SMSP Elapsed Cycles        cycle    9,445,992
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      103,533
    Memory Throughput                 %        76.96
    DRAM Throughput                   %        76.96
    Duration                         us        45.38
    L1/TEX Cache Throughput           %        15.75
    L2 Cache Throughput               %        47.25
    SM Active Cycles              cycle    98,317.12
    Compute (SM) Throughput           %        32.13
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.13
    Achieved Active Warps Per SM           warp        37.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.87%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      313,652
    Total DRAM Elapsed Cycles        cycle    1,630,208
    Average L1 Active Cycles         cycle    98,317.12
    Total L1 Elapsed Cycles          cycle    3,478,204
    Average L2 Active Cycles         cycle    83,887.38
    Total L2 Elapsed Cycles          cycle    1,376,224
    Average SM Active Cycles         cycle    98,317.12
    Total SM Elapsed Cycles          cycle    3,478,204
    Average SMSP Active Cycles       cycle   101,426.80
    Total SMSP Elapsed Cycles        cycle   13,912,816
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.97
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       67,923
    Memory Throughput                 %        92.10
    DRAM Throughput                   %        92.10
    Duration                         us        29.79
    L1/TEX Cache Throughput           %        22.04
    L2 Cache Throughput               %        21.74
    SM Active Cycles              cycle    60,694.32
    Compute (SM) Throughput           %        31.96
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.94
    Achieved Active Warps Per SM           warp        38.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.06%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      246,156
    Total DRAM Elapsed Cycles        cycle    1,069,056
    Average L1 Active Cycles         cycle    60,694.32
    Total L1 Elapsed Cycles          cycle    1,779,732
    Average L2 Active Cycles         cycle    53,363.88
    Total L2 Elapsed Cycles          cycle      902,896
    Average SM Active Cycles         cycle    60,694.32
    Total SM Elapsed Cycles          cycle    1,779,732
    Average SMSP Active Cycles       cycle    62,007.04
    Total SMSP Elapsed Cycles        cycle    7,118,928
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.86
    SM Frequency                    Ghz         2.23
    Elapsed Cycles                cycle        3,999
    Memory Throughput                 %         1.92
    DRAM Throughput                   %         1.92
    Duration                         us         1.79
    L1/TEX Cache Throughput           %         3.55
    L2 Cache Throughput               %         0.47
    SM Active Cycles              cycle       337.82
    Compute (SM) Throughput           %         0.38
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.87
    Achieved Active Warps Per SM           warp         7.14
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.13%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (14.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          304
    Total DRAM Elapsed Cycles        cycle       63,488
    Average L1 Active Cycles         cycle       337.82
    Total L1 Elapsed Cycles          cycle      115,798
    Average L2 Active Cycles         cycle       406.81
    Total L2 Elapsed Cycles          cycle       52,912
    Average SM Active Cycles         cycle       337.82
    Total SM Elapsed Cycles          cycle      115,798
    Average SMSP Active Cycles       cycle       296.30
    Total SMSP Elapsed Cycles        cycle      463,192
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.033%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.90% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.396%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.52% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.033%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.90% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.62%                                                                                           
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 45.69% above the average, while the minimum instance value is 89.68% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      103,055
    Memory Throughput                 %        73.27
    DRAM Throughput                   %        73.27
    Duration                         us        45.18
    L1/TEX Cache Throughput           %        16.27
    L2 Cache Throughput               %        47.11
    SM Active Cycles              cycle    95,218.09
    Compute (SM) Throughput           %        33.76
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.13
    Achieved Active Warps Per SM           warp        37.50
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.87%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      297,476
    Total DRAM Elapsed Cycles        cycle    1,624,064
    Average L1 Active Cycles         cycle    95,218.09
    Total L1 Elapsed Cycles          cycle    3,314,124
    Average L2 Active Cycles         cycle    75,894.50
    Total L2 Elapsed Cycles          cycle    1,370,416
    Average SM Active Cycles         cycle    95,218.09
    Total SM Elapsed Cycles          cycle    3,314,124
    Average SMSP Active Cycles       cycle    91,515.38
    Total SMSP Elapsed Cycles        cycle   13,256,496
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.98
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle       68,821
    Memory Throughput                 %        92.44
    DRAM Throughput                   %        92.44
    Duration                         us        30.18
    L1/TEX Cache Throughput           %        14.91
    L2 Cache Throughput               %        21.47
    SM Active Cycles              cycle    63,656.74
    Compute (SM) Throughput           %        21.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread       1,048,576
    Uses Green Context                                             0
    Waves Per SM                                               20.08
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.49
    Achieved Active Warps Per SM           warp        36.71
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.51%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      250,376
    Total DRAM Elapsed Cycles        cycle    1,083,392
    Average L1 Active Cycles         cycle    63,656.74
    Total L1 Elapsed Cycles          cycle    2,630,172
    Average L2 Active Cycles         cycle    63,237.62
    Total L2 Elapsed Cycles          cycle      914,304
    Average SM Active Cycles         cycle    63,656.74
    Total SM Elapsed Cycles          cycle    2,630,172
    Average SMSP Active Cycles       cycle    73,353.01
    Total SMSP Elapsed Cycles        cycle   10,520,688
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.67
    SM Frequency                    Ghz         2.21
    Elapsed Cycles                cycle        3,388
    Memory Throughput                 %         2.22
    DRAM Throughput                   %         2.22
    Duration                         us         1.54
    L1/TEX Cache Throughput           %         3.55
    L2 Cache Throughput               %         0.77
    SM Active Cycles              cycle       338.18
    Compute (SM) Throughput           %         0.42
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              34
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.05
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 70.59%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 34             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.10
    Achieved Active Warps Per SM           warp         7.25
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.9%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          296
    Total DRAM Elapsed Cycles        cycle       53,248
    Average L1 Active Cycles         cycle       338.18
    Total L1 Elapsed Cycles          cycle      119,634
    Average L2 Active Cycles         cycle       446.69
    Total L2 Elapsed Cycles          cycle       44,672
    Average SM Active Cycles         cycle       338.18
    Total SM Elapsed Cycles          cycle      119,634
    Average SMSP Active Cycles       cycle       312.04
    Total SMSP Elapsed Cycles        cycle      478,536
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.809%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 70.85% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.485%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 73.12% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.809%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.85% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.484%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 59.28% above the average, while the minimum instance value is 90.60% below the      
          average.                                                                                                      

