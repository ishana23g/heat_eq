==PROF== Connected to process 23414 (/home/driffyn/Documents/CMDA4634/heat_eq/non_optimized/cuda_heat_equation)
==PROF== Profiling "heat_kernel_2d" - 0: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 1: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 2: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 3: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 4: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 5: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 6: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 7: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 8: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 9: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 10: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 11: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 12: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 13: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 14: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 15: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 16: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 17: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 18: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 19: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 20: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 21: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 22: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 23: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 24: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 25: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 26: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 27: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 28: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 29: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 30: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 31: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 32: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 33: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 34: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 35: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 36: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 37: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 38: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 39: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 40: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 41: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 42: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 43: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 44: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 45: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 46: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 47: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 48: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 49: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 50: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 51: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 52: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 53: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 54: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 55: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 56: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 57: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 58: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 59: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 60: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 61: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 62: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 63: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 64: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 65: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 66: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 67: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 68: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 69: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 70: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 71: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 72: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 73: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 74: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 75: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 76: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 77: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 78: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 79: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 80: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 81: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 82: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 83: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 84: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 85: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 86: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 87: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 88: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 89: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 90: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 91: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 92: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 93: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 94: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 95: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 96: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 97: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 98: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 99: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 100: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 101: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 102: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 103: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 104: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 105: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 106: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 107: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 108: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 109: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 110: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 111: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 112: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 113: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 114: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 115: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 116: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 117: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 118: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 119: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 120: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 121: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 122: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 123: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 124: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 125: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 126: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 127: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 128: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 129: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 130: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 131: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 132: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 133: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 134: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 135: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 136: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 137: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 138: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 139: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 140: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 141: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 142: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 143: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 144: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 145: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 146: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 147: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 148: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 149: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 150: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 151: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 152: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 153: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 154: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 155: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 156: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 157: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 158: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 159: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 160: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 161: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 162: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 163: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 164: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 165: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 166: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 167: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 168: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 169: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 170: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 171: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 172: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 173: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 174: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 175: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 176: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 177: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 178: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 179: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 180: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 181: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 182: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 183: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 184: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 185: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 186: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 187: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 188: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 189: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 190: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 191: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 192: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 193: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 194: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 195: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 196: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 197: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 198: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 199: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 200: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 201: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 202: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 203: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 204: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 205: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 206: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 207: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 208: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 209: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 210: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 211: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 212: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 213: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 214: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 215: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 216: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 217: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 218: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 219: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 220: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 221: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 222: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 223: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 224: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 225: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 226: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 227: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 228: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 229: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 230: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 231: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 232: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 233: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 234: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 235: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 236: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 237: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_kernel_2d" - 238: 0%....50%....100% - 8 passes
==PROF== Profiling "heat_to_color_kernel_2d" - 239: 0%....50%....100% - 8 passes
==PROF== Profiling "add_heat_kernel_2d" - 240: 0%....50%....100% - 8 passes
==PROF== Disconnected from process 23414
[23414] cuda_heat_equation@127.0.0.1
  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       817.17
    Elapsed Cycles                cycle       48,745
    Memory Throughput                 %        44.96
    DRAM Throughput                   %        44.96
    Duration                         us        59.65
    L1/TEX Cache Throughput           %        36.29
    L2 Cache Throughput               %        34.62
    SM Active Cycles              cycle    46,591.17
    Compute (SM) Throughput           %        70.69
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.49
    Achieved Active Warps Per SM           warp        38.15
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.51%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   147,109.33
    Total DRAM Elapsed Cycles        cycle    1,963,008
    Average L1 Active Cycles         cycle    46,591.17
    Total L1 Elapsed Cycles          cycle    1,457,680
    Average L2 Active Cycles         cycle    43,125.12
    Total L2 Elapsed Cycles          cycle    1,116,216
    Average SM Active Cycles         cycle    46,591.17
    Total SM Elapsed Cycles          cycle    1,457,680
    Average SMSP Active Cycles       cycle    46,466.66
    Total SMSP Elapsed Cycles        cycle    5,830,720
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.04
    Elapsed Cycles                cycle       33,743
    Memory Throughput                 %        65.43
    DRAM Throughput                   %        65.43
    Duration                         us        41.34
    L1/TEX Cache Throughput           %        38.29
    L2 Cache Throughput               %        32.57
    SM Active Cycles              cycle    32,017.60
    Compute (SM) Throughput           %        52.22
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.61
    Achieved Active Warps Per SM           warp        37.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.39%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   148,301.33
    Total DRAM Elapsed Cycles        cycle    1,359,872
    Average L1 Active Cycles         cycle    32,017.60
    Total L1 Elapsed Cycles          cycle      999,730
    Average L2 Active Cycles         cycle    29,489.33
    Total L2 Elapsed Cycles          cycle      772,608
    Average SM Active Cycles         cycle    32,017.60
    Total SM Elapsed Cycles          cycle      999,730
    Average SMSP Active Cycles       cycle    31,282.33
    Total SMSP Elapsed Cycles        cycle    3,998,920
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.86
    Elapsed Cycles                cycle       48,910
    Memory Throughput                 %        49.18
    DRAM Throughput                   %        49.18
    Duration                         us        59.87
    L1/TEX Cache Throughput           %        36.32
    L2 Cache Throughput               %        34.55
    SM Active Cycles              cycle    46,556.93
    Compute (SM) Throughput           %        70.83
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.70
    Achieved Active Warps Per SM           warp        38.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.3%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,477.33
    Total DRAM Elapsed Cycles        cycle    1,970,176
    Average L1 Active Cycles         cycle    46,556.93
    Total L1 Elapsed Cycles          cycle    1,454,820
    Average L2 Active Cycles         cycle    43,289.12
    Total L2 Elapsed Cycles          cycle    1,119,888
    Average SM Active Cycles         cycle    46,556.93
    Total SM Elapsed Cycles          cycle    1,454,820
    Average SMSP Active Cycles       cycle    46,196.40
    Total SMSP Elapsed Cycles        cycle    5,819,280
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.62
    Elapsed Cycles                cycle       35,361
    Memory Throughput                 %        68.01
    DRAM Throughput                   %        68.01
    Duration                         us        43.30
    L1/TEX Cache Throughput           %        38.83
    L2 Cache Throughput               %        31.14
    SM Active Cycles              cycle    31,084.67
    Compute (SM) Throughput           %        52.89
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.17
    Achieved Active Warps Per SM           warp        37.52
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.83%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,464
    Total DRAM Elapsed Cycles        cycle    1,424,384
    Average L1 Active Cycles         cycle    31,084.67
    Total L1 Elapsed Cycles          cycle      986,940
    Average L2 Active Cycles         cycle    29,449.46
    Total L2 Elapsed Cycles          cycle      809,640
    Average SM Active Cycles         cycle    31,084.67
    Total SM Elapsed Cycles          cycle      986,940
    Average SMSP Active Cycles       cycle    30,920.83
    Total SMSP Elapsed Cycles        cycle    3,947,760
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.88
    Elapsed Cycles                cycle       48,860
    Memory Throughput                 %        44.92
    DRAM Throughput                   %        44.92
    Duration                         us        59.81
    L1/TEX Cache Throughput           %        36.22
    L2 Cache Throughput               %        34.63
    SM Active Cycles              cycle    46,681.03
    Compute (SM) Throughput           %        70.80
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.39
    Achieved Active Warps Per SM           warp        38.11
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.61%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      147,432
    Total DRAM Elapsed Cycles        cycle    1,969,152
    Average L1 Active Cycles         cycle    46,681.03
    Total L1 Elapsed Cycles          cycle    1,455,440
    Average L2 Active Cycles         cycle    43,295.46
    Total L2 Elapsed Cycles          cycle    1,118,808
    Average SM Active Cycles         cycle    46,681.03
    Total SM Elapsed Cycles          cycle    1,455,440
    Average SMSP Active Cycles       cycle    46,335.38
    Total SMSP Elapsed Cycles        cycle    5,821,760
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.34
    Elapsed Cycles                cycle       34,408
    Memory Throughput                 %        64.61
    DRAM Throughput                   %        64.61
    Duration                         us        42.14
    L1/TEX Cache Throughput           %        37.75
    L2 Cache Throughput               %        31.89
    SM Active Cycles              cycle    30,956.27
    Compute (SM) Throughput           %        51.44
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.81
    Achieved Active Warps Per SM           warp        37.35
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.19%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      149,416
    Total DRAM Elapsed Cycles        cycle    1,387,520
    Average L1 Active Cycles         cycle    30,956.27
    Total L1 Elapsed Cycles          cycle    1,014,810
    Average L2 Active Cycles         cycle    29,445.38
    Total L2 Elapsed Cycles          cycle      787,848
    Average SM Active Cycles         cycle    30,956.27
    Total SM Elapsed Cycles          cycle    1,014,810
    Average SMSP Active Cycles       cycle    31,078.78
    Total SMSP Elapsed Cycles        cycle    4,059,240
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.42
    Elapsed Cycles                cycle       48,761
    Memory Throughput                 %        45.09
    DRAM Throughput                   %        45.09
    Duration                         us        59.65
    L1/TEX Cache Throughput           %        36.38
    L2 Cache Throughput               %        34.69
    SM Active Cycles              cycle    46,475.13
    Compute (SM) Throughput           %        70.53
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.65
    Achieved Active Warps Per SM           warp        38.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.35%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      147,672
    Total DRAM Elapsed Cycles        cycle    1,965,056
    Average L1 Active Cycles         cycle    46,475.13
    Total L1 Elapsed Cycles          cycle    1,461,000
    Average L2 Active Cycles         cycle    43,045.12
    Total L2 Elapsed Cycles          cycle    1,116,504
    Average SM Active Cycles         cycle    46,475.13
    Total SM Elapsed Cycles          cycle    1,461,000
    Average SMSP Active Cycles       cycle    46,173.11
    Total SMSP Elapsed Cycles        cycle    5,844,000
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.17
    Elapsed Cycles                cycle       34,714
    Memory Throughput                 %        69.14
    DRAM Throughput                   %        69.14
    Duration                         us        42.53
    L1/TEX Cache Throughput           %        38.47
    L2 Cache Throughput               %        31.62
    SM Active Cycles              cycle    31,679.50
    Compute (SM) Throughput           %        52.41
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.12
    Achieved Active Warps Per SM           warp        37.50
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.88%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,192
    Total DRAM Elapsed Cycles        cycle    1,398,784
    Average L1 Active Cycles         cycle    31,679.50
    Total L1 Elapsed Cycles          cycle      995,970
    Average L2 Active Cycles         cycle    29,339.12
    Total L2 Elapsed Cycles          cycle      794,928
    Average SM Active Cycles         cycle    31,679.50
    Total SM Elapsed Cycles          cycle      995,970
    Average SMSP Active Cycles       cycle    31,913.78
    Total SMSP Elapsed Cycles        cycle    3,983,880
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.33
    SM Frequency                    Mhz       802.08
    Elapsed Cycles                cycle        2,619
    Memory Throughput                 %         0.97
    DRAM Throughput                   %         0.47
    Duration                         us         3.26
    L1/TEX Cache Throughput           %         3.90
    L2 Cache Throughput               %         0.97
    SM Active Cycles              cycle       307.90
    Compute (SM) Throughput           %         0.64
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.99
    Achieved Active Warps Per SM           warp         7.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.01%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        82.67
    Total DRAM Elapsed Cycles        cycle      104,448
    Average L1 Active Cycles         cycle       307.90
    Total L1 Elapsed Cycles          cycle       77,060
    Average L2 Active Cycles         cycle       206.58
    Total L2 Elapsed Cycles          cycle       59,952
    Average SM Active Cycles         cycle       307.90
    Total SM Elapsed Cycles          cycle       77,060
    Average SMSP Active Cycles       cycle       276.70
    Total SMSP Elapsed Cycles        cycle      308,240
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.06%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.24% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.532%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 69.92% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.06%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.24% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.334%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 64.50% above the average, while the minimum instance value is 82.09% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.89
    Elapsed Cycles                cycle       48,861
    Memory Throughput                 %        45.27
    DRAM Throughput                   %        45.27
    Duration                         us        59.81
    L1/TEX Cache Throughput           %        36.36
    L2 Cache Throughput               %        34.62
    SM Active Cycles              cycle    46,507.63
    Compute (SM) Throughput           %        70.79
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.65
    Achieved Active Warps Per SM           warp        38.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.35%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   148,557.33
    Total DRAM Elapsed Cycles        cycle    1,969,152
    Average L1 Active Cycles         cycle    46,507.63
    Total L1 Elapsed Cycles          cycle    1,455,550
    Average L2 Active Cycles         cycle    43,118.50
    Total L2 Elapsed Cycles          cycle    1,118,616
    Average SM Active Cycles         cycle    46,507.63
    Total SM Elapsed Cycles          cycle    1,455,550
    Average SMSP Active Cycles       cycle    46,403.67
    Total SMSP Elapsed Cycles        cycle    5,822,200
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.65
    Elapsed Cycles                cycle       35,100
    Memory Throughput                 %        68.74
    DRAM Throughput                   %        68.74
    Duration                         us        42.98
    L1/TEX Cache Throughput           %        38.37
    L2 Cache Throughput               %        31.25
    SM Active Cycles              cycle    31,395.60
    Compute (SM) Throughput           %        52.33
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.95
    Achieved Active Warps Per SM           warp        37.41
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.05%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      162,128
    Total DRAM Elapsed Cycles        cycle    1,415,168
    Average L1 Active Cycles         cycle    31,395.60
    Total L1 Elapsed Cycles          cycle      997,540
    Average L2 Active Cycles         cycle    29,203.79
    Total L2 Elapsed Cycles          cycle      803,808
    Average SM Active Cycles         cycle    31,395.60
    Total SM Elapsed Cycles          cycle      997,540
    Average SMSP Active Cycles       cycle    31,551.94
    Total SMSP Elapsed Cycles        cycle    3,990,160
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.46
    Elapsed Cycles                cycle       48,974
    Memory Throughput                 %        49.31
    DRAM Throughput                   %        49.31
    Duration                         us        59.90
    L1/TEX Cache Throughput           %        36.40
    L2 Cache Throughput               %        34.61
    SM Active Cycles              cycle    46,457.50
    Compute (SM) Throughput           %        70.74
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.76
    Achieved Active Warps Per SM           warp        38.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.24%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      162,176
    Total DRAM Elapsed Cycles        cycle    1,973,248
    Average L1 Active Cycles         cycle    46,457.50
    Total L1 Elapsed Cycles          cycle    1,456,670
    Average L2 Active Cycles         cycle    43,124.92
    Total L2 Elapsed Cycles          cycle    1,121,424
    Average SM Active Cycles         cycle    46,457.50
    Total SM Elapsed Cycles          cycle    1,456,670
    Average SMSP Active Cycles       cycle    46,373.81
    Total SMSP Elapsed Cycles        cycle    5,826,680
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.72
    Elapsed Cycles                cycle       35,209
    Memory Throughput                 %        68.16
    DRAM Throughput                   %        68.16
    Duration                         us        43.10
    L1/TEX Cache Throughput           %        38.29
    L2 Cache Throughput               %        31.17
    SM Active Cycles              cycle    31,840.90
    Compute (SM) Throughput           %        52.21
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.46
    Achieved Active Warps Per SM           warp        37.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.54%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,000
    Total DRAM Elapsed Cycles        cycle    1,417,216
    Average L1 Active Cycles         cycle    31,840.90
    Total L1 Elapsed Cycles          cycle      999,850
    Average L2 Active Cycles         cycle    29,694.88
    Total L2 Elapsed Cycles          cycle      806,208
    Average SM Active Cycles         cycle    31,840.90
    Total SM Elapsed Cycles          cycle      999,850
    Average SMSP Active Cycles       cycle    31,334.68
    Total SMSP Elapsed Cycles        cycle    3,999,400
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.56
    Elapsed Cycles                cycle       49,057
    Memory Throughput                 %        49.01
    DRAM Throughput                   %        49.01
    Duration                         us           60
    L1/TEX Cache Throughput           %        36.32
    L2 Cache Throughput               %        34.50
    SM Active Cycles              cycle    46,553.03
    Compute (SM) Throughput           %        70.74
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.52
    Achieved Active Warps Per SM           warp        38.17
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.48%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,341.33
    Total DRAM Elapsed Cycles        cycle    1,975,296
    Average L1 Active Cycles         cycle    46,553.03
    Total L1 Elapsed Cycles          cycle    1,456,630
    Average L2 Active Cycles         cycle    43,143.54
    Total L2 Elapsed Cycles          cycle    1,123,224
    Average SM Active Cycles         cycle    46,553.03
    Total SM Elapsed Cycles          cycle    1,456,630
    Average SMSP Active Cycles       cycle    46,387.72
    Total SMSP Elapsed Cycles        cycle    5,826,520
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.54
    Elapsed Cycles                cycle       34,914
    Memory Throughput                 %        68.70
    DRAM Throughput                   %        68.70
    Duration                         us        42.75
    L1/TEX Cache Throughput           %        38.40
    L2 Cache Throughput               %        31.43
    SM Active Cycles              cycle    31,611.87
    Compute (SM) Throughput           %        52.36
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.90
    Achieved Active Warps Per SM           warp        37.39
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.1%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,088
    Total DRAM Elapsed Cycles        cycle    1,406,976
    Average L1 Active Cycles         cycle    31,611.87
    Total L1 Elapsed Cycles          cycle      996,990
    Average L2 Active Cycles         cycle    28,802.79
    Total L2 Elapsed Cycles          cycle      799,560
    Average SM Active Cycles         cycle    31,611.87
    Total SM Elapsed Cycles          cycle      996,990
    Average SMSP Active Cycles       cycle    31,211.20
    Total SMSP Elapsed Cycles        cycle    3,987,960
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.29
    Elapsed Cycles                cycle       49,120
    Memory Throughput                 %        48.92
    DRAM Throughput                   %        48.92
    Duration                         us        60.10
    L1/TEX Cache Throughput           %        36.35
    L2 Cache Throughput               %        34.43
    SM Active Cycles              cycle    46,509.57
    Compute (SM) Throughput           %        70.83
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.69
    Achieved Active Warps Per SM           warp        38.25
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.31%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,381.33
    Total DRAM Elapsed Cycles        cycle    1,979,392
    Average L1 Active Cycles         cycle    46,509.57
    Total L1 Elapsed Cycles          cycle    1,454,840
    Average L2 Active Cycles         cycle    43,143.46
    Total L2 Elapsed Cycles          cycle    1,124,688
    Average SM Active Cycles         cycle    46,509.57
    Total SM Elapsed Cycles          cycle    1,454,840
    Average SMSP Active Cycles       cycle    46,468.06
    Total SMSP Elapsed Cycles        cycle    5,819,360
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.67
    Elapsed Cycles                cycle       35,022
    Memory Throughput                 %        68.80
    DRAM Throughput                   %        68.80
    Duration                         us        42.88
    L1/TEX Cache Throughput           %        37.75
    L2 Cache Throughput               %        31.37
    SM Active Cycles              cycle    31,857.53
    Compute (SM) Throughput           %        51.47
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.16
    Achieved Active Warps Per SM           warp        37.52
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,690.67
    Total DRAM Elapsed Cycles        cycle    1,410,048
    Average L1 Active Cycles         cycle    31,857.53
    Total L1 Elapsed Cycles          cycle    1,014,190
    Average L2 Active Cycles         cycle    29,285.75
    Total L2 Elapsed Cycles          cycle      801,984
    Average SM Active Cycles         cycle    31,857.53
    Total SM Elapsed Cycles          cycle    1,014,190
    Average SMSP Active Cycles       cycle    30,899.22
    Total SMSP Elapsed Cycles        cycle    4,056,760
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.33
    SM Frequency                    Mhz       799.15
    Elapsed Cycles                cycle        2,636
    Memory Throughput                 %         0.96
    DRAM Throughput                   %         0.44
    Duration                         us         3.30
    L1/TEX Cache Throughput           %         3.97
    L2 Cache Throughput               %         0.96
    SM Active Cycles              cycle       302.57
    Compute (SM) Throughput           %         0.65
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.99
    Achieved Active Warps Per SM           warp         7.20
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.01%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        77.33
    Total DRAM Elapsed Cycles        cycle      105,472
    Average L1 Active Cycles         cycle       302.57
    Total L1 Elapsed Cycles          cycle       77,740
    Average L2 Active Cycles         cycle       230.54
    Total L2 Elapsed Cycles          cycle       60,336
    Average SM Active Cycles         cycle       302.57
    Total SM Elapsed Cycles          cycle       77,740
    Average SMSP Active Cycles       cycle       267.40
    Total SMSP Elapsed Cycles        cycle      310,960
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.853%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.25% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.226%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 70.02% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.853%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.25% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.641%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 61.51% above the average, while the minimum instance value is 83.95% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.08
    Elapsed Cycles                cycle       48,874
    Memory Throughput                 %        45.46
    DRAM Throughput                   %        45.46
    Duration                         us        59.81
    L1/TEX Cache Throughput           %        36.28
    L2 Cache Throughput               %        34.56
    SM Active Cycles              cycle    46,602.37
    Compute (SM) Throughput           %        70.59
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.67
    Achieved Active Warps Per SM           warp        38.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.33%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   149,197.33
    Total DRAM Elapsed Cycles        cycle    1,969,152
    Average L1 Active Cycles         cycle    46,602.37
    Total L1 Elapsed Cycles          cycle    1,459,690
    Average L2 Active Cycles         cycle    43,160.75
    Total L2 Elapsed Cycles          cycle    1,119,096
    Average SM Active Cycles         cycle    46,602.37
    Total SM Elapsed Cycles          cycle    1,459,690
    Average SMSP Active Cycles       cycle    46,428.64
    Total SMSP Elapsed Cycles        cycle    5,838,760
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.28
    Elapsed Cycles                cycle       34,876
    Memory Throughput                 %        69.00
    DRAM Throughput                   %        69.00
    Duration                         us        42.72
    L1/TEX Cache Throughput           %        37.57
    L2 Cache Throughput               %        31.47
    SM Active Cycles              cycle    31,146.47
    Compute (SM) Throughput           %        51.24
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.79
    Achieved Active Warps Per SM           warp        37.34
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.21%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,677.33
    Total DRAM Elapsed Cycles        cycle    1,405,952
    Average L1 Active Cycles         cycle    31,146.47
    Total L1 Elapsed Cycles          cycle    1,018,820
    Average L2 Active Cycles         cycle    29,507.38
    Total L2 Elapsed Cycles          cycle      798,672
    Average SM Active Cycles         cycle    31,146.47
    Total SM Elapsed Cycles          cycle    1,018,820
    Average SMSP Active Cycles       cycle    32,053.27
    Total SMSP Elapsed Cycles        cycle    4,075,280
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.25
    Elapsed Cycles                cycle       48,776
    Memory Throughput                 %        45.03
    DRAM Throughput                   %        45.03
    Duration                         us        59.68
    L1/TEX Cache Throughput           %        36.34
    L2 Cache Throughput               %        34.64
    SM Active Cycles              cycle    46,530.80
    Compute (SM) Throughput           %        70.89
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.59
    Achieved Active Warps Per SM           warp        38.20
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.41%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   147,485.33
    Total DRAM Elapsed Cycles        cycle    1,965,056
    Average L1 Active Cycles         cycle    46,530.80
    Total L1 Elapsed Cycles          cycle    1,453,490
    Average L2 Active Cycles         cycle    43,117.17
    Total L2 Elapsed Cycles          cycle    1,116,888
    Average SM Active Cycles         cycle    46,530.80
    Total SM Elapsed Cycles          cycle    1,453,490
    Average SMSP Active Cycles       cycle    46,442.57
    Total SMSP Elapsed Cycles        cycle    5,813,960
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.46
    Elapsed Cycles                cycle       35,014
    Memory Throughput                 %        68.63
    DRAM Throughput                   %        68.63
    Duration                         us        42.88
    L1/TEX Cache Throughput           %        37.53
    L2 Cache Throughput               %        31.35
    SM Active Cycles              cycle    30,843.50
    Compute (SM) Throughput           %        51.18
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.36
    Achieved Active Warps Per SM           warp        37.61
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.64%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,282.67
    Total DRAM Elapsed Cycles        cycle    1,410,048
    Average L1 Active Cycles         cycle    30,843.50
    Total L1 Elapsed Cycles          cycle    1,019,950
    Average L2 Active Cycles         cycle    29,266.92
    Total L2 Elapsed Cycles          cycle      801,744
    Average SM Active Cycles         cycle    30,843.50
    Total SM Elapsed Cycles          cycle    1,019,950
    Average SMSP Active Cycles       cycle    31,411.55
    Total SMSP Elapsed Cycles        cycle    4,079,800
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.97
    Elapsed Cycles                cycle       48,815
    Memory Throughput                 %        45.51
    DRAM Throughput                   %        45.51
    Duration                         us        59.74
    L1/TEX Cache Throughput           %        36.32
    L2 Cache Throughput               %        34.74
    SM Active Cycles              cycle    46,552.17
    Compute (SM) Throughput           %        70.78
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.61
    Achieved Active Warps Per SM           warp        38.21
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.39%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      149,120
    Total DRAM Elapsed Cycles        cycle    1,966,080
    Average L1 Active Cycles         cycle    46,552.17
    Total L1 Elapsed Cycles          cycle    1,455,740
    Average L2 Active Cycles         cycle    43,118.08
    Total L2 Elapsed Cycles          cycle    1,117,608
    Average SM Active Cycles         cycle    46,552.17
    Total SM Elapsed Cycles          cycle    1,455,740
    Average SMSP Active Cycles       cycle    46,495.27
    Total SMSP Elapsed Cycles        cycle    5,822,960
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.86
    Elapsed Cycles                cycle       35,005
    Memory Throughput                 %        68.50
    DRAM Throughput                   %        68.50
    Duration                         us        42.85
    L1/TEX Cache Throughput           %        37.90
    L2 Cache Throughput               %        31.45
    SM Active Cycles              cycle    31,497.23
    Compute (SM) Throughput           %        51.68
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.70
    Achieved Active Warps Per SM           warp        37.30
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.3%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   160,970.67
    Total DRAM Elapsed Cycles        cycle    1,410,048
    Average L1 Active Cycles         cycle    31,497.23
    Total L1 Elapsed Cycles          cycle    1,010,020
    Average L2 Active Cycles         cycle    29,554.88
    Total L2 Elapsed Cycles          cycle      801,576
    Average SM Active Cycles         cycle    31,497.23
    Total SM Elapsed Cycles          cycle    1,010,020
    Average SMSP Active Cycles       cycle    31,870.34
    Total SMSP Elapsed Cycles        cycle    4,040,080
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.84
    Elapsed Cycles                cycle       48,987
    Memory Throughput                 %        49.26
    DRAM Throughput                   %        49.26
    Duration                         us        59.97
    L1/TEX Cache Throughput           %        36.20
    L2 Cache Throughput               %        34.52
    SM Active Cycles              cycle    46,708.10
    Compute (SM) Throughput           %        71.16
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.39
    Achieved Active Warps Per SM           warp        38.11
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.61%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,989.33
    Total DRAM Elapsed Cycles        cycle    1,973,248
    Average L1 Active Cycles         cycle    46,708.10
    Total L1 Elapsed Cycles          cycle    1,447,920
    Average L2 Active Cycles         cycle    42,889.71
    Total L2 Elapsed Cycles          cycle    1,121,712
    Average SM Active Cycles         cycle    46,708.10
    Total SM Elapsed Cycles          cycle    1,447,920
    Average SMSP Active Cycles       cycle    46,261.66
    Total SMSP Elapsed Cycles        cycle    5,791,680
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.01
    Elapsed Cycles                cycle       34,943
    Memory Throughput                 %        68.90
    DRAM Throughput                   %        68.90
    Duration                         us        42.82
    L1/TEX Cache Throughput           %        37.84
    L2 Cache Throughput               %        31.41
    SM Active Cycles              cycle    31,584.83
    Compute (SM) Throughput           %        51.59
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.00
    Achieved Active Warps Per SM           warp        37.44
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22%                                                                                       
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,562.67
    Total DRAM Elapsed Cycles        cycle    1,406,976
    Average L1 Active Cycles         cycle    31,584.83
    Total L1 Elapsed Cycles          cycle    1,011,860
    Average L2 Active Cycles         cycle    29,351.04
    Total L2 Elapsed Cycles          cycle      800,088
    Average SM Active Cycles         cycle    31,584.83
    Total SM Elapsed Cycles          cycle    1,011,860
    Average SMSP Active Cycles       cycle    31,157.95
    Total SMSP Elapsed Cycles        cycle    4,047,440
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.44
    SM Frequency                    Mhz       796.62
    Elapsed Cycles                cycle        2,629
    Memory Throughput                 %         0.96
    DRAM Throughput                   %         0.46
    Duration                         us         3.30
    L1/TEX Cache Throughput           %         3.90
    L2 Cache Throughput               %         0.96
    SM Active Cycles              cycle       307.30
    Compute (SM) Throughput           %         0.66
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.02
    Achieved Active Warps Per SM           warp         7.21
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        82.67
    Total DRAM Elapsed Cycles        cycle      107,520
    Average L1 Active Cycles         cycle       307.30
    Total L1 Elapsed Cycles          cycle       76,360
    Average L2 Active Cycles         cycle       224.67
    Total L2 Elapsed Cycles          cycle       60,192
    Average SM Active Cycles         cycle       307.30
    Total SM Elapsed Cycles          cycle       76,360
    Average SMSP Active Cycles       cycle       277.37
    Total SMSP Elapsed Cycles        cycle      305,440
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.122%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.27% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.619%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 69.92% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.122%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.27% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.833%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 65.11% above the average, while the minimum instance value is 83.53% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.15
    Elapsed Cycles                cycle       48,903
    Memory Throughput                 %        45.78
    DRAM Throughput                   %        45.78
    Duration                         us        59.84
    L1/TEX Cache Throughput           %        36.31
    L2 Cache Throughput               %        34.64
    SM Active Cycles              cycle    46,573.13
    Compute (SM) Throughput           %        70.77
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.54
    Achieved Active Warps Per SM           warp        38.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.46%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      150,408
    Total DRAM Elapsed Cycles        cycle    1,971,200
    Average L1 Active Cycles         cycle    46,573.13
    Total L1 Elapsed Cycles          cycle    1,455,890
    Average L2 Active Cycles         cycle    43,191.75
    Total L2 Elapsed Cycles          cycle    1,119,792
    Average SM Active Cycles         cycle    46,573.13
    Total SM Elapsed Cycles          cycle    1,455,890
    Average SMSP Active Cycles       cycle    46,335.98
    Total SMSP Elapsed Cycles        cycle    5,823,560
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.57
    Elapsed Cycles                cycle       34,915
    Memory Throughput                 %        68.69
    DRAM Throughput                   %        68.69
    Duration                         us        42.75
    L1/TEX Cache Throughput           %        38.03
    L2 Cache Throughput               %        31.43
    SM Active Cycles              cycle    31,092.40
    Compute (SM) Throughput           %        51.85
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.58
    Achieved Active Warps Per SM           warp        37.72
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.42%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,069.33
    Total DRAM Elapsed Cycles        cycle    1,406,976
    Average L1 Active Cycles         cycle    31,092.40
    Total L1 Elapsed Cycles          cycle    1,006,770
    Average L2 Active Cycles         cycle    29,226.04
    Total L2 Elapsed Cycles          cycle      799,488
    Average SM Active Cycles         cycle    31,092.40
    Total SM Elapsed Cycles          cycle    1,006,770
    Average SMSP Active Cycles       cycle    31,998.07
    Total SMSP Elapsed Cycles        cycle    4,027,080
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.10
    Elapsed Cycles                cycle       48,744
    Memory Throughput                 %        45.57
    DRAM Throughput                   %        45.57
    Duration                         us        59.65
    L1/TEX Cache Throughput           %        36.40
    L2 Cache Throughput               %        34.73
    SM Active Cycles              cycle    46,448.10
    Compute (SM) Throughput           %        71.00
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.77
    Achieved Active Warps Per SM           warp        38.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.23%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   149,234.67
    Total DRAM Elapsed Cycles        cycle    1,965,056
    Average L1 Active Cycles         cycle    46,448.10
    Total L1 Elapsed Cycles          cycle    1,451,040
    Average L2 Active Cycles         cycle    42,908.12
    Total L2 Elapsed Cycles          cycle    1,116,144
    Average SM Active Cycles         cycle    46,448.10
    Total SM Elapsed Cycles          cycle    1,451,040
    Average SMSP Active Cycles       cycle    46,144.32
    Total SMSP Elapsed Cycles        cycle    5,804,160
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.93
    Elapsed Cycles                cycle       35,348
    Memory Throughput                 %        67.90
    DRAM Throughput                   %        67.90
    Duration                         us        43.26
    L1/TEX Cache Throughput           %        38.05
    L2 Cache Throughput               %        31.05
    SM Active Cycles              cycle    31,410.43
    Compute (SM) Throughput           %        51.88
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.92
    Achieved Active Warps Per SM           warp        37.40
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.08%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,181.33
    Total DRAM Elapsed Cycles        cycle    1,424,384
    Average L1 Active Cycles         cycle    31,410.43
    Total L1 Elapsed Cycles          cycle    1,006,090
    Average L2 Active Cycles         cycle    29,538.42
    Total L2 Elapsed Cycles          cycle      809,376
    Average SM Active Cycles         cycle    31,410.43
    Total SM Elapsed Cycles          cycle    1,006,090
    Average SMSP Active Cycles       cycle    31,915.78
    Total SMSP Elapsed Cycles        cycle    4,024,360
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.33
    SM Frequency                    Mhz       795.71
    Elapsed Cycles                cycle        2,624
    Memory Throughput                 %         0.97
    DRAM Throughput                   %         0.42
    Duration                         us         3.30
    L1/TEX Cache Throughput           %         4.03
    L2 Cache Throughput               %         0.97
    SM Active Cycles              cycle       298.07
    Compute (SM) Throughput           %         0.63
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.97
    Achieved Active Warps Per SM           warp         7.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.03%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        74.67
    Total DRAM Elapsed Cycles        cycle      105,472
    Average L1 Active Cycles         cycle       298.07
    Total L1 Elapsed Cycles          cycle       79,820
    Average L2 Active Cycles         cycle       206.54
    Total L2 Elapsed Cycles          cycle       60,072
    Average SM Active Cycles         cycle       298.07
    Total SM Elapsed Cycles          cycle       79,820
    Average SMSP Active Cycles       cycle       267.17
    Total SMSP Elapsed Cycles        cycle      319,280
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.533%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.25% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.034%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 70.05% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.533%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.25% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.333%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 64.63% above the average, while the minimum instance value is 82.09% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.21
    Elapsed Cycles                cycle       48,515
    Memory Throughput                 %        45.47
    DRAM Throughput                   %        45.47
    Duration                         us        59.36
    L1/TEX Cache Throughput           %        36.26
    L2 Cache Throughput               %        34.85
    SM Active Cycles              cycle    46,637.07
    Compute (SM) Throughput           %        70.92
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.54
    Achieved Active Warps Per SM           warp        38.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.46%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   148,050.67
    Total DRAM Elapsed Cycles        cycle    1,953,792
    Average L1 Active Cycles         cycle    46,637.07
    Total L1 Elapsed Cycles          cycle    1,452,690
    Average L2 Active Cycles         cycle    43,081.21
    Total L2 Elapsed Cycles          cycle    1,110,696
    Average SM Active Cycles         cycle    46,637.07
    Total SM Elapsed Cycles          cycle    1,452,690
    Average SMSP Active Cycles       cycle    46,331.79
    Total SMSP Elapsed Cycles        cycle    5,810,760
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.85
    Elapsed Cycles                cycle       35,187
    Memory Throughput                 %        68.31
    DRAM Throughput                   %        68.31
    Duration                         us        43.07
    L1/TEX Cache Throughput           %        38.97
    L2 Cache Throughput               %        31.20
    SM Active Cycles              cycle    31,630.47
    Compute (SM) Throughput           %        53.14
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.53
    Achieved Active Warps Per SM           warp        37.22
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.47%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,360
    Total DRAM Elapsed Cycles        cycle    1,417,216
    Average L1 Active Cycles         cycle    31,630.47
    Total L1 Elapsed Cycles          cycle      982,170
    Average L2 Active Cycles         cycle    29,841.92
    Total L2 Elapsed Cycles          cycle      805,680
    Average SM Active Cycles         cycle    31,630.47
    Total SM Elapsed Cycles          cycle      982,170
    Average SMSP Active Cycles       cycle    31,574.13
    Total SMSP Elapsed Cycles        cycle    3,928,680
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.98
    Elapsed Cycles                cycle       48,918
    Memory Throughput                 %        49.18
    DRAM Throughput                   %        49.18
    Duration                         us        59.87
    L1/TEX Cache Throughput           %        36.36
    L2 Cache Throughput               %        34.59
    SM Active Cycles              cycle    46,501.43
    Compute (SM) Throughput           %        70.73
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.69
    Achieved Active Warps Per SM           warp        38.25
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.31%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,645.33
    Total DRAM Elapsed Cycles        cycle    1,972,224
    Average L1 Active Cycles         cycle    46,501.43
    Total L1 Elapsed Cycles          cycle    1,456,560
    Average L2 Active Cycles         cycle    43,081.21
    Total L2 Elapsed Cycles          cycle    1,120,176
    Average SM Active Cycles         cycle    46,501.43
    Total SM Elapsed Cycles          cycle    1,456,560
    Average SMSP Active Cycles       cycle    46,251.26
    Total SMSP Elapsed Cycles        cycle    5,826,240
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.58
    Elapsed Cycles                cycle       35,199
    Memory Throughput                 %        68.17
    DRAM Throughput                   %        68.17
    Duration                         us        43.10
    L1/TEX Cache Throughput           %        38.09
    L2 Cache Throughput               %        31.18
    SM Active Cycles              cycle    31,815.77
    Compute (SM) Throughput           %        51.93
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.89
    Achieved Active Warps Per SM           warp        37.39
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.11%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,125.33
    Total DRAM Elapsed Cycles        cycle    1,418,240
    Average L1 Active Cycles         cycle    31,815.77
    Total L1 Elapsed Cycles          cycle    1,005,160
    Average L2 Active Cycles         cycle    29,711.71
    Total L2 Elapsed Cycles          cycle      806,040
    Average SM Active Cycles         cycle    31,815.77
    Total SM Elapsed Cycles          cycle    1,005,160
    Average SMSP Active Cycles       cycle    31,511.77
    Total SMSP Elapsed Cycles        cycle    4,020,640
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.44
    SM Frequency                    Mhz       795.51
    Elapsed Cycles                cycle        2,626
    Memory Throughput                 %         0.97
    DRAM Throughput                   %         0.48
    Duration                         us         3.30
    L1/TEX Cache Throughput           %         3.89
    L2 Cache Throughput               %         0.97
    SM Active Cycles              cycle       308.33
    Compute (SM) Throughput           %         0.65
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.99
    Achieved Active Warps Per SM           warp         7.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.01%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        85.33
    Total DRAM Elapsed Cycles        cycle      107,520
    Average L1 Active Cycles         cycle       308.33
    Total L1 Elapsed Cycles          cycle       76,730
    Average L2 Active Cycles         cycle       217.21
    Total L2 Elapsed Cycles          cycle       60,072
    Average SM Active Cycles         cycle       308.33
    Total SM Elapsed Cycles          cycle       76,730
    Average SMSP Active Cycles       cycle       277.54
    Total SMSP Elapsed Cycles        cycle      306,920
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.109%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.27% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.585%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 69.90% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.109%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.27% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.417%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 62.42% above the average, while the minimum instance value is 82.97% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.24
    Elapsed Cycles                cycle       48,934
    Memory Throughput                 %        49.20
    DRAM Throughput                   %        49.20
    Duration                         us        59.87
    L1/TEX Cache Throughput           %        36.29
    L2 Cache Throughput               %        34.56
    SM Active Cycles              cycle    46,588.93
    Compute (SM) Throughput           %        70.82
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.86
    Achieved Active Warps Per SM           warp        38.33
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.14%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,738.67
    Total DRAM Elapsed Cycles        cycle    1,972,224
    Average L1 Active Cycles         cycle    46,588.93
    Total L1 Elapsed Cycles          cycle    1,454,720
    Average L2 Active Cycles         cycle    43,112.50
    Total L2 Elapsed Cycles          cycle    1,120,512
    Average SM Active Cycles         cycle    46,588.93
    Total SM Elapsed Cycles          cycle    1,454,720
    Average SMSP Active Cycles       cycle    46,093.24
    Total SMSP Elapsed Cycles        cycle    5,818,880
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.91
    Elapsed Cycles                cycle       34,642
    Memory Throughput                 %        64.13
    DRAM Throughput                   %        64.13
    Duration                         us        42.40
    L1/TEX Cache Throughput           %        38.23
    L2 Cache Throughput               %        31.69
    SM Active Cycles              cycle    31,615.50
    Compute (SM) Throughput           %        52.13
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.70
    Achieved Active Warps Per SM           warp        37.30
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.3%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      149,184
    Total DRAM Elapsed Cycles        cycle    1,395,712
    Average L1 Active Cycles         cycle    31,615.50
    Total L1 Elapsed Cycles          cycle    1,001,220
    Average L2 Active Cycles         cycle    29,204.83
    Total L2 Elapsed Cycles          cycle      793,128
    Average SM Active Cycles         cycle    31,615.50
    Total SM Elapsed Cycles          cycle    1,001,220
    Average SMSP Active Cycles       cycle    31,332.89
    Total SMSP Elapsed Cycles        cycle    4,004,880
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.22
    Elapsed Cycles                cycle       49,012
    Memory Throughput                 %        49.20
    DRAM Throughput                   %        49.20
    Duration                         us        59.97
    L1/TEX Cache Throughput           %        36.34
    L2 Cache Throughput               %        34.48
    SM Active Cycles              cycle    46,533.17
    Compute (SM) Throughput           %        70.45
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.73
    Achieved Active Warps Per SM           warp        38.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.27%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,882.67
    Total DRAM Elapsed Cycles        cycle    1,974,272
    Average L1 Active Cycles         cycle    46,533.17
    Total L1 Elapsed Cycles          cycle    1,462,230
    Average L2 Active Cycles         cycle    43,062.25
    Total L2 Elapsed Cycles          cycle    1,122,312
    Average SM Active Cycles         cycle    46,533.17
    Total SM Elapsed Cycles          cycle    1,462,230
    Average SMSP Active Cycles       cycle    46,267.82
    Total SMSP Elapsed Cycles        cycle    5,848,920
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.12
    Elapsed Cycles                cycle       34,477
    Memory Throughput                 %        64.19
    DRAM Throughput                   %        64.19
    Duration                         us        42.24
    L1/TEX Cache Throughput           %        37.75
    L2 Cache Throughput               %        31.83
    SM Active Cycles              cycle    31,233.77
    Compute (SM) Throughput           %        51.48
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.78
    Achieved Active Warps Per SM           warp        37.34
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.22%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      148,560
    Total DRAM Elapsed Cycles        cycle    1,388,544
    Average L1 Active Cycles         cycle    31,233.77
    Total L1 Elapsed Cycles          cycle    1,013,940
    Average L2 Active Cycles         cycle    29,430.75
    Total L2 Elapsed Cycles          cycle      789,480
    Average SM Active Cycles         cycle    31,233.77
    Total SM Elapsed Cycles          cycle    1,013,940
    Average SMSP Active Cycles       cycle    31,425.88
    Total SMSP Elapsed Cycles        cycle    4,055,760
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.39
    SM Frequency                    Mhz       801.57
    Elapsed Cycles                cycle        2,592
    Memory Throughput                 %         0.98
    DRAM Throughput                   %         0.44
    Duration                         us         3.23
    L1/TEX Cache Throughput           %         4.05
    L2 Cache Throughput               %         0.98
    SM Active Cycles              cycle       296.47
    Compute (SM) Throughput           %         0.65
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.96
    Achieved Active Warps Per SM           warp         7.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.04%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        77.33
    Total DRAM Elapsed Cycles        cycle      104,448
    Average L1 Active Cycles         cycle       296.47
    Total L1 Elapsed Cycles          cycle       77,940
    Average L2 Active Cycles         cycle       213.38
    Total L2 Elapsed Cycles          cycle       59,448
    Average SM Active Cycles         cycle       296.47
    Total SM Elapsed Cycles          cycle       77,940
    Average SMSP Active Cycles       cycle       279.13
    Total SMSP Elapsed Cycles        cycle      311,760
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.673%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.24% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.516%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 69.95% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.673%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.24% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.461%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.40% above the average, while the minimum instance value is 82.66% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.01
    Elapsed Cycles                cycle       48,868
    Memory Throughput                 %        49.14
    DRAM Throughput                   %        49.14
    Duration                         us        59.81
    L1/TEX Cache Throughput           %        36.21
    L2 Cache Throughput               %        34.65
    SM Active Cycles              cycle    46,694.30
    Compute (SM) Throughput           %        70.64
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.53
    Achieved Active Warps Per SM           warp        38.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.47%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,288
    Total DRAM Elapsed Cycles        cycle    1,969,152
    Average L1 Active Cycles         cycle    46,694.30
    Total L1 Elapsed Cycles          cycle    1,458,360
    Average L2 Active Cycles         cycle    43,110.21
    Total L2 Elapsed Cycles          cycle    1,119,000
    Average SM Active Cycles         cycle    46,694.30
    Total SM Elapsed Cycles          cycle    1,458,360
    Average SMSP Active Cycles       cycle    46,270.73
    Total SMSP Elapsed Cycles        cycle    5,833,440
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.21
    Elapsed Cycles                cycle       35,579
    Memory Throughput                 %        67.43
    DRAM Throughput                   %        67.43
    Duration                         us        43.58
    L1/TEX Cache Throughput           %        38.20
    L2 Cache Throughput               %        30.85
    SM Active Cycles              cycle    31,644.43
    Compute (SM) Throughput           %        52.08
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.48
    Achieved Active Warps Per SM           warp        37.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.52%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,117.33
    Total DRAM Elapsed Cycles        cycle    1,433,600
    Average L1 Active Cycles         cycle    31,644.43
    Total L1 Elapsed Cycles          cycle    1,002,160
    Average L2 Active Cycles         cycle    29,547.67
    Total L2 Elapsed Cycles          cycle      814,776
    Average SM Active Cycles         cycle    31,644.43
    Total SM Elapsed Cycles          cycle    1,002,160
    Average SMSP Active Cycles       cycle    31,459.03
    Total SMSP Elapsed Cycles        cycle    4,008,640
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.28
    SM Frequency                    Mhz       789.96
    Elapsed Cycles                cycle        2,631
    Memory Throughput                 %         0.96
    DRAM Throughput                   %         0.46
    Duration                         us         3.33
    L1/TEX Cache Throughput           %         3.85
    L2 Cache Throughput               %         0.96
    SM Active Cycles              cycle          312
    Compute (SM) Throughput           %         0.64
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.01
    Achieved Active Warps Per SM           warp         7.20
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.99%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle           80
    Total DRAM Elapsed Cycles        cycle      105,472
    Average L1 Active Cycles         cycle          312
    Total L1 Elapsed Cycles          cycle       77,030
    Average L2 Active Cycles         cycle       218.29
    Total L2 Elapsed Cycles          cycle       60,288
    Average SM Active Cycles         cycle          312
    Total SM Elapsed Cycles          cycle       77,030
    Average SMSP Active Cycles       cycle       274.87
    Total SMSP Elapsed Cycles        cycle      308,120
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.165%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.19% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.486%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 69.93% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.165%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.19% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.469%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 62.94% above the average, while the minimum instance value is 83.05% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.79
    Elapsed Cycles                cycle       48,697
    Memory Throughput                 %        45.67
    DRAM Throughput                   %        45.67
    Duration                         us        59.62
    L1/TEX Cache Throughput           %        36.28
    L2 Cache Throughput               %        34.78
    SM Active Cycles              cycle    46,611.10
    Compute (SM) Throughput           %        70.62
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.74
    Achieved Active Warps Per SM           warp        38.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.26%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   149,277.33
    Total DRAM Elapsed Cycles        cycle    1,960,960
    Average L1 Active Cycles         cycle    46,611.10
    Total L1 Elapsed Cycles          cycle    1,458,620
    Average L2 Active Cycles         cycle    43,031.25
    Total L2 Elapsed Cycles          cycle    1,115,040
    Average SM Active Cycles         cycle    46,611.10
    Total SM Elapsed Cycles          cycle    1,458,620
    Average SMSP Active Cycles       cycle    46,258.12
    Total SMSP Elapsed Cycles        cycle    5,834,480
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.78
    Elapsed Cycles                cycle       34,113
    Memory Throughput                 %        64.59
    DRAM Throughput                   %        64.59
    Duration                         us        41.76
    L1/TEX Cache Throughput           %        37.93
    L2 Cache Throughput               %        32.17
    SM Active Cycles              cycle    31,327.07
    Compute (SM) Throughput           %        51.70
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.98
    Achieved Active Warps Per SM           warp        37.43
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.02%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   147,829.33
    Total DRAM Elapsed Cycles        cycle    1,373,184
    Average L1 Active Cycles         cycle    31,327.07
    Total L1 Elapsed Cycles          cycle    1,009,420
    Average L2 Active Cycles         cycle    29,718.83
    Total L2 Elapsed Cycles          cycle      781,176
    Average SM Active Cycles         cycle    31,327.07
    Total SM Elapsed Cycles          cycle    1,009,420
    Average SMSP Active Cycles       cycle    31,408.42
    Total SMSP Elapsed Cycles        cycle    4,037,680
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.47
    Elapsed Cycles                cycle       48,975
    Memory Throughput                 %        49.16
    DRAM Throughput                   %        49.16
    Duration                         us        59.90
    L1/TEX Cache Throughput           %        36.32
    L2 Cache Throughput               %        34.54
    SM Active Cycles              cycle    46,548.37
    Compute (SM) Throughput           %        70.78
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.67
    Achieved Active Warps Per SM           warp        38.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.33%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,682.67
    Total DRAM Elapsed Cycles        cycle    1,973,248
    Average L1 Active Cycles         cycle    46,548.37
    Total L1 Elapsed Cycles          cycle    1,455,230
    Average L2 Active Cycles         cycle    42,955.54
    Total L2 Elapsed Cycles          cycle    1,121,424
    Average SM Active Cycles         cycle    46,548.37
    Total SM Elapsed Cycles          cycle    1,455,230
    Average SMSP Active Cycles       cycle    46,378.68
    Total SMSP Elapsed Cycles        cycle    5,820,920
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.62
    Elapsed Cycles                cycle       34,003
    Memory Throughput                 %        64.79
    DRAM Throughput                   %        64.79
    Duration                         us        41.63
    L1/TEX Cache Throughput           %        37.98
    L2 Cache Throughput               %        32.28
    SM Active Cycles              cycle    32,050.93
    Compute (SM) Throughput           %        51.78
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.04
    Achieved Active Warps Per SM           warp        37.46
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.96%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   147,957.33
    Total DRAM Elapsed Cycles        cycle    1,370,112
    Average L1 Active Cycles         cycle    32,050.93
    Total L1 Elapsed Cycles          cycle    1,007,920
    Average L2 Active Cycles         cycle    29,970.17
    Total L2 Elapsed Cycles          cycle      778,632
    Average SM Active Cycles         cycle    32,050.93
    Total SM Elapsed Cycles          cycle    1,007,920
    Average SMSP Active Cycles       cycle    31,464.33
    Total SMSP Elapsed Cycles        cycle    4,031,680
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.14
    Elapsed Cycles                cycle       49,006
    Memory Throughput                 %        48.99
    DRAM Throughput                   %        48.99
    Duration                         us        59.97
    L1/TEX Cache Throughput           %        36.36
    L2 Cache Throughput               %        34.45
    SM Active Cycles              cycle    46,504.17
    Compute (SM) Throughput           %        70.76
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.71
    Achieved Active Warps Per SM           warp        38.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.29%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,274.67
    Total DRAM Elapsed Cycles        cycle    1,975,296
    Average L1 Active Cycles         cycle    46,504.17
    Total L1 Elapsed Cycles          cycle    1,455,420
    Average L2 Active Cycles         cycle    43,104.21
    Total L2 Elapsed Cycles          cycle    1,122,072
    Average SM Active Cycles         cycle    46,504.17
    Total SM Elapsed Cycles          cycle    1,455,420
    Average SMSP Active Cycles       cycle    46,374.82
    Total SMSP Elapsed Cycles        cycle    5,821,680
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.13
    Elapsed Cycles                cycle       35,131
    Memory Throughput                 %        68.23
    DRAM Throughput                   %        68.23
    Duration                         us        43.04
    L1/TEX Cache Throughput           %        38.69
    L2 Cache Throughput               %        31.25
    SM Active Cycles              cycle    31,903.67
    Compute (SM) Throughput           %        52.74
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.37
    Achieved Active Warps Per SM           warp        37.62
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.63%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,042.67
    Total DRAM Elapsed Cycles        cycle    1,416,192
    Average L1 Active Cycles         cycle    31,903.67
    Total L1 Elapsed Cycles          cycle      989,400
    Average L2 Active Cycles         cycle    29,878.38
    Total L2 Elapsed Cycles          cycle      804,504
    Average SM Active Cycles         cycle    31,903.67
    Total SM Elapsed Cycles          cycle      989,400
    Average SMSP Active Cycles       cycle    31,101.40
    Total SMSP Elapsed Cycles        cycle    3,957,600
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.82
    Elapsed Cycles                cycle       48,777
    Memory Throughput                 %        49.22
    DRAM Throughput                   %        49.22
    Duration                         us        59.71
    L1/TEX Cache Throughput           %        36.28
    L2 Cache Throughput               %        34.70
    SM Active Cycles              cycle    46,600.20
    Compute (SM) Throughput           %        70.74
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.57
    Achieved Active Warps Per SM           warp        38.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.43%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,288
    Total DRAM Elapsed Cycles        cycle    1,966,080
    Average L1 Active Cycles         cycle    46,600.20
    Total L1 Elapsed Cycles          cycle    1,455,750
    Average L2 Active Cycles         cycle    43,171.04
    Total L2 Elapsed Cycles          cycle    1,116,816
    Average SM Active Cycles         cycle    46,600.20
    Total SM Elapsed Cycles          cycle    1,455,750
    Average SMSP Active Cycles       cycle    46,241.97
    Total SMSP Elapsed Cycles        cycle    5,823,000
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.77
    Elapsed Cycles                cycle       35,366
    Memory Throughput                 %        68.10
    DRAM Throughput                   %        68.10
    Duration                         us        43.30
    L1/TEX Cache Throughput           %        37.14
    L2 Cache Throughput               %        31.03
    SM Active Cycles              cycle    32,153.63
    Compute (SM) Throughput           %        50.63
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.84
    Achieved Active Warps Per SM           warp        37.36
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.16%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,778.67
    Total DRAM Elapsed Cycles        cycle    1,425,408
    Average L1 Active Cycles         cycle    32,153.63
    Total L1 Elapsed Cycles          cycle    1,030,470
    Average L2 Active Cycles         cycle    29,366.83
    Total L2 Elapsed Cycles          cycle      809,856
    Average SM Active Cycles         cycle    32,153.63
    Total SM Elapsed Cycles          cycle    1,030,470
    Average SMSP Active Cycles       cycle    31,866.89
    Total SMSP Elapsed Cycles        cycle    4,121,880
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.16
    Elapsed Cycles                cycle       48,877
    Memory Throughput                 %        49.31
    DRAM Throughput                   %        49.31
    Duration                         us        59.81
    L1/TEX Cache Throughput           %        36.44
    L2 Cache Throughput               %        34.64
    SM Active Cycles              cycle    46,406.10
    Compute (SM) Throughput           %        70.55
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.62
    Achieved Active Warps Per SM           warp        38.22
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.38%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,840
    Total DRAM Elapsed Cycles        cycle    1,969,152
    Average L1 Active Cycles         cycle    46,406.10
    Total L1 Elapsed Cycles          cycle    1,459,600
    Average L2 Active Cycles         cycle    43,040.29
    Total L2 Elapsed Cycles          cycle    1,119,120
    Average SM Active Cycles         cycle    46,406.10
    Total SM Elapsed Cycles          cycle    1,459,600
    Average SMSP Active Cycles       cycle    46,262.82
    Total SMSP Elapsed Cycles        cycle    5,838,400
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.33
    Elapsed Cycles                cycle       35,349
    Memory Throughput                 %        68.18
    DRAM Throughput                   %        68.18
    Duration                         us        43.30
    L1/TEX Cache Throughput           %        38.28
    L2 Cache Throughput               %        31.09
    SM Active Cycles              cycle    32,082.27
    Compute (SM) Throughput           %        52.18
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.20
    Achieved Active Warps Per SM           warp        37.54
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.8%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,864
    Total DRAM Elapsed Cycles        cycle    1,424,384
    Average L1 Active Cycles         cycle    32,082.27
    Total L1 Elapsed Cycles          cycle      999,960
    Average L2 Active Cycles         cycle    28,964.62
    Total L2 Elapsed Cycles          cycle      809,520
    Average SM Active Cycles         cycle    32,082.27
    Total SM Elapsed Cycles          cycle      999,960
    Average SMSP Active Cycles       cycle    30,834.39
    Total SMSP Elapsed Cycles        cycle    3,999,840
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.22
    Elapsed Cycles                cycle       49,013
    Memory Throughput                 %        49.25
    DRAM Throughput                   %        49.25
    Duration                         us        59.97
    L1/TEX Cache Throughput           %        36.29
    L2 Cache Throughput               %        34.50
    SM Active Cycles              cycle    46,594.67
    Compute (SM) Throughput           %        70.64
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.74
    Achieved Active Warps Per SM           warp        38.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.26%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      162,048
    Total DRAM Elapsed Cycles        cycle    1,974,272
    Average L1 Active Cycles         cycle    46,594.67
    Total L1 Elapsed Cycles          cycle    1,457,470
    Average L2 Active Cycles         cycle    42,983.42
    Total L2 Elapsed Cycles          cycle    1,122,216
    Average SM Active Cycles         cycle    46,594.67
    Total SM Elapsed Cycles          cycle    1,457,470
    Average SMSP Active Cycles       cycle    46,297.70
    Total SMSP Elapsed Cycles        cycle    5,829,880
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.43
    Elapsed Cycles                cycle       34,986
    Memory Throughput                 %        68.53
    DRAM Throughput                   %        68.53
    Duration                         us        42.85
    L1/TEX Cache Throughput           %        38.61
    L2 Cache Throughput               %        31.37
    SM Active Cycles              cycle    31,333.03
    Compute (SM) Throughput           %        52.62
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.18
    Achieved Active Warps Per SM           warp        37.05
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.82%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,045.33
    Total DRAM Elapsed Cycles        cycle    1,410,048
    Average L1 Active Cycles         cycle    31,333.03
    Total L1 Elapsed Cycles          cycle      991,480
    Average L2 Active Cycles         cycle    29,645.67
    Total L2 Elapsed Cycles          cycle      801,120
    Average SM Active Cycles         cycle    31,333.03
    Total SM Elapsed Cycles          cycle      991,480
    Average SMSP Active Cycles       cycle    31,538.29
    Total SMSP Elapsed Cycles        cycle    3,965,920
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.39
    SM Frequency                    Mhz       798.41
    Elapsed Cycles                cycle        2,610
    Memory Throughput                 %         0.97
    DRAM Throughput                   %         0.42
    Duration                         us         3.26
    L1/TEX Cache Throughput           %         3.99
    L2 Cache Throughput               %         0.97
    SM Active Cycles              cycle       300.90
    Compute (SM) Throughput           %         0.64
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.98
    Achieved Active Warps Per SM           warp         7.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.02%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        74.67
    Total DRAM Elapsed Cycles        cycle      105,472
    Average L1 Active Cycles         cycle       300.90
    Total L1 Elapsed Cycles          cycle       78,310
    Average L2 Active Cycles         cycle       221.25
    Total L2 Elapsed Cycles          cycle       59,640
    Average SM Active Cycles         cycle       300.90
    Total SM Elapsed Cycles          cycle       78,310
    Average SMSP Active Cycles       cycle       265.63
    Total SMSP Elapsed Cycles        cycle      313,240
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.749%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.22% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.125%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 70.02% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.749%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.22% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.863%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 65.86% above the average, while the minimum instance value is 83.28% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.09
    Elapsed Cycles                cycle       48,925
    Memory Throughput                 %        49.13
    DRAM Throughput                   %        49.13
    Duration                         us        59.87
    L1/TEX Cache Throughput           %        36.13
    L2 Cache Throughput               %        34.58
    SM Active Cycles              cycle    46,804.17
    Compute (SM) Throughput           %        70.72
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.37
    Achieved Active Warps Per SM           warp        38.10
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.63%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,402.67
    Total DRAM Elapsed Cycles        cycle    1,971,200
    Average L1 Active Cycles         cycle    46,804.17
    Total L1 Elapsed Cycles          cycle    1,455,480
    Average L2 Active Cycles         cycle    43,082.67
    Total L2 Elapsed Cycles          cycle    1,120,320
    Average SM Active Cycles         cycle    46,804.17
    Total SM Elapsed Cycles          cycle    1,455,480
    Average SMSP Active Cycles       cycle    46,274.77
    Total SMSP Elapsed Cycles        cycle    5,821,920
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.40
    Elapsed Cycles                cycle       34,412
    Memory Throughput                 %        64.87
    DRAM Throughput                   %        64.87
    Duration                         us        42.14
    L1/TEX Cache Throughput           %        38.56
    L2 Cache Throughput               %        31.90
    SM Active Cycles              cycle    31,295.67
    Compute (SM) Throughput           %        52.53
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.30
    Achieved Active Warps Per SM           warp        37.58
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.7%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   149,789.33
    Total DRAM Elapsed Cycles        cycle    1,385,472
    Average L1 Active Cycles         cycle    31,295.67
    Total L1 Elapsed Cycles          cycle      992,880
    Average L2 Active Cycles         cycle    29,760.42
    Total L2 Elapsed Cycles          cycle      787,896
    Average SM Active Cycles         cycle    31,295.67
    Total SM Elapsed Cycles          cycle      992,880
    Average SMSP Active Cycles       cycle    31,626.51
    Total SMSP Elapsed Cycles        cycle    3,971,520
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.28
    Elapsed Cycles                cycle       48,833
    Memory Throughput                 %        45.15
    DRAM Throughput                   %        45.15
    Duration                         us        59.74
    L1/TEX Cache Throughput           %        36.16
    L2 Cache Throughput               %        34.66
    SM Active Cycles              cycle    46,766.13
    Compute (SM) Throughput           %        70.95
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.65
    Achieved Active Warps Per SM           warp        38.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.35%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   148,010.67
    Total DRAM Elapsed Cycles        cycle    1,967,104
    Average L1 Active Cycles         cycle    46,766.13
    Total L1 Elapsed Cycles          cycle    1,450,570
    Average L2 Active Cycles         cycle    43,090.58
    Total L2 Elapsed Cycles          cycle    1,118,184
    Average SM Active Cycles         cycle    46,766.13
    Total SM Elapsed Cycles          cycle    1,450,570
    Average SMSP Active Cycles       cycle    46,448.92
    Total SMSP Elapsed Cycles        cycle    5,802,280
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.28
    Elapsed Cycles                cycle       34,929
    Memory Throughput                 %        68.97
    DRAM Throughput                   %        68.97
    Duration                         us        42.78
    L1/TEX Cache Throughput           %        37.49
    L2 Cache Throughput               %        31.43
    SM Active Cycles              cycle    30,843.03
    Compute (SM) Throughput           %        51.09
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.66
    Achieved Active Warps Per SM           warp        37.76
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.34%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,725.33
    Total DRAM Elapsed Cycles        cycle    1,406,976
    Average L1 Active Cycles         cycle    30,843.03
    Total L1 Elapsed Cycles          cycle    1,020,800
    Average L2 Active Cycles         cycle    29,237.04
    Total L2 Elapsed Cycles          cycle      799,848
    Average SM Active Cycles         cycle    30,843.03
    Total SM Elapsed Cycles          cycle    1,020,800
    Average SMSP Active Cycles       cycle    31,378.20
    Total SMSP Elapsed Cycles        cycle    4,083,200
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.33
    SM Frequency                    Mhz       796.88
    Elapsed Cycles                cycle        2,602
    Memory Throughput                 %         0.98
    DRAM Throughput                   %         0.51
    Duration                         us         3.26
    L1/TEX Cache Throughput           %         3.77
    L2 Cache Throughput               %         0.98
    SM Active Cycles              cycle       318.03
    Compute (SM) Throughput           %         0.59
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.86
    Achieved Active Warps Per SM           warp         7.13
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.14%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (14.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle           88
    Total DRAM Elapsed Cycles        cycle      104,448
    Average L1 Active Cycles         cycle       318.03
    Total L1 Elapsed Cycles          cycle       76,810
    Average L2 Active Cycles         cycle       232.96
    Total L2 Elapsed Cycles          cycle       59,592
    Average SM Active Cycles         cycle       318.03
    Total SM Elapsed Cycles          cycle       76,810
    Average SMSP Active Cycles       cycle       277.57
    Total SMSP Elapsed Cycles        cycle      307,240
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.345%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.18% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.595%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 70.06% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.345%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.18% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.999%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.94% above the average, while the minimum instance value is 84.12% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.21
    Elapsed Cycles                cycle       48,619
    Memory Throughput                 %        45.21
    DRAM Throughput                   %        45.21
    Duration                         us        59.49
    L1/TEX Cache Throughput           %        36.44
    L2 Cache Throughput               %        34.76
    SM Active Cycles              cycle    46,400.70
    Compute (SM) Throughput           %        70.67
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.79
    Achieved Active Warps Per SM           warp        38.30
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.21%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      147,696
    Total DRAM Elapsed Cycles        cycle    1,959,936
    Average L1 Active Cycles         cycle    46,400.70
    Total L1 Elapsed Cycles          cycle    1,455,990
    Average L2 Active Cycles         cycle       43,084
    Total L2 Elapsed Cycles          cycle    1,113,360
    Average SM Active Cycles         cycle    46,400.70
    Total SM Elapsed Cycles          cycle    1,455,990
    Average SMSP Active Cycles       cycle    46,277.84
    Total SMSP Elapsed Cycles        cycle    5,823,960
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       815.98
    Elapsed Cycles                cycle       33,846
    Memory Throughput                 %        64.96
    DRAM Throughput                   %        64.96
    Duration                         us        41.47
    L1/TEX Cache Throughput           %        38.02
    L2 Cache Throughput               %        32.48
    SM Active Cycles              cycle    32,036.20
    Compute (SM) Throughput           %        51.79
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.95
    Achieved Active Warps Per SM           warp        37.42
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.05%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      147,664
    Total DRAM Elapsed Cycles        cycle    1,363,968
    Average L1 Active Cycles         cycle    32,036.20
    Total L1 Elapsed Cycles          cycle    1,006,850
    Average L2 Active Cycles         cycle    29,534.67
    Total L2 Elapsed Cycles          cycle      775,032
    Average SM Active Cycles         cycle    32,036.20
    Total SM Elapsed Cycles          cycle    1,006,850
    Average SMSP Active Cycles       cycle    32,027.12
    Total SMSP Elapsed Cycles        cycle    4,027,400
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.33
    SM Frequency                    Mhz       797.18
    Elapsed Cycles                cycle        2,605
    Memory Throughput                 %         1.00
    DRAM Throughput                   %         0.51
    Duration                         us         3.26
    L1/TEX Cache Throughput           %         3.82
    L2 Cache Throughput               %         1.00
    SM Active Cycles              cycle       314.13
    Compute (SM) Throughput           %         0.63
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.05
    Achieved Active Warps Per SM           warp         7.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.95%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle           88
    Total DRAM Elapsed Cycles        cycle      104,448
    Average L1 Active Cycles         cycle       314.13
    Total L1 Elapsed Cycles          cycle       80,000
    Average L2 Active Cycles         cycle       231.25
    Total L2 Elapsed Cycles          cycle       59,568
    Average SM Active Cycles         cycle       314.13
    Total SM Elapsed Cycles          cycle       80,000
    Average SMSP Active Cycles       cycle       267.27
    Total SMSP Elapsed Cycles        cycle      320,000
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.925%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.28% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.019%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 70.04% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.925%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.28% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.563%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 59.71% above the average, while the minimum instance value is 84.00% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.53
    Elapsed Cycles                cycle       48,897
    Memory Throughput                 %        49.12
    DRAM Throughput                   %        49.12
    Duration                         us        59.81
    L1/TEX Cache Throughput           %        36.45
    L2 Cache Throughput               %        34.60
    SM Active Cycles              cycle    46,393.47
    Compute (SM) Throughput           %        70.74
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.66
    Achieved Active Warps Per SM           warp        38.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.34%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,280
    Total DRAM Elapsed Cycles        cycle    1,970,176
    Average L1 Active Cycles         cycle    46,393.47
    Total L1 Elapsed Cycles          cycle    1,454,240
    Average L2 Active Cycles         cycle    43,163.08
    Total L2 Elapsed Cycles          cycle    1,119,672
    Average SM Active Cycles         cycle    46,393.47
    Total SM Elapsed Cycles          cycle    1,454,240
    Average SMSP Active Cycles       cycle    46,250.01
    Total SMSP Elapsed Cycles        cycle    5,816,960
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.17
    Elapsed Cycles                cycle       35,289
    Memory Throughput                 %        68.30
    DRAM Throughput                   %        68.30
    Duration                         us        43.23
    L1/TEX Cache Throughput           %        37.76
    L2 Cache Throughput               %        31.12
    SM Active Cycles              cycle    31,823.23
    Compute (SM) Throughput           %        51.41
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.77
    Achieved Active Warps Per SM           warp        37.33
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.23%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,917.33
    Total DRAM Elapsed Cycles        cycle    1,422,336
    Average L1 Active Cycles         cycle    31,823.23
    Total L1 Elapsed Cycles          cycle    1,013,990
    Average L2 Active Cycles         cycle       29,661
    Total L2 Elapsed Cycles          cycle      808,080
    Average SM Active Cycles         cycle    31,823.23
    Total SM Elapsed Cycles          cycle    1,013,990
    Average SMSP Active Cycles       cycle    31,317.10
    Total SMSP Elapsed Cycles        cycle    4,055,960
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.38
    SM Frequency                    Mhz       797.02
    Elapsed Cycles                cycle        2,682
    Memory Throughput                 %         0.95
    DRAM Throughput                   %         0.46
    Duration                         us         3.36
    L1/TEX Cache Throughput           %         3.88
    L2 Cache Throughput               %         0.95
    SM Active Cycles              cycle       308.97
    Compute (SM) Throughput           %         0.66
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.03
    Achieved Active Warps Per SM           warp         7.21
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.97%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        82.67
    Total DRAM Elapsed Cycles        cycle      108,544
    Average L1 Active Cycles         cycle       308.97
    Total L1 Elapsed Cycles          cycle       76,420
    Average L2 Active Cycles         cycle       200.46
    Total L2 Elapsed Cycles          cycle       61,392
    Average SM Active Cycles         cycle       308.97
    Total SM Elapsed Cycles          cycle       76,420
    Average SMSP Active Cycles       cycle       276.48
    Total SMSP Elapsed Cycles        cycle      305,680
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.159%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.27% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.588%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 69.92% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.159%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.27% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.169%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 65.97% above the average, while the minimum instance value is 81.54% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.95
    Elapsed Cycles                cycle       48,892
    Memory Throughput                 %        49.46
    DRAM Throughput                   %        49.46
    Duration                         us        59.84
    L1/TEX Cache Throughput           %        36.37
    L2 Cache Throughput               %        34.51
    SM Active Cycles              cycle    46,489.57
    Compute (SM) Throughput           %        70.77
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.62
    Achieved Active Warps Per SM           warp        38.22
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.38%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   162,421.33
    Total DRAM Elapsed Cycles        cycle    1,970,176
    Average L1 Active Cycles         cycle    46,489.57
    Total L1 Elapsed Cycles          cycle    1,453,250
    Average L2 Active Cycles         cycle    43,166.42
    Total L2 Elapsed Cycles          cycle    1,119,552
    Average SM Active Cycles         cycle    46,489.57
    Total SM Elapsed Cycles          cycle    1,453,250
    Average SMSP Active Cycles       cycle    46,173.73
    Total SMSP Elapsed Cycles        cycle    5,813,000
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.67
    Elapsed Cycles                cycle       35,338
    Memory Throughput                 %        68.31
    DRAM Throughput                   %        68.31
    Duration                         us        43.26
    L1/TEX Cache Throughput           %        38.21
    L2 Cache Throughput               %        31.06
    SM Active Cycles              cycle    31,577.47
    Compute (SM) Throughput           %        52.03
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.03
    Achieved Active Warps Per SM           warp        37.46
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.97%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      162,040
    Total DRAM Elapsed Cycles        cycle    1,423,360
    Average L1 Active Cycles         cycle    31,577.47
    Total L1 Elapsed Cycles          cycle    1,001,800
    Average L2 Active Cycles         cycle    29,724.46
    Total L2 Elapsed Cycles          cycle      809,184
    Average SM Active Cycles         cycle    31,577.47
    Total SM Elapsed Cycles          cycle    1,001,800
    Average SMSP Active Cycles       cycle    31,180.18
    Total SMSP Elapsed Cycles        cycle    4,007,200
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.33
    SM Frequency                    Mhz       799.05
    Elapsed Cycles                cycle        2,639
    Memory Throughput                 %         0.97
    DRAM Throughput                   %         0.46
    Duration                         us         3.30
    L1/TEX Cache Throughput           %         4.02
    L2 Cache Throughput               %         0.97
    SM Active Cycles              cycle       298.53
    Compute (SM) Throughput           %         0.65
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.97
    Achieved Active Warps Per SM           warp         7.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.03%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle           80
    Total DRAM Elapsed Cycles        cycle      105,472
    Average L1 Active Cycles         cycle       298.53
    Total L1 Elapsed Cycles          cycle       77,900
    Average L2 Active Cycles         cycle       211.88
    Total L2 Elapsed Cycles          cycle       60,288
    Average SM Active Cycles         cycle       298.53
    Total SM Elapsed Cycles          cycle       77,900
    Average SMSP Active Cycles       cycle       266.03
    Total SMSP Elapsed Cycles        cycle      311,600
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.733%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.27% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.186%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 70.14% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.733%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.27% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.561%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 65.94% above the average, while the minimum instance value is 82.54% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.84
    Elapsed Cycles                cycle       48,624
    Memory Throughput                 %        45.24
    DRAM Throughput                   %        45.24
    Duration                         us        59.52
    L1/TEX Cache Throughput           %        36.43
    L2 Cache Throughput               %        34.80
    SM Active Cycles              cycle    46,415.70
    Compute (SM) Throughput           %        70.73
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.75
    Achieved Active Warps Per SM           warp        38.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.25%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   147,765.33
    Total DRAM Elapsed Cycles        cycle    1,959,936
    Average L1 Active Cycles         cycle    46,415.70
    Total L1 Elapsed Cycles          cycle    1,453,810
    Average L2 Active Cycles         cycle    43,161.92
    Total L2 Elapsed Cycles          cycle    1,113,408
    Average SM Active Cycles         cycle    46,415.70
    Total SM Elapsed Cycles          cycle    1,453,810
    Average SMSP Active Cycles       cycle    46,382.39
    Total SMSP Elapsed Cycles        cycle    5,815,240
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.48
    Elapsed Cycles                cycle       34,151
    Memory Throughput                 %        70.20
    DRAM Throughput                   %        70.20
    Duration                         us        41.82
    L1/TEX Cache Throughput           %        38.23
    L2 Cache Throughput               %        32.22
    SM Active Cycles              cycle    31,644.63
    Compute (SM) Throughput           %        52.05
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.66
    Achieved Active Warps Per SM           warp        37.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.34%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,032
    Total DRAM Elapsed Cycles        cycle    1,376,256
    Average L1 Active Cycles         cycle    31,644.63
    Total L1 Elapsed Cycles          cycle    1,001,050
    Average L2 Active Cycles         cycle    28,922.33
    Total L2 Elapsed Cycles          cycle      781,992
    Average SM Active Cycles         cycle    31,644.63
    Total SM Elapsed Cycles          cycle    1,001,050
    Average SMSP Active Cycles       cycle    31,494.12
    Total SMSP Elapsed Cycles        cycle    4,004,200
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.23
    Elapsed Cycles                cycle       48,960
    Memory Throughput                 %        49.18
    DRAM Throughput                   %        49.18
    Duration                         us        59.90
    L1/TEX Cache Throughput           %        36.46
    L2 Cache Throughput               %        34.54
    SM Active Cycles              cycle    46,378.83
    Compute (SM) Throughput           %        70.64
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.70
    Achieved Active Warps Per SM           warp        38.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.3%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,749.33
    Total DRAM Elapsed Cycles        cycle    1,973,248
    Average L1 Active Cycles         cycle    46,378.83
    Total L1 Elapsed Cycles          cycle    1,455,120
    Average L2 Active Cycles         cycle    43,169.21
    Total L2 Elapsed Cycles          cycle    1,121,136
    Average SM Active Cycles         cycle    46,378.83
    Total SM Elapsed Cycles          cycle    1,455,120
    Average SMSP Active Cycles       cycle    46,261.82
    Total SMSP Elapsed Cycles        cycle    5,820,480
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.86
    Elapsed Cycles                cycle       34,769
    Memory Throughput                 %        63.36
    DRAM Throughput                   %        63.36
    Duration                         us        42.56
    L1/TEX Cache Throughput           %        38.04
    L2 Cache Throughput               %        31.57
    SM Active Cycles              cycle    31,093.90
    Compute (SM) Throughput           %        51.76
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.32
    Achieved Active Warps Per SM           warp        37.59
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.68%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   147,933.33
    Total DRAM Elapsed Cycles        cycle    1,400,832
    Average L1 Active Cycles         cycle    31,093.90
    Total L1 Elapsed Cycles          cycle    1,006,480
    Average L2 Active Cycles         cycle    29,511.54
    Total L2 Elapsed Cycles          cycle      796,128
    Average SM Active Cycles         cycle    31,093.90
    Total SM Elapsed Cycles          cycle    1,006,480
    Average SMSP Active Cycles       cycle    31,086.10
    Total SMSP Elapsed Cycles        cycle    4,025,920
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.17
    Elapsed Cycles                cycle       48,747
    Memory Throughput                 %        49.36
    DRAM Throughput                   %        49.36
    Duration                         us        59.65
    L1/TEX Cache Throughput           %        36.41
    L2 Cache Throughput               %        34.60
    SM Active Cycles              cycle    46,443.77
    Compute (SM) Throughput           %        70.49
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.56
    Achieved Active Warps Per SM           warp        38.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.44%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,576
    Total DRAM Elapsed Cycles        cycle    1,964,032
    Average L1 Active Cycles         cycle    46,443.77
    Total L1 Elapsed Cycles          cycle    1,457,630
    Average L2 Active Cycles         cycle    43,045.71
    Total L2 Elapsed Cycles          cycle    1,116,240
    Average SM Active Cycles         cycle    46,443.77
    Total SM Elapsed Cycles          cycle    1,457,630
    Average SMSP Active Cycles       cycle    46,101.43
    Total SMSP Elapsed Cycles        cycle    5,830,520
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.75
    Elapsed Cycles                cycle       34,582
    Memory Throughput                 %        69.67
    DRAM Throughput                   %        69.67
    Duration                         us        42.34
    L1/TEX Cache Throughput           %        37.28
    L2 Cache Throughput               %        31.72
    SM Active Cycles              cycle    31,699.87
    Compute (SM) Throughput           %        50.72
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.73
    Achieved Active Warps Per SM           warp        37.31
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.27%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,704
    Total DRAM Elapsed Cycles        cycle    1,392,640
    Average L1 Active Cycles         cycle    31,699.87
    Total L1 Elapsed Cycles          cycle    1,026,810
    Average L2 Active Cycles         cycle    29,406.42
    Total L2 Elapsed Cycles          cycle      791,904
    Average SM Active Cycles         cycle    31,699.87
    Total SM Elapsed Cycles          cycle    1,026,810
    Average SMSP Active Cycles       cycle    31,828.17
    Total SMSP Elapsed Cycles        cycle    4,107,240
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.50
    Elapsed Cycles                cycle       48,660
    Memory Throughput                 %        45.29
    DRAM Throughput                   %        45.29
    Duration                         us        59.52
    L1/TEX Cache Throughput           %        36.37
    L2 Cache Throughput               %        34.75
    SM Active Cycles              cycle       46,487
    Compute (SM) Throughput           %        70.90
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.67
    Achieved Active Warps Per SM           warp        38.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.33%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   147,946.67
    Total DRAM Elapsed Cycles        cycle    1,959,936
    Average L1 Active Cycles         cycle       46,487
    Total L1 Elapsed Cycles          cycle    1,448,870
    Average L2 Active Cycles         cycle    43,200.21
    Total L2 Elapsed Cycles          cycle    1,114,248
    Average SM Active Cycles         cycle       46,487
    Total SM Elapsed Cycles          cycle    1,448,870
    Average SMSP Active Cycles       cycle    46,336.13
    Total SMSP Elapsed Cycles        cycle    5,795,480
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.76
    Elapsed Cycles                cycle       35,050
    Memory Throughput                 %        68.46
    DRAM Throughput                   %        68.46
    Duration                         us        42.91
    L1/TEX Cache Throughput           %        38.49
    L2 Cache Throughput               %        31.31
    SM Active Cycles              cycle    31,230.77
    Compute (SM) Throughput           %        52.35
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.74
    Achieved Active Warps Per SM           warp        37.31
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.26%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,248
    Total DRAM Elapsed Cycles        cycle    1,413,120
    Average L1 Active Cycles         cycle    31,230.77
    Total L1 Elapsed Cycles          cycle      994,660
    Average L2 Active Cycles         cycle    29,811.33
    Total L2 Elapsed Cycles          cycle      802,632
    Average SM Active Cycles         cycle    31,230.77
    Total SM Elapsed Cycles          cycle      994,660
    Average SMSP Active Cycles       cycle    31,758.56
    Total SMSP Elapsed Cycles        cycle    3,978,640
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.17
    Elapsed Cycles                cycle       48,694
    Memory Throughput                 %        49.36
    DRAM Throughput                   %        49.36
    Duration                         us        59.58
    L1/TEX Cache Throughput           %        36.41
    L2 Cache Throughput               %        34.67
    SM Active Cycles              cycle    46,439.33
    Compute (SM) Throughput           %        70.67
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.56
    Achieved Active Warps Per SM           warp        38.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.44%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,490.67
    Total DRAM Elapsed Cycles        cycle    1,963,008
    Average L1 Active Cycles         cycle    46,439.33
    Total L1 Elapsed Cycles          cycle    1,452,990
    Average L2 Active Cycles         cycle       42,992
    Total L2 Elapsed Cycles          cycle    1,115,088
    Average SM Active Cycles         cycle    46,439.33
    Total SM Elapsed Cycles          cycle    1,452,990
    Average SMSP Active Cycles       cycle    46,268.70
    Total SMSP Elapsed Cycles        cycle    5,811,960
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.66
    Elapsed Cycles                cycle       35,256
    Memory Throughput                 %        68.00
    DRAM Throughput                   %        68.00
    Duration                         us        43.17
    L1/TEX Cache Throughput           %        37.71
    L2 Cache Throughput               %        31.13
    SM Active Cycles              cycle    31,454.93
    Compute (SM) Throughput           %        51.29
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.77
    Achieved Active Warps Per SM           warp        37.33
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.23%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,088
    Total DRAM Elapsed Cycles        cycle    1,421,312
    Average L1 Active Cycles         cycle    31,454.93
    Total L1 Elapsed Cycles          cycle    1,015,020
    Average L2 Active Cycles         cycle    29,749.92
    Total L2 Elapsed Cycles          cycle      807,264
    Average SM Active Cycles         cycle    31,454.93
    Total SM Elapsed Cycles          cycle    1,015,020
    Average SMSP Active Cycles       cycle    31,651.14
    Total SMSP Elapsed Cycles        cycle    4,060,080
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.47
    Elapsed Cycles                cycle       48,793
    Memory Throughput                 %        45.40
    DRAM Throughput                   %        45.40
    Duration                         us        59.68
    L1/TEX Cache Throughput           %        36.43
    L2 Cache Throughput               %        34.69
    SM Active Cycles              cycle    46,410.37
    Compute (SM) Throughput           %        70.61
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.64
    Achieved Active Warps Per SM           warp        38.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.36%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   148,762.67
    Total DRAM Elapsed Cycles        cycle    1,966,080
    Average L1 Active Cycles         cycle    46,410.37
    Total L1 Elapsed Cycles          cycle    1,453,700
    Average L2 Active Cycles         cycle    43,018.17
    Total L2 Elapsed Cycles          cycle    1,117,272
    Average SM Active Cycles         cycle    46,410.37
    Total SM Elapsed Cycles          cycle    1,453,700
    Average SMSP Active Cycles       cycle    46,292.57
    Total SMSP Elapsed Cycles        cycle    5,814,800
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.43
    Elapsed Cycles                cycle       34,856
    Memory Throughput                 %        69.17
    DRAM Throughput                   %        69.17
    Duration                         us        42.69
    L1/TEX Cache Throughput           %        38.78
    L2 Cache Throughput               %        31.51
    SM Active Cycles              cycle    31,347.67
    Compute (SM) Throughput           %        52.72
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.99
    Achieved Active Warps Per SM           warp        37.43
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.01%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,973.33
    Total DRAM Elapsed Cycles        cycle    1,404,928
    Average L1 Active Cycles         cycle    31,347.67
    Total L1 Elapsed Cycles          cycle      987,090
    Average L2 Active Cycles         cycle    29,567.54
    Total L2 Elapsed Cycles          cycle      798,144
    Average SM Active Cycles         cycle    31,347.67
    Total SM Elapsed Cycles          cycle      987,090
    Average SMSP Active Cycles       cycle    31,383.33
    Total SMSP Elapsed Cycles        cycle    3,948,360
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.08
    Elapsed Cycles                cycle       48,974
    Memory Throughput                 %        49.22
    DRAM Throughput                   %        49.22
    Duration                         us        59.94
    L1/TEX Cache Throughput           %        36.37
    L2 Cache Throughput               %        34.54
    SM Active Cycles              cycle    46,493.77
    Compute (SM) Throughput           %        70.56
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.42
    Achieved Active Warps Per SM           warp        38.12
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.58%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,970.67
    Total DRAM Elapsed Cycles        cycle    1,974,272
    Average L1 Active Cycles         cycle    46,493.77
    Total L1 Elapsed Cycles          cycle    1,454,210
    Average L2 Active Cycles         cycle    42,983.25
    Total L2 Elapsed Cycles          cycle    1,121,424
    Average SM Active Cycles         cycle    46,493.77
    Total SM Elapsed Cycles          cycle    1,454,210
    Average SMSP Active Cycles       cycle    46,180.72
    Total SMSP Elapsed Cycles        cycle    5,816,840
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.57
    Elapsed Cycles                cycle       33,609
    Memory Throughput                 %        65.88
    DRAM Throughput                   %        65.88
    Duration                         us        41.15
    L1/TEX Cache Throughput           %        37.66
    L2 Cache Throughput               %        32.72
    SM Active Cycles              cycle    31,373.93
    Compute (SM) Throughput           %        51.19
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.78
    Achieved Active Warps Per SM           warp        37.33
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.22%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   148,629.33
    Total DRAM Elapsed Cycles        cycle    1,353,728
    Average L1 Active Cycles         cycle    31,373.93
    Total L1 Elapsed Cycles          cycle    1,016,370
    Average L2 Active Cycles         cycle    29,231.71
    Total L2 Elapsed Cycles          cycle      769,608
    Average SM Active Cycles         cycle    31,373.93
    Total SM Elapsed Cycles          cycle    1,016,370
    Average SMSP Active Cycles       cycle    31,049.53
    Total SMSP Elapsed Cycles        cycle    4,065,480
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.33
    SM Frequency                    Mhz       792.37
    Elapsed Cycles                cycle        2,638
    Memory Throughput                 %         0.99
    DRAM Throughput                   %         0.51
    Duration                         us         3.33
    L1/TEX Cache Throughput           %         3.84
    L2 Cache Throughput               %         0.99
    SM Active Cycles              cycle       312.20
    Compute (SM) Throughput           %         0.59
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.82
    Achieved Active Warps Per SM           warp         7.11
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.18%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (14.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        90.67
    Total DRAM Elapsed Cycles        cycle      106,496
    Average L1 Active Cycles         cycle       312.20
    Total L1 Elapsed Cycles          cycle       76,490
    Average L2 Active Cycles         cycle       224.29
    Total L2 Elapsed Cycles          cycle       60,408
    Average SM Active Cycles         cycle       312.20
    Total SM Elapsed Cycles          cycle       76,490
    Average SMSP Active Cycles       cycle       275.63
    Total SMSP Elapsed Cycles        cycle      305,960
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.221%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.14% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.572%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 70.04% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.221%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.14% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.954%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 66.82% above the average, while the minimum instance value is 83.50% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.24
    Elapsed Cycles                cycle       48,751
    Memory Throughput                 %        46.46
    DRAM Throughput                   %        46.46
    Duration                         us        59.65
    L1/TEX Cache Throughput           %        36.51
    L2 Cache Throughput               %        34.71
    SM Active Cycles              cycle    46,316.50
    Compute (SM) Throughput           %        70.86
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.65
    Achieved Active Warps Per SM           warp        38.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.35%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      152,080
    Total DRAM Elapsed Cycles        cycle    1,964,032
    Average L1 Active Cycles         cycle    46,316.50
    Total L1 Elapsed Cycles          cycle    1,447,440
    Average L2 Active Cycles         cycle    43,130.79
    Total L2 Elapsed Cycles          cycle    1,116,360
    Average SM Active Cycles         cycle    46,316.50
    Total SM Elapsed Cycles          cycle    1,447,440
    Average SMSP Active Cycles       cycle    46,041.26
    Total SMSP Elapsed Cycles        cycle    5,789,760
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.54
    Elapsed Cycles                cycle       34,519
    Memory Throughput                 %        64.18
    DRAM Throughput                   %        64.18
    Duration                         us        42.27
    L1/TEX Cache Throughput           %        38.53
    L2 Cache Throughput               %        31.78
    SM Active Cycles              cycle    31,307.20
    Compute (SM) Throughput           %        52.35
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.05
    Achieved Active Warps Per SM           warp        37.46
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.95%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   148,757.33
    Total DRAM Elapsed Cycles        cycle    1,390,592
    Average L1 Active Cycles         cycle    31,307.20
    Total L1 Elapsed Cycles          cycle      993,450
    Average L2 Active Cycles         cycle    29,413.50
    Total L2 Elapsed Cycles          cycle      790,464
    Average SM Active Cycles         cycle    31,307.20
    Total SM Elapsed Cycles          cycle      993,450
    Average SMSP Active Cycles       cycle    31,375.42
    Total SMSP Elapsed Cycles        cycle    3,973,800
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.43
    SM Frequency                    Mhz       798.91
    Elapsed Cycles                cycle        2,688
    Memory Throughput                 %         0.95
    DRAM Throughput                   %         0.45
    Duration                         us         3.36
    L1/TEX Cache Throughput           %         4.02
    L2 Cache Throughput               %         0.95
    SM Active Cycles              cycle       298.47
    Compute (SM) Throughput           %         0.66
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.97
    Achieved Active Warps Per SM           warp         7.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.03%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        82.67
    Total DRAM Elapsed Cycles        cycle      109,568
    Average L1 Active Cycles         cycle       298.47
    Total L1 Elapsed Cycles          cycle       76,990
    Average L2 Active Cycles         cycle       185.04
    Total L2 Elapsed Cycles          cycle       61,464
    Average SM Active Cycles         cycle       298.47
    Total SM Elapsed Cycles          cycle       76,990
    Average SMSP Active Cycles       cycle       265.80
    Total SMSP Elapsed Cycles        cycle      307,960
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.824%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.27% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.254%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 70.03% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.824%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.27% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.113%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.77% above the average, while the minimum instance value is 80.00% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.17
    Elapsed Cycles                cycle       48,854
    Memory Throughput                 %        49.19
    DRAM Throughput                   %        49.19
    Duration                         us        59.78
    L1/TEX Cache Throughput           %        36.36
    L2 Cache Throughput               %        34.62
    SM Active Cycles              cycle    46,508.10
    Compute (SM) Throughput           %        70.69
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.30
    Achieved Active Warps Per SM           warp        38.06
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.7%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,341.33
    Total DRAM Elapsed Cycles        cycle    1,968,128
    Average L1 Active Cycles         cycle    46,508.10
    Total L1 Elapsed Cycles          cycle    1,450,440
    Average L2 Active Cycles         cycle    43,064.25
    Total L2 Elapsed Cycles          cycle    1,118,640
    Average SM Active Cycles         cycle    46,508.10
    Total SM Elapsed Cycles          cycle    1,450,440
    Average SMSP Active Cycles       cycle    46,136.27
    Total SMSP Elapsed Cycles        cycle    5,801,760
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.25
    Elapsed Cycles                cycle       34,875
    Memory Throughput                 %        68.84
    DRAM Throughput                   %        68.84
    Duration                         us        42.72
    L1/TEX Cache Throughput           %        38.91
    L2 Cache Throughput               %        31.56
    SM Active Cycles              cycle    31,583.67
    Compute (SM) Throughput           %        52.85
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.75
    Achieved Active Warps Per SM           warp        37.32
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.25%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,301.33
    Total DRAM Elapsed Cycles        cycle    1,405,952
    Average L1 Active Cycles         cycle    31,583.67
    Total L1 Elapsed Cycles          cycle      983,690
    Average L2 Active Cycles         cycle    29,343.67
    Total L2 Elapsed Cycles          cycle      798,600
    Average SM Active Cycles         cycle    31,583.67
    Total SM Elapsed Cycles          cycle      983,690
    Average SMSP Active Cycles       cycle    31,489.05
    Total SMSP Elapsed Cycles        cycle    3,934,760
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.15
    Elapsed Cycles                cycle       48,772
    Memory Throughput                 %        49.25
    DRAM Throughput                   %        49.25
    Duration                         us        59.68
    L1/TEX Cache Throughput           %        36.43
    L2 Cache Throughput               %        34.67
    SM Active Cycles              cycle    46,411.57
    Compute (SM) Throughput           %        70.85
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.55
    Achieved Active Warps Per SM           warp        38.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.45%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,376
    Total DRAM Elapsed Cycles        cycle    1,966,080
    Average L1 Active Cycles         cycle    46,411.57
    Total L1 Elapsed Cycles          cycle    1,446,560
    Average L2 Active Cycles         cycle    43,018.88
    Total L2 Elapsed Cycles          cycle    1,116,840
    Average SM Active Cycles         cycle    46,411.57
    Total SM Elapsed Cycles          cycle    1,446,560
    Average SMSP Active Cycles       cycle    46,155.46
    Total SMSP Elapsed Cycles        cycle    5,786,240
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.49
    Elapsed Cycles                cycle       33,474
    Memory Throughput                 %        65.61
    DRAM Throughput                   %        65.61
    Duration                         us        40.99
    L1/TEX Cache Throughput           %        37.83
    L2 Cache Throughput               %        32.90
    SM Active Cycles              cycle    31,286.50
    Compute (SM) Throughput           %        51.36
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.07
    Achieved Active Warps Per SM           warp        37.47
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.93%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      147,472
    Total DRAM Elapsed Cycles        cycle    1,348,608
    Average L1 Active Cycles         cycle    31,286.50
    Total L1 Elapsed Cycles          cycle    1,012,020
    Average L2 Active Cycles         cycle    28,996.38
    Total L2 Elapsed Cycles          cycle      766,368
    Average SM Active Cycles         cycle    31,286.50
    Total SM Elapsed Cycles          cycle    1,012,020
    Average SMSP Active Cycles       cycle    31,965.72
    Total SMSP Elapsed Cycles        cycle    4,048,080
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.44
    SM Frequency                    Mhz       796.42
    Elapsed Cycles                cycle        2,626
    Memory Throughput                 %         0.97
    DRAM Throughput                   %         0.48
    Duration                         us         3.30
    L1/TEX Cache Throughput           %         3.89
    L2 Cache Throughput               %         0.97
    SM Active Cycles              cycle       308.47
    Compute (SM) Throughput           %         0.66
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.02
    Achieved Active Warps Per SM           warp         7.21
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        85.33
    Total DRAM Elapsed Cycles        cycle      107,520
    Average L1 Active Cycles         cycle       308.47
    Total L1 Elapsed Cycles          cycle       76,380
    Average L2 Active Cycles         cycle       217.12
    Total L2 Elapsed Cycles          cycle       60,144
    Average SM Active Cycles         cycle       308.47
    Total SM Elapsed Cycles          cycle       76,380
    Average SMSP Active Cycles       cycle       279.47
    Total SMSP Elapsed Cycles        cycle      305,520
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.144%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.22% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.675%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 69.92% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.144%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.22% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.729%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 66.13% above the average, while the minimum instance value is 82.96% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.21
    Elapsed Cycles                cycle       48,878
    Memory Throughput                 %        49.28
    DRAM Throughput                   %        49.28
    Duration                         us        59.81
    L1/TEX Cache Throughput           %        36.41
    L2 Cache Throughput               %        34.51
    SM Active Cycles              cycle    46,440.50
    Compute (SM) Throughput           %        70.58
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.65
    Achieved Active Warps Per SM           warp        38.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.35%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,744
    Total DRAM Elapsed Cycles        cycle    1,969,152
    Average L1 Active Cycles         cycle    46,440.50
    Total L1 Elapsed Cycles          cycle    1,451,470
    Average L2 Active Cycles         cycle    42,953.79
    Total L2 Elapsed Cycles          cycle    1,119,144
    Average SM Active Cycles         cycle    46,440.50
    Total SM Elapsed Cycles          cycle    1,451,470
    Average SMSP Active Cycles       cycle    46,131.25
    Total SMSP Elapsed Cycles        cycle    5,805,880
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       815.78
    Elapsed Cycles                cycle       33,652
    Memory Throughput                 %        65.52
    DRAM Throughput                   %        65.52
    Duration                         us        41.25
    L1/TEX Cache Throughput           %        38.41
    L2 Cache Throughput               %        32.65
    SM Active Cycles              cycle    31,476.97
    Compute (SM) Throughput           %        52.13
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.59
    Achieved Active Warps Per SM           warp        37.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.41%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   148,045.33
    Total DRAM Elapsed Cycles        cycle    1,355,776
    Average L1 Active Cycles         cycle    31,476.97
    Total L1 Elapsed Cycles          cycle      996,520
    Average L2 Active Cycles         cycle    29,876.54
    Total L2 Elapsed Cycles          cycle      770,568
    Average SM Active Cycles         cycle    31,476.97
    Total SM Elapsed Cycles          cycle      996,520
    Average SMSP Active Cycles       cycle    31,378.88
    Total SMSP Elapsed Cycles        cycle    3,986,080
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.33
    SM Frequency                    Mhz       795.00
    Elapsed Cycles                cycle        2,622
    Memory Throughput                 %         0.99
    DRAM Throughput                   %         0.44
    Duration                         us         3.30
    L1/TEX Cache Throughput           %         3.96
    L2 Cache Throughput               %         0.99
    SM Active Cycles              cycle       302.83
    Compute (SM) Throughput           %         0.60
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.82
    Achieved Active Warps Per SM           warp         7.12
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.18%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (14.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        77.33
    Total DRAM Elapsed Cycles        cycle      105,472
    Average L1 Active Cycles         cycle       302.83
    Total L1 Elapsed Cycles          cycle       77,660
    Average L2 Active Cycles         cycle       207.71
    Total L2 Elapsed Cycles          cycle       60,024
    Average SM Active Cycles         cycle       302.83
    Total SM Elapsed Cycles          cycle       77,660
    Average SMSP Active Cycles       cycle       270.95
    Total SMSP Elapsed Cycles        cycle      310,640
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.856%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.15% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.319%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 69.93% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.856%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.15% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.157%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 62.10% above the average, while the minimum instance value is 82.19% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.11
    Elapsed Cycles                cycle       48,664
    Memory Throughput                 %        45.36
    DRAM Throughput                   %        45.36
    Duration                         us        59.55
    L1/TEX Cache Throughput           %        36.44
    L2 Cache Throughput               %        34.74
    SM Active Cycles              cycle    46,404.37
    Compute (SM) Throughput           %        70.76
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.55
    Achieved Active Warps Per SM           warp        38.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.45%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      148,248
    Total DRAM Elapsed Cycles        cycle    1,960,960
    Average L1 Active Cycles         cycle    46,404.37
    Total L1 Elapsed Cycles          cycle    1,447,050
    Average L2 Active Cycles         cycle    42,934.71
    Total L2 Elapsed Cycles          cycle    1,114,272
    Average SM Active Cycles         cycle    46,404.37
    Total SM Elapsed Cycles          cycle    1,447,050
    Average SMSP Active Cycles       cycle    46,217.48
    Total SMSP Elapsed Cycles        cycle    5,788,200
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.34
    Elapsed Cycles                cycle       34,567
    Memory Throughput                 %        69.56
    DRAM Throughput                   %        69.56
    Duration                         us        42.34
    L1/TEX Cache Throughput           %        37.73
    L2 Cache Throughput               %        31.75
    SM Active Cycles              cycle    31,745.47
    Compute (SM) Throughput           %        51.19
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.92
    Achieved Active Warps Per SM           warp        37.40
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.08%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,568
    Total DRAM Elapsed Cycles        cycle    1,393,664
    Average L1 Active Cycles         cycle    31,745.47
    Total L1 Elapsed Cycles          cycle    1,014,500
    Average L2 Active Cycles         cycle    29,586.58
    Total L2 Elapsed Cycles          cycle      791,544
    Average SM Active Cycles         cycle    31,745.47
    Total SM Elapsed Cycles          cycle    1,014,500
    Average SMSP Active Cycles       cycle    30,921.75
    Total SMSP Elapsed Cycles        cycle    4,058,000
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.18
    Elapsed Cycles                cycle       48,958
    Memory Throughput                 %        49.28
    DRAM Throughput                   %        49.28
    Duration                         us        59.90
    L1/TEX Cache Throughput           %        36.34
    L2 Cache Throughput               %        34.53
    SM Active Cycles              cycle    46,532.03
    Compute (SM) Throughput           %        70.77
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.57
    Achieved Active Warps Per SM           warp        38.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.43%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,989.33
    Total DRAM Elapsed Cycles        cycle    1,972,224
    Average L1 Active Cycles         cycle    46,532.03
    Total L1 Elapsed Cycles          cycle    1,446,000
    Average L2 Active Cycles         cycle    42,982.21
    Total L2 Elapsed Cycles          cycle    1,121,016
    Average SM Active Cycles         cycle    46,532.03
    Total SM Elapsed Cycles          cycle    1,446,000
    Average SMSP Active Cycles       cycle    46,133.31
    Total SMSP Elapsed Cycles        cycle    5,784,000
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.26
    Elapsed Cycles                cycle       34,117
    Memory Throughput                 %        64.44
    DRAM Throughput                   %        64.44
    Duration                         us        41.79
    L1/TEX Cache Throughput           %        38.07
    L2 Cache Throughput               %        32.17
    SM Active Cycles              cycle    32,087.77
    Compute (SM) Throughput           %        51.63
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.63
    Achieved Active Warps Per SM           warp        37.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.37%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   147,578.67
    Total DRAM Elapsed Cycles        cycle    1,374,208
    Average L1 Active Cycles         cycle    32,087.77
    Total L1 Elapsed Cycles          cycle    1,005,420
    Average L2 Active Cycles         cycle    29,118.54
    Total L2 Elapsed Cycles          cycle      781,200
    Average SM Active Cycles         cycle    32,087.77
    Total SM Elapsed Cycles          cycle    1,005,420
    Average SMSP Active Cycles       cycle    31,281.09
    Total SMSP Elapsed Cycles        cycle    4,021,680
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.33
    SM Frequency                    Mhz       796.22
    Elapsed Cycles                cycle        2,626
    Memory Throughput                 %         0.98
    DRAM Throughput                   %         0.44
    Duration                         us         3.30
    L1/TEX Cache Throughput           %         3.90
    L2 Cache Throughput               %         0.98
    SM Active Cycles              cycle       307.93
    Compute (SM) Throughput           %         0.66
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.02
    Achieved Active Warps Per SM           warp         7.21
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        77.33
    Total DRAM Elapsed Cycles        cycle      105,472
    Average L1 Active Cycles         cycle       307.93
    Total L1 Elapsed Cycles          cycle       76,880
    Average L2 Active Cycles         cycle       209.58
    Total L2 Elapsed Cycles          cycle       60,096
    Average SM Active Cycles         cycle       307.93
    Total SM Elapsed Cycles          cycle       76,880
    Average SMSP Active Cycles       cycle       287.23
    Total SMSP Elapsed Cycles        cycle      307,520
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.08%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.24% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.834%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 69.89% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.08%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.24% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.361%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 64.05% above the average, while the minimum instance value is 82.35% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.32
    Elapsed Cycles                cycle       48,627
    Memory Throughput                 %        45.27
    DRAM Throughput                   %        45.27
    Duration                         us        59.49
    L1/TEX Cache Throughput           %        36.46
    L2 Cache Throughput               %        34.82
    SM Active Cycles              cycle    46,372.33
    Compute (SM) Throughput           %        70.77
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.60
    Achieved Active Warps Per SM           warp        38.21
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.4%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   147,786.67
    Total DRAM Elapsed Cycles        cycle    1,958,912
    Average L1 Active Cycles         cycle    46,372.33
    Total L1 Elapsed Cycles          cycle    1,445,340
    Average L2 Active Cycles         cycle    43,042.71
    Total L2 Elapsed Cycles          cycle    1,113,408
    Average SM Active Cycles         cycle    46,372.33
    Total SM Elapsed Cycles          cycle    1,445,340
    Average SMSP Active Cycles       cycle    46,015.17
    Total SMSP Elapsed Cycles        cycle    5,781,360
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.87
    Elapsed Cycles                cycle       35,369
    Memory Throughput                 %        67.85
    DRAM Throughput                   %        67.85
    Duration                         us        43.30
    L1/TEX Cache Throughput           %        38.20
    L2 Cache Throughput               %        31.03
    SM Active Cycles              cycle    31,144.60
    Compute (SM) Throughput           %        51.77
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.18
    Achieved Active Warps Per SM           warp        37.53
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.82%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,178.67
    Total DRAM Elapsed Cycles        cycle    1,425,408
    Average L1 Active Cycles         cycle    31,144.60
    Total L1 Elapsed Cycles          cycle    1,002,100
    Average L2 Active Cycles         cycle    29,613.67
    Total L2 Elapsed Cycles          cycle      809,928
    Average SM Active Cycles         cycle    31,144.60
    Total SM Elapsed Cycles          cycle    1,002,100
    Average SMSP Active Cycles       cycle    31,781.35
    Total SMSP Elapsed Cycles        cycle    4,008,400
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.09
    Elapsed Cycles                cycle       48,639
    Memory Throughput                 %        49.43
    DRAM Throughput                   %        49.43
    Duration                         us        59.52
    L1/TEX Cache Throughput           %        36.57
    L2 Cache Throughput               %        34.71
    SM Active Cycles              cycle    46,241.87
    Compute (SM) Throughput           %        70.61
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.60
    Achieved Active Warps Per SM           warp        38.21
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.4%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,368
    Total DRAM Elapsed Cycles        cycle    1,958,912
    Average L1 Active Cycles         cycle    46,241.87
    Total L1 Elapsed Cycles          cycle    1,447,690
    Average L2 Active Cycles         cycle    43,027.21
    Total L2 Elapsed Cycles          cycle    1,113,792
    Average SM Active Cycles         cycle    46,241.87
    Total SM Elapsed Cycles          cycle    1,447,690
    Average SMSP Active Cycles       cycle    46,046.15
    Total SMSP Elapsed Cycles        cycle    5,790,760
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.12
    Elapsed Cycles                cycle       35,002
    Memory Throughput                 %        63.99
    DRAM Throughput                   %        63.99
    Duration                         us        42.88
    L1/TEX Cache Throughput           %        37.79
    L2 Cache Throughput               %        31.36
    SM Active Cycles              cycle    31,644.60
    Compute (SM) Throughput           %        51.20
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.47
    Achieved Active Warps Per SM           warp        37.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.53%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      150,392
    Total DRAM Elapsed Cycles        cycle    1,410,048
    Average L1 Active Cycles         cycle    31,644.60
    Total L1 Elapsed Cycles          cycle    1,012,830
    Average L2 Active Cycles         cycle       29,476
    Total L2 Elapsed Cycles          cycle      801,504
    Average SM Active Cycles         cycle    31,644.60
    Total SM Elapsed Cycles          cycle    1,012,830
    Average SMSP Active Cycles       cycle    31,273.33
    Total SMSP Elapsed Cycles        cycle    4,051,320
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.34
    Elapsed Cycles                cycle       48,341
    Memory Throughput                 %        45.81
    DRAM Throughput                   %        45.81
    Duration                         us        59.14
    L1/TEX Cache Throughput           %        36.48
    L2 Cache Throughput               %        35.00
    SM Active Cycles              cycle    46,347.80
    Compute (SM) Throughput           %        70.50
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.36
    Achieved Active Warps Per SM           warp        38.09
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.64%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      148,688
    Total DRAM Elapsed Cycles        cycle    1,947,648
    Average L1 Active Cycles         cycle    46,347.80
    Total L1 Elapsed Cycles          cycle    1,448,830
    Average L2 Active Cycles         cycle    42,887.83
    Total L2 Elapsed Cycles          cycle    1,106,904
    Average SM Active Cycles         cycle    46,347.80
    Total SM Elapsed Cycles          cycle    1,448,830
    Average SMSP Active Cycles       cycle    46,023.67
    Total SMSP Elapsed Cycles        cycle    5,795,320
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.22
    Elapsed Cycles                cycle       34,951
    Memory Throughput                 %        68.84
    DRAM Throughput                   %        68.84
    Duration                         us        42.82
    L1/TEX Cache Throughput           %        37.58
    L2 Cache Throughput               %        31.41
    SM Active Cycles              cycle    31,161.10
    Compute (SM) Throughput           %        50.88
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.03
    Achieved Active Warps Per SM           warp        37.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.97%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,552
    Total DRAM Elapsed Cycles        cycle    1,408,000
    Average L1 Active Cycles         cycle    31,161.10
    Total L1 Elapsed Cycles          cycle    1,018,540
    Average L2 Active Cycles         cycle    29,280.08
    Total L2 Elapsed Cycles          cycle      800,304
    Average SM Active Cycles         cycle    31,161.10
    Total SM Elapsed Cycles          cycle    1,018,540
    Average SMSP Active Cycles       cycle    31,731.74
    Total SMSP Elapsed Cycles        cycle    4,074,160
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.26
    Elapsed Cycles                cycle       48,778
    Memory Throughput                 %        49.53
    DRAM Throughput                   %        49.53
    Duration                         us        59.68
    L1/TEX Cache Throughput           %        36.57
    L2 Cache Throughput               %        34.48
    SM Active Cycles              cycle    46,231.50
    Compute (SM) Throughput           %        70.72
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.41
    Achieved Active Warps Per SM           warp        38.12
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.59%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   162,202.67
    Total DRAM Elapsed Cycles        cycle    1,965,056
    Average L1 Active Cycles         cycle    46,231.50
    Total L1 Elapsed Cycles          cycle    1,443,220
    Average L2 Active Cycles         cycle    42,938.25
    Total L2 Elapsed Cycles          cycle    1,116,960
    Average SM Active Cycles         cycle    46,231.50
    Total SM Elapsed Cycles          cycle    1,443,220
    Average SMSP Active Cycles       cycle    45,943.37
    Total SMSP Elapsed Cycles        cycle    5,772,880
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.96
    Elapsed Cycles                cycle       35,348
    Memory Throughput                 %        68.23
    DRAM Throughput                   %        68.23
    Duration                         us        43.26
    L1/TEX Cache Throughput           %        37.89
    L2 Cache Throughput               %        31.04
    SM Active Cycles              cycle    31,822.87
    Compute (SM) Throughput           %        51.26
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.68
    Achieved Active Warps Per SM           warp        37.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.32%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   162,085.33
    Total DRAM Elapsed Cycles        cycle    1,425,408
    Average L1 Active Cycles         cycle    31,822.87
    Total L1 Elapsed Cycles          cycle    1,010,340
    Average L2 Active Cycles         cycle    29,963.25
    Total L2 Elapsed Cycles          cycle      809,472
    Average SM Active Cycles         cycle    31,822.87
    Total SM Elapsed Cycles          cycle    1,010,340
    Average SMSP Active Cycles       cycle    31,541.79
    Total SMSP Elapsed Cycles        cycle    4,041,360
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.11
    Elapsed Cycles                cycle       48,509
    Memory Throughput                 %        45.28
    DRAM Throughput                   %        45.28
    Duration                         us        59.36
    L1/TEX Cache Throughput           %        36.65
    L2 Cache Throughput               %        34.84
    SM Active Cycles              cycle    46,136.97
    Compute (SM) Throughput           %        70.33
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.47
    Achieved Active Warps Per SM           warp        38.15
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.53%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   147,530.67
    Total DRAM Elapsed Cycles        cycle    1,954,816
    Average L1 Active Cycles         cycle    46,136.97
    Total L1 Elapsed Cycles          cycle    1,450,060
    Average L2 Active Cycles         cycle    42,812.92
    Total L2 Elapsed Cycles          cycle    1,110,768
    Average SM Active Cycles         cycle    46,136.97
    Total SM Elapsed Cycles          cycle    1,450,060
    Average SMSP Active Cycles       cycle    45,893.04
    Total SMSP Elapsed Cycles        cycle    5,800,240
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.05
    Elapsed Cycles                cycle       33,978
    Memory Throughput                 %        65.37
    DRAM Throughput                   %        65.37
    Duration                         us        41.63
    L1/TEX Cache Throughput           %        38.08
    L2 Cache Throughput               %        32.40
    SM Active Cycles              cycle    31,870.93
    Compute (SM) Throughput           %        51.49
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.69
    Achieved Active Warps Per SM           warp        37.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.31%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   149,162.67
    Total DRAM Elapsed Cycles        cycle    1,369,088
    Average L1 Active Cycles         cycle    31,870.93
    Total L1 Elapsed Cycles          cycle    1,005,190
    Average L2 Active Cycles         cycle    29,430.92
    Total L2 Elapsed Cycles          cycle      778,104
    Average SM Active Cycles         cycle    31,870.93
    Total SM Elapsed Cycles          cycle    1,005,190
    Average SMSP Active Cycles       cycle    31,833.96
    Total SMSP Elapsed Cycles        cycle    4,020,760
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.33
    SM Frequency                    Mhz       801.57
    Elapsed Cycles                cycle        2,619
    Memory Throughput                 %         0.99
    DRAM Throughput                   %         0.49
    Duration                         us         3.26
    L1/TEX Cache Throughput           %         3.98
    L2 Cache Throughput               %         0.99
    SM Active Cycles              cycle       301.17
    Compute (SM) Throughput           %         0.63
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.98
    Achieved Active Warps Per SM           warp         7.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.02%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        85.33
    Total DRAM Elapsed Cycles        cycle      104,448
    Average L1 Active Cycles         cycle       301.17
    Total L1 Elapsed Cycles          cycle       79,720
    Average L2 Active Cycles         cycle       219.42
    Total L2 Elapsed Cycles          cycle       59,904
    Average SM Active Cycles         cycle       301.17
    Total SM Elapsed Cycles          cycle       79,720
    Average SMSP Active Cycles       cycle          266
    Total SMSP Elapsed Cycles        cycle      318,880
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.615%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.19% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.015%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 70.08% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.615%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.19% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.454%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 62.04% above the average, while the minimum instance value is 83.14% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.32
    Elapsed Cycles                cycle       48,676
    Memory Throughput                 %        49.49
    DRAM Throughput                   %        49.49
    Duration                         us        59.55
    L1/TEX Cache Throughput           %        36.38
    L2 Cache Throughput               %        34.71
    SM Active Cycles              cycle    46,475.77
    Compute (SM) Throughput           %        70.38
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.47
    Achieved Active Warps Per SM           warp        38.15
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.53%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,730.67
    Total DRAM Elapsed Cycles        cycle    1,960,960
    Average L1 Active Cycles         cycle    46,475.77
    Total L1 Elapsed Cycles          cycle    1,448,100
    Average L2 Active Cycles         cycle       43,023
    Total L2 Elapsed Cycles          cycle    1,114,560
    Average SM Active Cycles         cycle    46,475.77
    Total SM Elapsed Cycles          cycle    1,448,100
    Average SMSP Active Cycles       cycle    45,912.27
    Total SMSP Elapsed Cycles        cycle    5,792,400
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.26
    Elapsed Cycles                cycle       33,361
    Memory Throughput                 %        65.97
    DRAM Throughput                   %        65.97
    Duration                         us        40.86
    L1/TEX Cache Throughput           %        38.20
    L2 Cache Throughput               %        32.89
    SM Active Cycles              cycle    31,743.33
    Compute (SM) Throughput           %        51.62
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.66
    Achieved Active Warps Per SM           warp        37.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.34%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   147,949.33
    Total DRAM Elapsed Cycles        cycle    1,345,536
    Average L1 Active Cycles         cycle    31,743.33
    Total L1 Elapsed Cycles          cycle    1,002,000
    Average L2 Active Cycles         cycle    29,385.29
    Total L2 Elapsed Cycles          cycle      763,944
    Average SM Active Cycles         cycle    31,743.33
    Total SM Elapsed Cycles          cycle    1,002,000
    Average SMSP Active Cycles       cycle    30,771.52
    Total SMSP Elapsed Cycles        cycle    4,008,000
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.79
    Elapsed Cycles                cycle       48,335
    Memory Throughput                 %        46.07
    DRAM Throughput                   %        46.07
    Duration                         us        59.17
    L1/TEX Cache Throughput           %        36.64
    L2 Cache Throughput               %        34.92
    SM Active Cycles              cycle    46,147.53
    Compute (SM) Throughput           %        70.53
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.51
    Achieved Active Warps Per SM           warp        38.17
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.49%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   149,546.67
    Total DRAM Elapsed Cycles        cycle    1,947,648
    Average L1 Active Cycles         cycle    46,147.53
    Total L1 Elapsed Cycles          cycle    1,443,810
    Average L2 Active Cycles         cycle    42,938.75
    Total L2 Elapsed Cycles          cycle    1,106,736
    Average SM Active Cycles         cycle    46,147.53
    Total SM Elapsed Cycles          cycle    1,443,810
    Average SMSP Active Cycles       cycle    46,080.81
    Total SMSP Elapsed Cycles        cycle    5,775,240
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.49
    Elapsed Cycles                cycle       35,512
    Memory Throughput                 %        67.82
    DRAM Throughput                   %        67.82
    Duration                         us        43.49
    L1/TEX Cache Throughput           %        38.11
    L2 Cache Throughput               %        30.91
    SM Active Cycles              cycle    31,618.20
    Compute (SM) Throughput           %        51.46
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.89
    Achieved Active Warps Per SM           warp        37.39
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.11%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,696
    Total DRAM Elapsed Cycles        cycle    1,430,528
    Average L1 Active Cycles         cycle    31,618.20
    Total L1 Elapsed Cycles          cycle    1,004,570
    Average L2 Active Cycles         cycle    29,074.25
    Total L2 Elapsed Cycles          cycle      813,168
    Average SM Active Cycles         cycle    31,618.20
    Total SM Elapsed Cycles          cycle    1,004,570
    Average SMSP Active Cycles       cycle    31,547.16
    Total SMSP Elapsed Cycles        cycle    4,018,280
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.32
    Elapsed Cycles                cycle       48,494
    Memory Throughput                 %        45.90
    DRAM Throughput                   %        45.90
    Duration                         us        59.33
    L1/TEX Cache Throughput           %        36.58
    L2 Cache Throughput               %        34.86
    SM Active Cycles              cycle    46,216.97
    Compute (SM) Throughput           %        70.60
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.48
    Achieved Active Warps Per SM           warp        38.15
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.52%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   149,458.67
    Total DRAM Elapsed Cycles        cycle    1,953,792
    Average L1 Active Cycles         cycle    46,216.97
    Total L1 Elapsed Cycles          cycle    1,441,240
    Average L2 Active Cycles         cycle    42,908.38
    Total L2 Elapsed Cycles          cycle    1,110,504
    Average SM Active Cycles         cycle    46,216.97
    Total SM Elapsed Cycles          cycle    1,441,240
    Average SMSP Active Cycles       cycle    45,907.09
    Total SMSP Elapsed Cycles        cycle    5,764,960
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.76
    Elapsed Cycles                cycle       35,263
    Memory Throughput                 %        68.12
    DRAM Throughput                   %        68.12
    Duration                         us        43.17
    L1/TEX Cache Throughput           %        38.62
    L2 Cache Throughput               %        31.13
    SM Active Cycles              cycle    31,429.57
    Compute (SM) Throughput           %        52.14
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.60
    Achieved Active Warps Per SM           warp        37.25
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.4%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,138.67
    Total DRAM Elapsed Cycles        cycle    1,419,264
    Average L1 Active Cycles         cycle    31,429.57
    Total L1 Elapsed Cycles          cycle      990,950
    Average L2 Active Cycles         cycle    29,451.42
    Total L2 Elapsed Cycles          cycle      807,408
    Average SM Active Cycles         cycle    31,429.57
    Total SM Elapsed Cycles          cycle      990,950
    Average SMSP Active Cycles       cycle    31,295.18
    Total SMSP Elapsed Cycles        cycle    3,963,800
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.44
    SM Frequency                    Mhz       803.30
    Elapsed Cycles                cycle        2,648
    Memory Throughput                 %         0.96
    DRAM Throughput                   %         0.45
    Duration                         us         3.30
    L1/TEX Cache Throughput           %         3.90
    L2 Cache Throughput               %         0.96
    SM Active Cycles              cycle       307.63
    Compute (SM) Throughput           %         0.66
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.02
    Achieved Active Warps Per SM           warp         7.21
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle           80
    Total DRAM Elapsed Cycles        cycle      107,520
    Average L1 Active Cycles         cycle       307.63
    Total L1 Elapsed Cycles          cycle       76,670
    Average L2 Active Cycles         cycle       211.08
    Total L2 Elapsed Cycles          cycle       60,672
    Average SM Active Cycles         cycle       307.63
    Total SM Elapsed Cycles          cycle       76,670
    Average SMSP Active Cycles       cycle          274
    Total SMSP Elapsed Cycles        cycle      306,680
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.094%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.24% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.5%                                                                                            
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 69.96% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.094%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.24% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.342%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.98% above the average, while the minimum instance value is 82.47% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.38
    Elapsed Cycles                cycle       48,551
    Memory Throughput                 %        49.47
    DRAM Throughput                   %        49.47
    Duration                         us        59.39
    L1/TEX Cache Throughput           %        36.74
    L2 Cache Throughput               %        34.84
    SM Active Cycles              cycle    46,023.33
    Compute (SM) Throughput           %        70.48
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.52
    Achieved Active Warps Per SM           warp        38.17
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.48%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,352
    Total DRAM Elapsed Cycles        cycle    1,956,864
    Average L1 Active Cycles         cycle    46,023.33
    Total L1 Elapsed Cycles          cycle    1,442,550
    Average L2 Active Cycles         cycle       42,860
    Total L2 Elapsed Cycles          cycle    1,111,680
    Average SM Active Cycles         cycle    46,023.33
    Total SM Elapsed Cycles          cycle    1,442,550
    Average SMSP Active Cycles       cycle    45,707.83
    Total SMSP Elapsed Cycles        cycle    5,770,200
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.21
    Elapsed Cycles                cycle       35,056
    Memory Throughput                 %        68.40
    DRAM Throughput                   %        68.40
    Duration                         us        42.94
    L1/TEX Cache Throughput           %        38.48
    L2 Cache Throughput               %        31.41
    SM Active Cycles              cycle    31,382.43
    Compute (SM) Throughput           %        51.90
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.64
    Achieved Active Warps Per SM           warp        37.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.36%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,104
    Total DRAM Elapsed Cycles        cycle    1,413,120
    Average L1 Active Cycles         cycle    31,382.43
    Total L1 Elapsed Cycles          cycle      994,800
    Average L2 Active Cycles         cycle    29,456.79
    Total L2 Elapsed Cycles          cycle      802,752
    Average SM Active Cycles         cycle    31,382.43
    Total SM Elapsed Cycles          cycle      994,800
    Average SMSP Active Cycles       cycle    31,481.67
    Total SMSP Elapsed Cycles        cycle    3,979,200
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.23
    Elapsed Cycles                cycle       48,252
    Memory Throughput                 %        45.77
    DRAM Throughput                   %        45.77
    Duration                         us        59.04
    L1/TEX Cache Throughput           %        36.79
    L2 Cache Throughput               %        35.02
    SM Active Cycles              cycle    45,958.27
    Compute (SM) Throughput           %        70.32
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.48
    Achieved Active Warps Per SM           warp        38.15
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.52%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   148,349.33
    Total DRAM Elapsed Cycles        cycle    1,944,576
    Average L1 Active Cycles         cycle    45,958.27
    Total L1 Elapsed Cycles          cycle    1,444,660
    Average L2 Active Cycles         cycle    42,870.92
    Total L2 Elapsed Cycles          cycle    1,104,840
    Average SM Active Cycles         cycle    45,958.27
    Total SM Elapsed Cycles          cycle    1,444,660
    Average SMSP Active Cycles       cycle    45,913.73
    Total SMSP Elapsed Cycles        cycle    5,778,640
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.69
    Elapsed Cycles                cycle       35,495
    Memory Throughput                 %        67.71
    DRAM Throughput                   %        67.71
    Duration                         us        43.46
    L1/TEX Cache Throughput           %        38.82
    L2 Cache Throughput               %        30.95
    SM Active Cycles              cycle    31,034.67
    Compute (SM) Throughput           %        52.33
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.78
    Achieved Active Warps Per SM           warp        37.34
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.22%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,549.33
    Total DRAM Elapsed Cycles        cycle    1,431,552
    Average L1 Active Cycles         cycle    31,034.67
    Total L1 Elapsed Cycles          cycle      985,930
    Average L2 Active Cycles         cycle    29,496.54
    Total L2 Elapsed Cycles          cycle      812,760
    Average SM Active Cycles         cycle    31,034.67
    Total SM Elapsed Cycles          cycle      985,930
    Average SMSP Active Cycles       cycle    31,535.29
    Total SMSP Elapsed Cycles        cycle    3,943,720
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.39
    SM Frequency                    Mhz       794.19
    Elapsed Cycles                cycle        2,619
    Memory Throughput                 %         0.98
    DRAM Throughput                   %         0.50
    Duration                         us         3.30
    L1/TEX Cache Throughput           %         3.93
    L2 Cache Throughput               %         0.98
    SM Active Cycles              cycle       305.07
    Compute (SM) Throughput           %         0.65
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.01
    Achieved Active Warps Per SM           warp         7.20
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.99%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle           88
    Total DRAM Elapsed Cycles        cycle      106,496
    Average L1 Active Cycles         cycle       305.07
    Total L1 Elapsed Cycles          cycle       77,460
    Average L2 Active Cycles         cycle       213.25
    Total L2 Elapsed Cycles          cycle       59,904
    Average SM Active Cycles         cycle       305.07
    Total SM Elapsed Cycles          cycle       77,460
    Average SMSP Active Cycles       cycle       271.73
    Total SMSP Elapsed Cycles        cycle      309,840
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.944%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.23% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.371%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 70.04% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.944%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.23% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.325%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 62.32% above the average, while the minimum instance value is 82.65% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.23
    Elapsed Cycles                cycle       48,305
    Memory Throughput                 %        45.91
    DRAM Throughput                   %        45.91
    Duration                         us        59.10
    L1/TEX Cache Throughput           %        36.63
    L2 Cache Throughput               %        34.83
    SM Active Cycles              cycle    46,162.47
    Compute (SM) Throughput           %        70.69
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.49
    Achieved Active Warps Per SM           warp        38.15
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.51%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      148,880
    Total DRAM Elapsed Cycles        cycle    1,945,600
    Average L1 Active Cycles         cycle    46,162.47
    Total L1 Elapsed Cycles          cycle    1,435,630
    Average L2 Active Cycles         cycle    42,795.21
    Total L2 Elapsed Cycles          cycle    1,106,136
    Average SM Active Cycles         cycle    46,162.47
    Total SM Elapsed Cycles          cycle    1,435,630
    Average SMSP Active Cycles       cycle    45,915.37
    Total SMSP Elapsed Cycles        cycle    5,742,520
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.75
    Elapsed Cycles                cycle       35,210
    Memory Throughput                 %        68.57
    DRAM Throughput                   %        68.57
    Duration                         us        43.10
    L1/TEX Cache Throughput           %        38.10
    L2 Cache Throughput               %        31.16
    SM Active Cycles              cycle    31,639.40
    Compute (SM) Throughput           %        51.31
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.79
    Achieved Active Warps Per SM           warp        37.34
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.21%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      162,208
    Total DRAM Elapsed Cycles        cycle    1,419,264
    Average L1 Active Cycles         cycle    31,639.40
    Total L1 Elapsed Cycles          cycle    1,004,680
    Average L2 Active Cycles         cycle    29,391.79
    Total L2 Elapsed Cycles          cycle      806,256
    Average SM Active Cycles         cycle    31,639.40
    Total SM Elapsed Cycles          cycle    1,004,680
    Average SMSP Active Cycles       cycle    31,475.20
    Total SMSP Elapsed Cycles        cycle    4,018,720
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.26
    Elapsed Cycles                cycle       48,491
    Memory Throughput                 %        49.65
    DRAM Throughput                   %        49.65
    Duration                         us        59.33
    L1/TEX Cache Throughput           %        36.81
    L2 Cache Throughput               %        34.79
    SM Active Cycles              cycle    45,934.97
    Compute (SM) Throughput           %        70.42
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.42
    Achieved Active Warps Per SM           warp        38.12
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.58%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,680
    Total DRAM Elapsed Cycles        cycle    1,953,792
    Average L1 Active Cycles         cycle    45,934.97
    Total L1 Elapsed Cycles          cycle    1,439,920
    Average L2 Active Cycles         cycle    42,813.96
    Total L2 Elapsed Cycles          cycle    1,110,360
    Average SM Active Cycles         cycle    45,934.97
    Total SM Elapsed Cycles          cycle    1,439,920
    Average SMSP Active Cycles       cycle    45,692.43
    Total SMSP Elapsed Cycles        cycle    5,759,680
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.83
    Elapsed Cycles                cycle       34,742
    Memory Throughput                 %        69.09
    DRAM Throughput                   %        69.09
    Duration                         us        42.53
    L1/TEX Cache Throughput           %        37.98
    L2 Cache Throughput               %        31.59
    SM Active Cycles              cycle    30,998.20
    Compute (SM) Throughput           %        51.10
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.23
    Achieved Active Warps Per SM           warp        37.07
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.77%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,072
    Total DRAM Elapsed Cycles        cycle    1,398,784
    Average L1 Active Cycles         cycle    30,998.20
    Total L1 Elapsed Cycles          cycle    1,008,030
    Average L2 Active Cycles         cycle    29,300.42
    Total L2 Elapsed Cycles          cycle      795,528
    Average SM Active Cycles         cycle    30,998.20
    Total SM Elapsed Cycles          cycle    1,008,030
    Average SMSP Active Cycles       cycle    30,950.20
    Total SMSP Elapsed Cycles        cycle    4,032,120
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.39
    SM Frequency                    Mhz       799.56
    Elapsed Cycles                cycle        2,637
    Memory Throughput                 %         0.96
    DRAM Throughput                   %         0.44
    Duration                         us         3.30
    L1/TEX Cache Throughput           %         3.89
    L2 Cache Throughput               %         0.96
    SM Active Cycles              cycle       308.40
    Compute (SM) Throughput           %         0.66
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.02
    Achieved Active Warps Per SM           warp         7.21
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        77.33
    Total DRAM Elapsed Cycles        cycle      106,496
    Average L1 Active Cycles         cycle       308.40
    Total L1 Elapsed Cycles          cycle       76,820
    Average L2 Active Cycles         cycle       208.29
    Total L2 Elapsed Cycles          cycle       60,384
    Average SM Active Cycles         cycle       308.40
    Total SM Elapsed Cycles          cycle       76,820
    Average SMSP Active Cycles       cycle       277.07
    Total SMSP Elapsed Cycles        cycle      307,280
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.097%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.23% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.572%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 69.98% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.097%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.23% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.259%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.52% above the average, while the minimum instance value is 82.24% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.89
    Elapsed Cycles                cycle       48,155
    Memory Throughput                 %        45.58
    DRAM Throughput                   %        45.58
    Duration                         us        58.94
    L1/TEX Cache Throughput           %        36.74
    L2 Cache Throughput               %        35.09
    SM Active Cycles              cycle    46,015.83
    Compute (SM) Throughput           %        70.28
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.29
    Achieved Active Warps Per SM           warp        38.06
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.71%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   147,485.33
    Total DRAM Elapsed Cycles        cycle    1,941,504
    Average L1 Active Cycles         cycle    46,015.83
    Total L1 Elapsed Cycles          cycle    1,441,520
    Average L2 Active Cycles         cycle    42,732.75
    Total L2 Elapsed Cycles          cycle    1,102,584
    Average SM Active Cycles         cycle    46,015.83
    Total SM Elapsed Cycles          cycle    1,441,520
    Average SMSP Active Cycles       cycle    45,740.47
    Total SMSP Elapsed Cycles        cycle    5,766,080
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.46
    Elapsed Cycles                cycle       35,143
    Memory Throughput                 %        68.31
    DRAM Throughput                   %        68.31
    Duration                         us        43.04
    L1/TEX Cache Throughput           %        38.49
    L2 Cache Throughput               %        31.33
    SM Active Cycles              cycle    31,341.53
    Compute (SM) Throughput           %        51.76
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.71
    Achieved Active Warps Per SM           warp        37.30
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.29%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,237.33
    Total DRAM Elapsed Cycles        cycle    1,416,192
    Average L1 Active Cycles         cycle    31,341.53
    Total L1 Elapsed Cycles          cycle      994,540
    Average L2 Active Cycles         cycle    29,220.71
    Total L2 Elapsed Cycles          cycle      804,696
    Average SM Active Cycles         cycle    31,341.53
    Total SM Elapsed Cycles          cycle      994,540
    Average SMSP Active Cycles       cycle    30,998.36
    Total SMSP Elapsed Cycles        cycle    3,978,160
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.53
    Elapsed Cycles                cycle       48,560
    Memory Throughput                 %        49.68
    DRAM Throughput                   %        49.68
    Duration                         us        59.39
    L1/TEX Cache Throughput           %        36.76
    L2 Cache Throughput               %        34.72
    SM Active Cycles              cycle    45,995.23
    Compute (SM) Throughput           %        70.52
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.30
    Achieved Active Warps Per SM           warp        38.07
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.7%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,949.33
    Total DRAM Elapsed Cycles        cycle    1,955,840
    Average L1 Active Cycles         cycle    45,995.23
    Total L1 Elapsed Cycles          cycle    1,435,280
    Average L2 Active Cycles         cycle    42,826.54
    Total L2 Elapsed Cycles          cycle    1,111,968
    Average SM Active Cycles         cycle    45,995.23
    Total SM Elapsed Cycles          cycle    1,435,280
    Average SMSP Active Cycles       cycle    45,698.38
    Total SMSP Elapsed Cycles        cycle    5,741,120
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.34
    Elapsed Cycles                cycle       34,979
    Memory Throughput                 %        68.54
    DRAM Throughput                   %        68.54
    Duration                         us        42.85
    L1/TEX Cache Throughput           %        38.31
    L2 Cache Throughput               %        31.39
    SM Active Cycles              cycle    31,650.23
    Compute (SM) Throughput           %        51.50
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.88
    Achieved Active Warps Per SM           warp        37.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.12%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,066.67
    Total DRAM Elapsed Cycles        cycle    1,410,048
    Average L1 Active Cycles         cycle    31,650.23
    Total L1 Elapsed Cycles          cycle      999,020
    Average L2 Active Cycles         cycle    29,779.88
    Total L2 Elapsed Cycles          cycle      800,952
    Average SM Active Cycles         cycle    31,650.23
    Total SM Elapsed Cycles          cycle      999,020
    Average SMSP Active Cycles       cycle    30,832.33
    Total SMSP Elapsed Cycles        cycle    3,996,080
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.23
    Elapsed Cycles                cycle       48,123
    Memory Throughput                 %        45.83
    DRAM Throughput                   %        45.83
    Duration                         us        58.88
    L1/TEX Cache Throughput           %        36.81
    L2 Cache Throughput               %        35.04
    SM Active Cycles              cycle    45,929.53
    Compute (SM) Throughput           %        70.18
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.32
    Achieved Active Warps Per SM           warp        38.07
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.68%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   148,058.67
    Total DRAM Elapsed Cycles        cycle    1,938,432
    Average L1 Active Cycles         cycle    45,929.53
    Total L1 Elapsed Cycles          cycle    1,441,060
    Average L2 Active Cycles         cycle    42,609.25
    Total L2 Elapsed Cycles          cycle    1,102,008
    Average SM Active Cycles         cycle    45,929.53
    Total SM Elapsed Cycles          cycle    1,441,060
    Average SMSP Active Cycles       cycle    45,697.23
    Total SMSP Elapsed Cycles        cycle    5,764,240
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.98
    Elapsed Cycles                cycle       35,244
    Memory Throughput                 %        68.25
    DRAM Throughput                   %        68.25
    Duration                         us        43.14
    L1/TEX Cache Throughput           %        38.32
    L2 Cache Throughput               %        31.16
    SM Active Cycles              cycle    31,395.70
    Compute (SM) Throughput           %        51.45
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.62
    Achieved Active Warps Per SM           warp        37.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.38%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,440
    Total DRAM Elapsed Cycles        cycle    1,419,264
    Average L1 Active Cycles         cycle    31,395.70
    Total L1 Elapsed Cycles          cycle      998,980
    Average L2 Active Cycles         cycle    28,935.67
    Total L2 Elapsed Cycles          cycle      807,000
    Average SM Active Cycles         cycle    31,395.70
    Total SM Elapsed Cycles          cycle      998,980
    Average SMSP Active Cycles       cycle       30,801
    Total SMSP Elapsed Cycles        cycle    3,995,920
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.39
    SM Frequency                    Mhz       796.57
    Elapsed Cycles                cycle        2,602
    Memory Throughput                 %         0.97
    DRAM Throughput                   %         0.47
    Duration                         us         3.26
    L1/TEX Cache Throughput           %         4.05
    L2 Cache Throughput               %         0.97
    SM Active Cycles              cycle       296.27
    Compute (SM) Throughput           %         0.63
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.92
    Achieved Active Warps Per SM           warp         7.16
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.08%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (14.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        82.67
    Total DRAM Elapsed Cycles        cycle      105,472
    Average L1 Active Cycles         cycle       296.27
    Total L1 Elapsed Cycles          cycle       78,160
    Average L2 Active Cycles         cycle       203.08
    Total L2 Elapsed Cycles          cycle       59,544
    Average SM Active Cycles         cycle       296.27
    Total SM Elapsed Cycles          cycle       78,160
    Average SMSP Active Cycles       cycle       264.87
    Total SMSP Elapsed Cycles        cycle      312,640
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.653%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.30% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.124%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 70.07% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.653%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.30% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.616%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 68.61% above the average, while the minimum instance value is 81.78% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.11
    Elapsed Cycles                cycle       48,115
    Memory Throughput                 %        45.72
    DRAM Throughput                   %        45.72
    Duration                         us        58.88
    L1/TEX Cache Throughput           %        36.79
    L2 Cache Throughput               %        35.04
    SM Active Cycles              cycle    45,954.37
    Compute (SM) Throughput           %        70.47
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.31
    Achieved Active Warps Per SM           warp        38.07
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.69%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      147,704
    Total DRAM Elapsed Cycles        cycle    1,938,432
    Average L1 Active Cycles         cycle    45,954.37
    Total L1 Elapsed Cycles          cycle    1,433,460
    Average L2 Active Cycles         cycle    42,576.83
    Total L2 Elapsed Cycles          cycle    1,101,792
    Average SM Active Cycles         cycle    45,954.37
    Total SM Elapsed Cycles          cycle    1,433,460
    Average SMSP Active Cycles       cycle    45,799.38
    Total SMSP Elapsed Cycles        cycle    5,733,840
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.78
    Elapsed Cycles                cycle       34,817
    Memory Throughput                 %        68.95
    DRAM Throughput                   %        68.95
    Duration                         us        42.62
    L1/TEX Cache Throughput           %        38.55
    L2 Cache Throughput               %        31.52
    SM Active Cycles              cycle    30,963.23
    Compute (SM) Throughput           %        51.72
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.43
    Achieved Active Warps Per SM           warp        37.17
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.57%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,336
    Total DRAM Elapsed Cycles        cycle    1,403,904
    Average L1 Active Cycles         cycle    30,963.23
    Total L1 Elapsed Cycles          cycle      992,990
    Average L2 Active Cycles         cycle    28,952.33
    Total L2 Elapsed Cycles          cycle      797,232
    Average SM Active Cycles         cycle    30,963.23
    Total SM Elapsed Cycles          cycle      992,990
    Average SMSP Active Cycles       cycle    31,343.26
    Total SMSP Elapsed Cycles        cycle    3,971,960
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.28
    SM Frequency                    Mhz       795.81
    Elapsed Cycles                cycle        2,624
    Memory Throughput                 %         0.98
    DRAM Throughput                   %         0.52
    Duration                         us         3.30
    L1/TEX Cache Throughput           %         3.90
    L2 Cache Throughput               %         0.98
    SM Active Cycles              cycle       307.53
    Compute (SM) Throughput           %         0.66
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.02
    Achieved Active Warps Per SM           warp         7.21
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        90.67
    Total DRAM Elapsed Cycles        cycle      104,448
    Average L1 Active Cycles         cycle       307.53
    Total L1 Elapsed Cycles          cycle       77,040
    Average L2 Active Cycles         cycle       218.62
    Total L2 Elapsed Cycles          cycle       60,072
    Average SM Active Cycles         cycle       307.53
    Total SM Elapsed Cycles          cycle       77,040
    Average SMSP Active Cycles       cycle       275.70
    Total SMSP Elapsed Cycles        cycle      308,160
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.058%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.28% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.505%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 69.90% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.058%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.28% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.476%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 62.69% above the average, while the minimum instance value is 83.08% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.18
    Elapsed Cycles                cycle       48,410
    Memory Throughput                 %        49.81
    DRAM Throughput                   %        49.81
    Duration                         us        59.23
    L1/TEX Cache Throughput           %        36.81
    L2 Cache Throughput               %        34.82
    SM Active Cycles              cycle    45,935.57
    Compute (SM) Throughput           %        70.14
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.39
    Achieved Active Warps Per SM           warp        38.11
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.61%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,936
    Total DRAM Elapsed Cycles        cycle    1,950,720
    Average L1 Active Cycles         cycle    45,935.57
    Total L1 Elapsed Cycles          cycle    1,438,720
    Average L2 Active Cycles         cycle    42,856.17
    Total L2 Elapsed Cycles          cycle    1,108,440
    Average SM Active Cycles         cycle    45,935.57
    Total SM Elapsed Cycles          cycle    1,438,720
    Average SMSP Active Cycles       cycle    45,557.82
    Total SMSP Elapsed Cycles        cycle    5,754,880
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.51
    Elapsed Cycles                cycle       34,520
    Memory Throughput                 %        64.65
    DRAM Throughput                   %        64.65
    Duration                         us        42.27
    L1/TEX Cache Throughput           %        38.37
    L2 Cache Throughput               %        31.79
    SM Active Cycles              cycle    31,581.87
    Compute (SM) Throughput           %        51.42
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.99
    Achieved Active Warps Per SM           warp        37.43
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.01%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   149,941.33
    Total DRAM Elapsed Cycles        cycle    1,391,616
    Average L1 Active Cycles         cycle    31,581.87
    Total L1 Elapsed Cycles          cycle      997,830
    Average L2 Active Cycles         cycle    28,884.46
    Total L2 Elapsed Cycles          cycle      790,368
    Average SM Active Cycles         cycle    31,581.87
    Total SM Elapsed Cycles          cycle      997,830
    Average SMSP Active Cycles       cycle    31,564.06
    Total SMSP Elapsed Cycles        cycle    3,991,320
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.43
    SM Frequency                    Mhz       805.56
    Elapsed Cycles                cycle        2,707
    Memory Throughput                 %         0.94
    DRAM Throughput                   %         0.51
    Duration                         us         3.36
    L1/TEX Cache Throughput           %         4.04
    L2 Cache Throughput               %         0.94
    SM Active Cycles              cycle       297.30
    Compute (SM) Throughput           %         0.64
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.96
    Achieved Active Warps Per SM           warp         7.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.04%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        93.33
    Total DRAM Elapsed Cycles        cycle      109,568
    Average L1 Active Cycles         cycle       297.30
    Total L1 Elapsed Cycles          cycle       79,230
    Average L2 Active Cycles         cycle       211.04
    Total L2 Elapsed Cycles          cycle       61,944
    Average SM Active Cycles         cycle       297.30
    Total SM Elapsed Cycles          cycle       79,230
    Average SMSP Active Cycles       cycle       265.63
    Total SMSP Elapsed Cycles        cycle      316,920
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.563%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.19% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.049%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 70.09% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.563%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.19% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.334%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 65.23% above the average, while the minimum instance value is 82.47% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.30
    Elapsed Cycles                cycle       48,101
    Memory Throughput                 %        46.10
    DRAM Throughput                   %        46.10
    Duration                         us        58.85
    L1/TEX Cache Throughput           %        36.82
    L2 Cache Throughput               %        35.07
    SM Active Cycles              cycle    45,927.27
    Compute (SM) Throughput           %        70.14
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.33
    Achieved Active Warps Per SM           warp        38.08
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.67%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   148,858.67
    Total DRAM Elapsed Cycles        cycle    1,937,408
    Average L1 Active Cycles         cycle    45,927.27
    Total L1 Elapsed Cycles          cycle    1,437,230
    Average L2 Active Cycles         cycle    42,723.88
    Total L2 Elapsed Cycles          cycle    1,101,408
    Average SM Active Cycles         cycle    45,927.27
    Total SM Elapsed Cycles          cycle    1,437,230
    Average SMSP Active Cycles       cycle    45,766.78
    Total SMSP Elapsed Cycles        cycle    5,748,920
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.85
    Elapsed Cycles                cycle       34,743
    Memory Throughput                 %        69.52
    DRAM Throughput                   %        69.52
    Duration                         us        42.53
    L1/TEX Cache Throughput           %        38.15
    L2 Cache Throughput               %        31.68
    SM Active Cycles              cycle    30,811.57
    Compute (SM) Throughput           %        51.11
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.09
    Achieved Active Warps Per SM           warp        37.48
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.91%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   162,301.33
    Total DRAM Elapsed Cycles        cycle    1,400,832
    Average L1 Active Cycles         cycle    30,811.57
    Total L1 Elapsed Cycles          cycle    1,003,140
    Average L2 Active Cycles         cycle    29,531.71
    Total L2 Elapsed Cycles          cycle      795,576
    Average SM Active Cycles         cycle    30,811.57
    Total SM Elapsed Cycles          cycle    1,003,140
    Average SMSP Active Cycles       cycle    31,591.18
    Total SMSP Elapsed Cycles        cycle    4,012,560
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.33
    SM Frequency                    Mhz       795.57
    Elapsed Cycles                cycle        2,649
    Memory Throughput                 %         0.98
    DRAM Throughput                   %         0.51
    Duration                         us         3.33
    L1/TEX Cache Throughput           %         3.54
    L2 Cache Throughput               %         0.98
    SM Active Cycles              cycle       338.97
    Compute (SM) Throughput           %         0.66
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.17
    Achieved Active Warps Per SM           warp         7.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.83%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        90.67
    Total DRAM Elapsed Cycles        cycle      106,496
    Average L1 Active Cycles         cycle       338.97
    Total L1 Elapsed Cycles          cycle       76,990
    Average L2 Active Cycles         cycle       191.75
    Total L2 Elapsed Cycles          cycle       60,600
    Average SM Active Cycles         cycle       338.97
    Total SM Elapsed Cycles          cycle       76,990
    Average SMSP Active Cycles       cycle       276.14
    Total SMSP Elapsed Cycles        cycle      307,960
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.874%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.19% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.527%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 69.95% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.874%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.19% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.179%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 68.20% above the average, while the minimum instance value is 80.70% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.48
    Elapsed Cycles                cycle       48,136
    Memory Throughput                 %        45.65
    DRAM Throughput                   %        45.65
    Duration                         us        58.88
    L1/TEX Cache Throughput           %        36.94
    L2 Cache Throughput               %        35.06
    SM Active Cycles              cycle    45,775.40
    Compute (SM) Throughput           %        70.12
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.27
    Achieved Active Warps Per SM           warp        38.05
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.73%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   147,562.67
    Total DRAM Elapsed Cycles        cycle    1,939,456
    Average L1 Active Cycles         cycle    45,775.40
    Total L1 Elapsed Cycles          cycle    1,435,870
    Average L2 Active Cycles         cycle    42,685.96
    Total L2 Elapsed Cycles          cycle    1,102,200
    Average SM Active Cycles         cycle    45,775.40
    Total SM Elapsed Cycles          cycle    1,435,870
    Average SMSP Active Cycles       cycle    45,514.11
    Total SMSP Elapsed Cycles        cycle    5,743,480
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.56
    Elapsed Cycles                cycle       34,784
    Memory Throughput                 %        69.06
    DRAM Throughput                   %        69.06
    Duration                         us        42.59
    L1/TEX Cache Throughput           %        38.66
    L2 Cache Throughput               %        31.55
    SM Active Cycles              cycle    31,410.93
    Compute (SM) Throughput           %        51.73
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.57
    Achieved Active Warps Per SM           warp        37.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.43%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,237.33
    Total DRAM Elapsed Cycles        cycle    1,400,832
    Average L1 Active Cycles         cycle    31,410.93
    Total L1 Elapsed Cycles          cycle      990,150
    Average L2 Active Cycles         cycle    29,219.71
    Total L2 Elapsed Cycles          cycle      796,560
    Average SM Active Cycles         cycle    31,410.93
    Total SM Elapsed Cycles          cycle      990,150
    Average SMSP Active Cycles       cycle    31,645.63
    Total SMSP Elapsed Cycles        cycle    3,960,600
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.93
    Elapsed Cycles                cycle       47,897
    Memory Throughput                 %        46.41
    DRAM Throughput                   %        46.41
    Duration                         us        58.62
    L1/TEX Cache Throughput           %        36.74
    L2 Cache Throughput               %        35.21
    SM Active Cycles              cycle    46,022.10
    Compute (SM) Throughput           %        69.92
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.11
    Achieved Active Warps Per SM           warp        37.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.89%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   149,210.67
    Total DRAM Elapsed Cycles        cycle    1,929,216
    Average L1 Active Cycles         cycle    46,022.10
    Total L1 Elapsed Cycles          cycle    1,438,110
    Average L2 Active Cycles         cycle    42,853.54
    Total L2 Elapsed Cycles          cycle    1,096,632
    Average SM Active Cycles         cycle    46,022.10
    Total SM Elapsed Cycles          cycle    1,438,110
    Average SMSP Active Cycles       cycle    45,551.22
    Total SMSP Elapsed Cycles        cycle    5,752,440
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.75
    Elapsed Cycles                cycle       34,817
    Memory Throughput                 %        69.03
    DRAM Throughput                   %        69.03
    Duration                         us        42.62
    L1/TEX Cache Throughput           %        38.80
    L2 Cache Throughput               %        31.52
    SM Active Cycles              cycle    31,299.70
    Compute (SM) Throughput           %        51.86
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.49
    Achieved Active Warps Per SM           warp        37.20
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.51%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,394.67
    Total DRAM Elapsed Cycles        cycle    1,402,880
    Average L1 Active Cycles         cycle    31,299.70
    Total L1 Elapsed Cycles          cycle      986,560
    Average L2 Active Cycles         cycle    29,647.33
    Total L2 Elapsed Cycles          cycle      797,208
    Average SM Active Cycles         cycle    31,299.70
    Total SM Elapsed Cycles          cycle      986,560
    Average SMSP Active Cycles       cycle    31,358.73
    Total SMSP Elapsed Cycles        cycle    3,946,240
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.44
    SM Frequency                    Mhz       801.78
    Elapsed Cycles                cycle        2,618
    Memory Throughput                 %         0.97
    DRAM Throughput                   %         0.53
    Duration                         us         3.26
    L1/TEX Cache Throughput           %         4.03
    L2 Cache Throughput               %         0.97
    SM Active Cycles              cycle       297.87
    Compute (SM) Throughput           %         0.65
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.97
    Achieved Active Warps Per SM           warp         7.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.03%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        93.33
    Total DRAM Elapsed Cycles        cycle      106,496
    Average L1 Active Cycles         cycle       297.87
    Total L1 Elapsed Cycles          cycle       77,590
    Average L2 Active Cycles         cycle       231.17
    Total L2 Elapsed Cycles          cycle       59,904
    Average SM Active Cycles         cycle       297.87
    Total SM Elapsed Cycles          cycle       77,590
    Average SMSP Active Cycles       cycle       264.37
    Total SMSP Elapsed Cycles        cycle      310,360
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.747%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.27% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.168%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 70.13% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.747%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.27% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.023%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 65.03% above the average, while the minimum instance value is 83.99% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.20
    Elapsed Cycles                cycle       47,939
    Memory Throughput                 %        45.99
    DRAM Throughput                   %        45.99
    Duration                         us        58.66
    L1/TEX Cache Throughput           %        36.91
    L2 Cache Throughput               %        35.14
    SM Active Cycles              cycle    45,811.10
    Compute (SM) Throughput           %        70.20
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.26
    Achieved Active Warps Per SM           warp        38.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.74%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   148,117.33
    Total DRAM Elapsed Cycles        cycle    1,932,288
    Average L1 Active Cycles         cycle    45,811.10
    Total L1 Elapsed Cycles          cycle    1,430,670
    Average L2 Active Cycles         cycle    42,653.46
    Total L2 Elapsed Cycles          cycle    1,097,784
    Average SM Active Cycles         cycle    45,811.10
    Total SM Elapsed Cycles          cycle    1,430,670
    Average SMSP Active Cycles       cycle    45,623.07
    Total SMSP Elapsed Cycles        cycle    5,722,680
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.60
    Elapsed Cycles                cycle       34,865
    Memory Throughput                 %        68.93
    DRAM Throughput                   %        68.93
    Duration                         us        42.69
    L1/TEX Cache Throughput           %        38.14
    L2 Cache Throughput               %        31.51
    SM Active Cycles              cycle    31,159.90
    Compute (SM) Throughput           %        50.92
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.95
    Achieved Active Warps Per SM           warp        37.42
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.05%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,285.33
    Total DRAM Elapsed Cycles        cycle    1,403,904
    Average L1 Active Cycles         cycle    31,159.90
    Total L1 Elapsed Cycles          cycle    1,003,840
    Average L2 Active Cycles         cycle       29,048
    Total L2 Elapsed Cycles          cycle      798,360
    Average SM Active Cycles         cycle    31,159.90
    Total SM Elapsed Cycles          cycle    1,003,840
    Average SMSP Active Cycles       cycle    30,919.67
    Total SMSP Elapsed Cycles        cycle    4,015,360
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       817.14
    Elapsed Cycles                cycle       48,170
    Memory Throughput                 %        49.94
    DRAM Throughput                   %        49.94
    Duration                         us        58.94
    L1/TEX Cache Throughput           %        37.02
    L2 Cache Throughput               %        34.96
    SM Active Cycles              cycle    45,674.93
    Compute (SM) Throughput           %        69.91
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.20
    Achieved Active Warps Per SM           warp        38.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.8%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,440
    Total DRAM Elapsed Cycles        cycle    1,939,456
    Average L1 Active Cycles         cycle    45,674.93
    Total L1 Elapsed Cycles          cycle    1,434,800
    Average L2 Active Cycles         cycle    42,555.12
    Total L2 Elapsed Cycles          cycle    1,102,944
    Average SM Active Cycles         cycle    45,674.93
    Total SM Elapsed Cycles          cycle    1,434,800
    Average SMSP Active Cycles       cycle    45,530.57
    Total SMSP Elapsed Cycles        cycle    5,739,200
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.49
    Elapsed Cycles                cycle       34,493
    Memory Throughput                 %        69.58
    DRAM Throughput                   %        69.58
    Duration                         us        42.24
    L1/TEX Cache Throughput           %        37.86
    L2 Cache Throughput               %        31.81
    SM Active Cycles              cycle    31,523.53
    Compute (SM) Throughput           %        50.52
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.55
    Achieved Active Warps Per SM           warp        37.22
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.45%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,272
    Total DRAM Elapsed Cycles        cycle    1,390,592
    Average L1 Active Cycles         cycle    31,523.53
    Total L1 Elapsed Cycles          cycle    1,010,970
    Average L2 Active Cycles         cycle    29,648.29
    Total L2 Elapsed Cycles          cycle      789,864
    Average SM Active Cycles         cycle    31,523.53
    Total SM Elapsed Cycles          cycle    1,010,970
    Average SMSP Active Cycles       cycle    31,486.34
    Total SMSP Elapsed Cycles        cycle    4,043,880
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.30
    Elapsed Cycles                cycle       47,736
    Memory Throughput                 %        46.08
    DRAM Throughput                   %        46.08
    Duration                         us        58.40
    L1/TEX Cache Throughput           %        36.94
    L2 Cache Throughput               %        35.22
    SM Active Cycles              cycle    45,767.53
    Compute (SM) Throughput           %        70.15
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.32
    Achieved Active Warps Per SM           warp        38.07
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.68%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      147,688
    Total DRAM Elapsed Cycles        cycle    1,923,072
    Average L1 Active Cycles         cycle    45,767.53
    Total L1 Elapsed Cycles          cycle    1,428,340
    Average L2 Active Cycles         cycle    42,595.96
    Total L2 Elapsed Cycles          cycle    1,093,080
    Average SM Active Cycles         cycle    45,767.53
    Total SM Elapsed Cycles          cycle    1,428,340
    Average SMSP Active Cycles       cycle    45,514.60
    Total SMSP Elapsed Cycles        cycle    5,713,360
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.61
    Elapsed Cycles                cycle       34,708
    Memory Throughput                 %        69.26
    DRAM Throughput                   %        69.26
    Duration                         us        42.50
    L1/TEX Cache Throughput           %        38.11
    L2 Cache Throughput               %        31.62
    SM Active Cycles              cycle    31,175.10
    Compute (SM) Throughput           %        50.80
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.17
    Achieved Active Warps Per SM           warp        37.52
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.83%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,357.33
    Total DRAM Elapsed Cycles        cycle    1,397,760
    Average L1 Active Cycles         cycle    31,175.10
    Total L1 Elapsed Cycles          cycle    1,004,340
    Average L2 Active Cycles         cycle    29,350.62
    Total L2 Elapsed Cycles          cycle      794,736
    Average SM Active Cycles         cycle    31,175.10
    Total SM Elapsed Cycles          cycle    1,004,340
    Average SMSP Active Cycles       cycle    31,319.12
    Total SMSP Elapsed Cycles        cycle    4,017,360
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.84
    Elapsed Cycles                cycle       47,970
    Memory Throughput                 %        46.01
    DRAM Throughput                   %        46.01
    Duration                         us        58.72
    L1/TEX Cache Throughput           %        37.00
    L2 Cache Throughput               %        35.09
    SM Active Cycles              cycle    45,698.33
    Compute (SM) Throughput           %        70.22
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.18
    Achieved Active Warps Per SM           warp        38.01
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.82%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      148,320
    Total DRAM Elapsed Cycles        cycle    1,934,336
    Average L1 Active Cycles         cycle    45,698.33
    Total L1 Elapsed Cycles          cycle    1,425,090
    Average L2 Active Cycles         cycle    42,404.75
    Total L2 Elapsed Cycles          cycle    1,098,312
    Average SM Active Cycles         cycle    45,698.33
    Total SM Elapsed Cycles          cycle    1,425,090
    Average SMSP Active Cycles       cycle    45,479.27
    Total SMSP Elapsed Cycles        cycle    5,700,360
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       817.01
    Elapsed Cycles                cycle       34,175
    Memory Throughput                 %        65.36
    DRAM Throughput                   %        65.36
    Duration                         us        41.82
    L1/TEX Cache Throughput           %        38.66
    L2 Cache Throughput               %        32.11
    SM Active Cycles              cycle    31,351.07
    Compute (SM) Throughput           %        51.49
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.60
    Achieved Active Warps Per SM           warp        37.25
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.4%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   149,930.67
    Total DRAM Elapsed Cycles        cycle    1,376,256
    Average L1 Active Cycles         cycle    31,351.07
    Total L1 Elapsed Cycles          cycle      989,910
    Average L2 Active Cycles         cycle    29,533.62
    Total L2 Elapsed Cycles          cycle      782,592
    Average SM Active Cycles         cycle    31,351.07
    Total SM Elapsed Cycles          cycle      989,910
    Average SMSP Active Cycles       cycle    30,989.73
    Total SMSP Elapsed Cycles        cycle    3,959,640
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.09
    Elapsed Cycles                cycle       48,035
    Memory Throughput                 %        46.18
    DRAM Throughput                   %        46.18
    Duration                         us        58.78
    L1/TEX Cache Throughput           %        37.04
    L2 Cache Throughput               %        35.04
    SM Active Cycles              cycle    45,652.63
    Compute (SM) Throughput           %        70.18
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.18
    Achieved Active Warps Per SM           warp        38.01
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.82%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   148,954.67
    Total DRAM Elapsed Cycles        cycle    1,935,360
    Average L1 Active Cycles         cycle    45,652.63
    Total L1 Elapsed Cycles          cycle    1,423,850
    Average L2 Active Cycles         cycle    42,335.58
    Total L2 Elapsed Cycles          cycle    1,099,920
    Average SM Active Cycles         cycle    45,652.63
    Total SM Elapsed Cycles          cycle    1,423,850
    Average SMSP Active Cycles       cycle    45,342.28
    Total SMSP Elapsed Cycles        cycle    5,695,400
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.29
    Elapsed Cycles                cycle       34,407
    Memory Throughput                 %        64.31
    DRAM Throughput                   %        64.31
    Duration                         us        42.14
    L1/TEX Cache Throughput           %        38.01
    L2 Cache Throughput               %        31.95
    SM Active Cycles              cycle    30,940.83
    Compute (SM) Throughput           %        50.56
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.53
    Achieved Active Warps Per SM           warp        37.21
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.47%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   148,722.67
    Total DRAM Elapsed Cycles        cycle    1,387,520
    Average L1 Active Cycles         cycle    30,940.83
    Total L1 Elapsed Cycles          cycle    1,007,180
    Average L2 Active Cycles         cycle    29,211.17
    Total L2 Elapsed Cycles          cycle      787,800
    Average SM Active Cycles         cycle    30,940.83
    Total SM Elapsed Cycles          cycle    1,007,180
    Average SMSP Active Cycles       cycle    31,338.75
    Total SMSP Elapsed Cycles        cycle    4,028,720
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.28
    SM Frequency                    Mhz       793.17
    Elapsed Cycles                cycle        2,641
    Memory Throughput                 %         0.97
    DRAM Throughput                   %         0.50
    Duration                         us         3.33
    L1/TEX Cache Throughput           %         3.92
    L2 Cache Throughput               %         0.97
    SM Active Cycles              cycle       306.03
    Compute (SM) Throughput           %         0.66
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.01
    Achieved Active Warps Per SM           warp         7.20
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.99%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle           88
    Total DRAM Elapsed Cycles        cycle      105,472
    Average L1 Active Cycles         cycle       306.03
    Total L1 Elapsed Cycles          cycle       76,510
    Average L2 Active Cycles         cycle       228.58
    Total L2 Elapsed Cycles          cycle       60,456
    Average SM Active Cycles         cycle       306.03
    Total SM Elapsed Cycles          cycle       76,510
    Average SMSP Active Cycles       cycle       276.98
    Total SMSP Elapsed Cycles        cycle      306,040
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.068%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.23% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.591%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 69.89% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.068%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.23% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.623%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 61.97% above the average, while the minimum instance value is 83.81% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.43
    Elapsed Cycles                cycle       48,241
    Memory Throughput                 %        49.99
    DRAM Throughput                   %        49.99
    Duration                         us        59.01
    L1/TEX Cache Throughput           %        37.10
    L2 Cache Throughput               %        34.98
    SM Active Cycles              cycle    45,576.83
    Compute (SM) Throughput           %        69.93
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.11
    Achieved Active Warps Per SM           warp        37.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.89%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   162,026.67
    Total DRAM Elapsed Cycles        cycle    1,944,576
    Average L1 Active Cycles         cycle    45,576.83
    Total L1 Elapsed Cycles          cycle    1,427,310
    Average L2 Active Cycles         cycle    42,502.67
    Total L2 Elapsed Cycles          cycle    1,104,600
    Average SM Active Cycles         cycle    45,576.83
    Total SM Elapsed Cycles          cycle    1,427,310
    Average SMSP Active Cycles       cycle    45,341.90
    Total SMSP Elapsed Cycles        cycle    5,709,240
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.65
    Elapsed Cycles                cycle       34,656
    Memory Throughput                 %        69.24
    DRAM Throughput                   %        69.24
    Duration                         us        42.43
    L1/TEX Cache Throughput           %        38.55
    L2 Cache Throughput               %        31.73
    SM Active Cycles              cycle    30,620.10
    Compute (SM) Throughput           %        51.24
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.75
    Achieved Active Warps Per SM           warp        37.32
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.25%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,194.67
    Total DRAM Elapsed Cycles        cycle    1,396,736
    Average L1 Active Cycles         cycle    30,620.10
    Total L1 Elapsed Cycles          cycle      992,870
    Average L2 Active Cycles         cycle    29,779.21
    Total L2 Elapsed Cycles          cycle      793,560
    Average SM Active Cycles         cycle    30,620.10
    Total SM Elapsed Cycles          cycle      992,870
    Average SMSP Active Cycles       cycle    31,278.07
    Total SMSP Elapsed Cycles        cycle    3,971,480
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.39
    SM Frequency                    Mhz       797.94
    Elapsed Cycles                cycle        2,631
    Memory Throughput                 %         0.98
    DRAM Throughput                   %         0.45
    Duration                         us         3.30
    L1/TEX Cache Throughput           %         3.83
    L2 Cache Throughput               %         0.98
    SM Active Cycles              cycle       313.43
    Compute (SM) Throughput           %         0.65
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.05
    Achieved Active Warps Per SM           warp         7.22
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.95%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle           80
    Total DRAM Elapsed Cycles        cycle      106,496
    Average L1 Active Cycles         cycle       313.43
    Total L1 Elapsed Cycles          cycle       77,760
    Average L2 Active Cycles         cycle       256.38
    Total L2 Elapsed Cycles          cycle       60,168
    Average SM Active Cycles         cycle       313.43
    Total SM Elapsed Cycles          cycle       77,760
    Average SMSP Active Cycles       cycle       267.31
    Total SMSP Elapsed Cycles        cycle      311,040
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.119%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.15% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.229%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 70.10% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.119%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.15% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.874%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.22% above the average, while the minimum instance value is 85.57% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.03
    Elapsed Cycles                cycle       47,876
    Memory Throughput                 %        50.13
    DRAM Throughput                   %        50.13
    Duration                         us        58.59
    L1/TEX Cache Throughput           %        36.98
    L2 Cache Throughput               %        35.18
    SM Active Cycles              cycle    45,720.83
    Compute (SM) Throughput           %        70.02
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.99
    Achieved Active Warps Per SM           warp        37.92
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.01%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,181.33
    Total DRAM Elapsed Cycles        cycle    1,929,216
    Average L1 Active Cycles         cycle    45,720.83
    Total L1 Elapsed Cycles          cycle    1,423,950
    Average L2 Active Cycles         cycle    42,449.38
    Total L2 Elapsed Cycles          cycle    1,096,128
    Average SM Active Cycles         cycle    45,720.83
    Total SM Elapsed Cycles          cycle    1,423,950
    Average SMSP Active Cycles       cycle    45,256.79
    Total SMSP Elapsed Cycles        cycle    5,695,800
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.81
    Elapsed Cycles                cycle       34,271
    Memory Throughput                 %        65.08
    DRAM Throughput                   %        65.08
    Duration                         us        41.95
    L1/TEX Cache Throughput           %        38.59
    L2 Cache Throughput               %        32.03
    SM Active Cycles              cycle    31,588.87
    Compute (SM) Throughput           %        51.24
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.66
    Achieved Active Warps Per SM           warp        37.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.34%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      149,832
    Total DRAM Elapsed Cycles        cycle    1,381,376
    Average L1 Active Cycles         cycle    31,588.87
    Total L1 Elapsed Cycles          cycle      991,890
    Average L2 Active Cycles         cycle       29,362
    Total L2 Elapsed Cycles          cycle      784,776
    Average SM Active Cycles         cycle    31,588.87
    Total SM Elapsed Cycles          cycle      991,890
    Average SMSP Active Cycles       cycle    31,147.53
    Total SMSP Elapsed Cycles        cycle    3,967,560
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.17
    Elapsed Cycles                cycle       47,910
    Memory Throughput                 %        45.99
    DRAM Throughput                   %        45.99
    Duration                         us        58.62
    L1/TEX Cache Throughput           %        37.19
    L2 Cache Throughput               %        35.09
    SM Active Cycles              cycle    45,464.13
    Compute (SM) Throughput           %        69.31
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.16
    Achieved Active Warps Per SM           warp        38.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   147,957.33
    Total DRAM Elapsed Cycles        cycle    1,930,240
    Average L1 Active Cycles         cycle    45,464.13
    Total L1 Elapsed Cycles          cycle    1,436,670
    Average L2 Active Cycles         cycle    42,296.92
    Total L2 Elapsed Cycles          cycle    1,096,944
    Average SM Active Cycles         cycle    45,464.13
    Total SM Elapsed Cycles          cycle    1,436,670
    Average SMSP Active Cycles       cycle    45,266.14
    Total SMSP Elapsed Cycles        cycle    5,746,680
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.19
    Elapsed Cycles                cycle       32,940
    Memory Throughput                 %        66.89
    DRAM Throughput                   %        66.89
    Duration                         us        40.35
    L1/TEX Cache Throughput           %        37.80
    L2 Cache Throughput               %        33.33
    SM Active Cycles              cycle    31,248.23
    Compute (SM) Throughput           %        50.15
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.49
    Achieved Active Warps Per SM           warp        37.20
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.51%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      147,944
    Total DRAM Elapsed Cycles        cycle    1,327,104
    Average L1 Active Cycles         cycle    31,248.23
    Total L1 Elapsed Cycles          cycle    1,012,410
    Average L2 Active Cycles         cycle    29,346.67
    Total L2 Elapsed Cycles          cycle      754,152
    Average SM Active Cycles         cycle    31,248.23
    Total SM Elapsed Cycles          cycle    1,012,410
    Average SMSP Active Cycles       cycle    31,542.22
    Total SMSP Elapsed Cycles        cycle    4,049,640
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.99
    Elapsed Cycles                cycle       47,587
    Memory Throughput                 %        46.88
    DRAM Throughput                   %        46.88
    Duration                         us        58.24
    L1/TEX Cache Throughput           %        37.02
    L2 Cache Throughput               %        35.37
    SM Active Cycles              cycle    45,675.97
    Compute (SM) Throughput           %        69.68
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.94
    Achieved Active Warps Per SM           warp        37.89
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.06%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   149,765.33
    Total DRAM Elapsed Cycles        cycle    1,916,928
    Average L1 Active Cycles         cycle    45,675.97
    Total L1 Elapsed Cycles          cycle    1,427,030
    Average L2 Active Cycles         cycle    42,348.83
    Total L2 Elapsed Cycles          cycle    1,089,672
    Average SM Active Cycles         cycle    45,675.97
    Total SM Elapsed Cycles          cycle    1,427,030
    Average SMSP Active Cycles       cycle    45,251.70
    Total SMSP Elapsed Cycles        cycle    5,708,120
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.64
    Elapsed Cycles                cycle       34,579
    Memory Throughput                 %        69.64
    DRAM Throughput                   %        69.64
    Duration                         us        42.34
    L1/TEX Cache Throughput           %        39.23
    L2 Cache Throughput               %        31.73
    SM Active Cycles              cycle    31,293.77
    Compute (SM) Throughput           %        52.00
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.44
    Achieved Active Warps Per SM           warp        37.17
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.56%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,634.67
    Total DRAM Elapsed Cycles        cycle    1,392,640
    Average L1 Active Cycles         cycle    31,293.77
    Total L1 Elapsed Cycles          cycle      975,740
    Average L2 Active Cycles         cycle    29,136.79
    Total L2 Elapsed Cycles          cycle      791,784
    Average SM Active Cycles         cycle    31,293.77
    Total SM Elapsed Cycles          cycle      975,740
    Average SMSP Active Cycles       cycle    30,784.80
    Total SMSP Elapsed Cycles        cycle    3,902,960
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.39
    Elapsed Cycles                cycle       48,210
    Memory Throughput                 %        50.02
    DRAM Throughput                   %        50.02
    Duration                         us        58.98
    L1/TEX Cache Throughput           %        37.09
    L2 Cache Throughput               %        34.77
    SM Active Cycles              cycle    45,590.50
    Compute (SM) Throughput           %        69.62
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.08
    Achieved Active Warps Per SM           warp        37.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.92%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,952
    Total DRAM Elapsed Cycles        cycle    1,942,528
    Average L1 Active Cycles         cycle    45,590.50
    Total L1 Elapsed Cycles          cycle    1,426,960
    Average L2 Active Cycles         cycle    42,371.42
    Total L2 Elapsed Cycles          cycle    1,103,904
    Average SM Active Cycles         cycle    45,590.50
    Total SM Elapsed Cycles          cycle    1,426,960
    Average SMSP Active Cycles       cycle    45,245.57
    Total SMSP Elapsed Cycles        cycle    5,707,840
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.45
    Elapsed Cycles                cycle       34,621
    Memory Throughput                 %        69.39
    DRAM Throughput                   %        69.39
    Duration                         us        42.40
    L1/TEX Cache Throughput           %        38.62
    L2 Cache Throughput               %        31.70
    SM Active Cycles              cycle    30,667.07
    Compute (SM) Throughput           %        51.14
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.66
    Achieved Active Warps Per SM           warp        37.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.34%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,405.33
    Total DRAM Elapsed Cycles        cycle    1,395,712
    Average L1 Active Cycles         cycle    30,667.07
    Total L1 Elapsed Cycles          cycle      991,060
    Average L2 Active Cycles         cycle    28,729.54
    Total L2 Elapsed Cycles          cycle      792,792
    Average SM Active Cycles         cycle    30,667.07
    Total SM Elapsed Cycles          cycle      991,060
    Average SMSP Active Cycles       cycle    30,820.40
    Total SMSP Elapsed Cycles        cycle    3,964,240
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.28
    SM Frequency                    Mhz       797.03
    Elapsed Cycles                cycle        2,630
    Memory Throughput                 %         1.17
    DRAM Throughput                   %         0.51
    Duration                         us         3.30
    L1/TEX Cache Throughput           %         3.90
    L2 Cache Throughput               %         1.17
    SM Active Cycles              cycle       307.33
    Compute (SM) Throughput           %         0.59
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.79
    Achieved Active Warps Per SM           warp         7.10
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.21%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (14.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle           88
    Total DRAM Elapsed Cycles        cycle      104,448
    Average L1 Active Cycles         cycle       307.33
    Total L1 Elapsed Cycles          cycle       77,280
    Average L2 Active Cycles         cycle       256.83
    Total L2 Elapsed Cycles          cycle       60,240
    Average SM Active Cycles         cycle       307.33
    Total SM Elapsed Cycles          cycle       77,280
    Average SMSP Active Cycles       cycle       277.10
    Total SMSP Elapsed Cycles        cycle      309,120
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.017%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.20% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.531%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 70.01% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.017%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.20% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.86%                                                                                           
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 57.27% above the average, while the minimum instance value is 85.59% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.18
    Elapsed Cycles                cycle       47,752
    Memory Throughput                 %        50.34
    DRAM Throughput                   %        50.34
    Duration                         us        58.43
    L1/TEX Cache Throughput           %        37.05
    L2 Cache Throughput               %        35.27
    SM Active Cycles              cycle    45,632.60
    Compute (SM) Throughput           %        69.56
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.93
    Achieved Active Warps Per SM           warp        37.88
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.07%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,509.33
    Total DRAM Elapsed Cycles        cycle    1,925,120
    Average L1 Active Cycles         cycle    45,632.60
    Total L1 Elapsed Cycles          cycle    1,426,410
    Average L2 Active Cycles         cycle    42,252.96
    Total L2 Elapsed Cycles          cycle    1,093,464
    Average SM Active Cycles         cycle    45,632.60
    Total SM Elapsed Cycles          cycle    1,426,410
    Average SMSP Active Cycles       cycle    45,277.32
    Total SMSP Elapsed Cycles        cycle    5,705,640
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.05
    Elapsed Cycles                cycle       33,627
    Memory Throughput                 %        65.41
    DRAM Throughput                   %        65.41
    Duration                         us        41.15
    L1/TEX Cache Throughput           %        39.10
    L2 Cache Throughput               %        32.65
    SM Active Cycles              cycle    31,574.87
    Compute (SM) Throughput           %        51.71
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.65
    Achieved Active Warps Per SM           warp        37.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.35%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      147,680
    Total DRAM Elapsed Cycles        cycle    1,354,752
    Average L1 Active Cycles         cycle    31,574.87
    Total L1 Elapsed Cycles          cycle      978,990
    Average L2 Active Cycles         cycle    28,982.04
    Total L2 Elapsed Cycles          cycle      769,896
    Average SM Active Cycles         cycle    31,574.87
    Total SM Elapsed Cycles          cycle      978,990
    Average SMSP Active Cycles       cycle    30,813.38
    Total SMSP Elapsed Cycles        cycle    3,915,960
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.82
    Elapsed Cycles                cycle       47,968
    Memory Throughput                 %        50.05
    DRAM Throughput                   %        50.05
    Duration                         us        58.72
    L1/TEX Cache Throughput           %        37.16
    L2 Cache Throughput               %        35.00
    SM Active Cycles              cycle    45,504.73
    Compute (SM) Throughput           %        69.38
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.90
    Achieved Active Warps Per SM           warp        37.87
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.1%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,280
    Total DRAM Elapsed Cycles        cycle    1,933,312
    Average L1 Active Cycles         cycle    45,504.73
    Total L1 Elapsed Cycles          cycle    1,428,210
    Average L2 Active Cycles         cycle    42,311.12
    Total L2 Elapsed Cycles          cycle    1,098,264
    Average SM Active Cycles         cycle    45,504.73
    Total SM Elapsed Cycles          cycle    1,428,210
    Average SMSP Active Cycles       cycle    45,207.07
    Total SMSP Elapsed Cycles        cycle    5,712,840
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.98
    Elapsed Cycles                cycle       34,355
    Memory Throughput                 %        69.86
    DRAM Throughput                   %        69.86
    Duration                         us        42.05
    L1/TEX Cache Throughput           %        38.02
    L2 Cache Throughput               %        31.94
    SM Active Cycles              cycle    31,076.03
    Compute (SM) Throughput           %        50.22
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.36
    Achieved Active Warps Per SM           warp        37.13
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.64%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,186.67
    Total DRAM Elapsed Cycles        cycle    1,384,448
    Average L1 Active Cycles         cycle    31,076.03
    Total L1 Elapsed Cycles          cycle    1,006,930
    Average L2 Active Cycles         cycle    28,939.83
    Total L2 Elapsed Cycles          cycle      786,696
    Average SM Active Cycles         cycle    31,076.03
    Total SM Elapsed Cycles          cycle    1,006,930
    Average SMSP Active Cycles       cycle    30,646.06
    Total SMSP Elapsed Cycles        cycle    4,027,720
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.36
    Elapsed Cycles                cycle       47,555
    Memory Throughput                 %        46.96
    DRAM Throughput                   %        46.96
    Duration                         us        58.18
    L1/TEX Cache Throughput           %        37.23
    L2 Cache Throughput               %        35.33
    SM Active Cycles              cycle    45,414.47
    Compute (SM) Throughput           %        69.53
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.85
    Achieved Active Warps Per SM           warp        37.85
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.15%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   150,045.33
    Total DRAM Elapsed Cycles        cycle    1,916,928
    Average L1 Active Cycles         cycle    45,414.47
    Total L1 Elapsed Cycles          cycle    1,422,710
    Average L2 Active Cycles         cycle    42,327.33
    Total L2 Elapsed Cycles          cycle    1,088,832
    Average SM Active Cycles         cycle    45,414.47
    Total SM Elapsed Cycles          cycle    1,422,710
    Average SMSP Active Cycles       cycle    45,385.12
    Total SMSP Elapsed Cycles        cycle    5,690,840
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.95
    Elapsed Cycles                cycle       34,723
    Memory Throughput                 %        69.43
    DRAM Throughput                   %        69.43
    Duration                         us        42.50
    L1/TEX Cache Throughput           %        38.68
    L2 Cache Throughput               %        31.68
    SM Active Cycles              cycle       31,019
    Compute (SM) Throughput           %        51.06
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.68
    Achieved Active Warps Per SM           warp        37.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.32%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,866.67
    Total DRAM Elapsed Cycles        cycle    1,398,784
    Average L1 Active Cycles         cycle       31,019
    Total L1 Elapsed Cycles          cycle      989,510
    Average L2 Active Cycles         cycle    29,058.12
    Total L2 Elapsed Cycles          cycle      795,072
    Average SM Active Cycles         cycle       31,019
    Total SM Elapsed Cycles          cycle      989,510
    Average SMSP Active Cycles       cycle    31,039.40
    Total SMSP Elapsed Cycles        cycle    3,958,040
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.06
    Elapsed Cycles                cycle       47,932
    Memory Throughput                 %        50.34
    DRAM Throughput                   %        50.34
    Duration                         us        58.66
    L1/TEX Cache Throughput           %        37.12
    L2 Cache Throughput               %        35.07
    SM Active Cycles              cycle    45,552.90
    Compute (SM) Throughput           %        69.52
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.79
    Achieved Active Warps Per SM           warp        37.82
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.21%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   162,034.67
    Total DRAM Elapsed Cycles        cycle    1,931,264
    Average L1 Active Cycles         cycle    45,552.90
    Total L1 Elapsed Cycles          cycle    1,421,130
    Average L2 Active Cycles         cycle    42,125.88
    Total L2 Elapsed Cycles          cycle    1,097,520
    Average SM Active Cycles         cycle    45,552.90
    Total SM Elapsed Cycles          cycle    1,421,130
    Average SMSP Active Cycles       cycle    45,127.50
    Total SMSP Elapsed Cycles        cycle    5,684,520
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.20
    Elapsed Cycles                cycle       34,976
    Memory Throughput                 %        68.89
    DRAM Throughput                   %        68.89
    Duration                         us        42.85
    L1/TEX Cache Throughput           %        38.88
    L2 Cache Throughput               %        31.45
    SM Active Cycles              cycle    30,841.53
    Compute (SM) Throughput           %        51.26
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.32
    Achieved Active Warps Per SM           warp        37.11
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.68%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,896
    Total DRAM Elapsed Cycles        cycle    1,410,048
    Average L1 Active Cycles         cycle    30,841.53
    Total L1 Elapsed Cycles          cycle      984,400
    Average L2 Active Cycles         cycle    29,008.71
    Total L2 Elapsed Cycles          cycle      800,736
    Average SM Active Cycles         cycle    30,841.53
    Total SM Elapsed Cycles          cycle      984,400
    Average SMSP Active Cycles       cycle    31,064.33
    Total SMSP Elapsed Cycles        cycle    3,937,600
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.89
    Elapsed Cycles                cycle       47,552
    Memory Throughput                 %        46.67
    DRAM Throughput                   %        46.67
    Duration                         us        58.21
    L1/TEX Cache Throughput           %        37.34
    L2 Cache Throughput               %        35.30
    SM Active Cycles              cycle    45,285.80
    Compute (SM) Throughput           %        69.37
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.77
    Achieved Active Warps Per SM           warp        37.81
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.23%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   149,037.33
    Total DRAM Elapsed Cycles        cycle    1,915,904
    Average L1 Active Cycles         cycle    45,285.80
    Total L1 Elapsed Cycles          cycle    1,422,370
    Average L2 Active Cycles         cycle    42,250.92
    Total L2 Elapsed Cycles          cycle    1,088,856
    Average SM Active Cycles         cycle    45,285.80
    Total SM Elapsed Cycles          cycle    1,422,370
    Average SMSP Active Cycles       cycle    45,098.13
    Total SMSP Elapsed Cycles        cycle    5,689,480
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.34
    Elapsed Cycles                cycle       34,801
    Memory Throughput                 %        69.38
    DRAM Throughput                   %        69.38
    Duration                         us        42.62
    L1/TEX Cache Throughput           %        38.94
    L2 Cache Throughput               %        31.53
    SM Active Cycles              cycle    31,390.43
    Compute (SM) Throughput           %        51.28
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.50
    Achieved Active Warps Per SM           warp        37.20
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.5%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      162,096
    Total DRAM Elapsed Cycles        cycle    1,401,856
    Average L1 Active Cycles         cycle    31,390.43
    Total L1 Elapsed Cycles          cycle      983,100
    Average L2 Active Cycles         cycle    28,626.08
    Total L2 Elapsed Cycles          cycle      796,872
    Average SM Active Cycles         cycle    31,390.43
    Total SM Elapsed Cycles          cycle      983,100
    Average SMSP Active Cycles       cycle    30,881.33
    Total SMSP Elapsed Cycles        cycle    3,932,400
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.99
    Elapsed Cycles                cycle       47,531
    Memory Throughput                 %        47.14
    DRAM Throughput                   %        47.14
    Duration                         us        58.18
    L1/TEX Cache Throughput           %        37.28
    L2 Cache Throughput               %        35.32
    SM Active Cycles              cycle    45,359.50
    Compute (SM) Throughput           %        69.55
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.71
    Achieved Active Warps Per SM           warp        37.78
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.29%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      150,536
    Total DRAM Elapsed Cycles        cycle    1,915,904
    Average L1 Active Cycles         cycle    45,359.50
    Total L1 Elapsed Cycles          cycle    1,416,820
    Average L2 Active Cycles         cycle    42,192.96
    Total L2 Elapsed Cycles          cycle    1,088,448
    Average SM Active Cycles         cycle    45,359.50
    Total SM Elapsed Cycles          cycle    1,416,820
    Average SMSP Active Cycles       cycle    45,066.76
    Total SMSP Elapsed Cycles        cycle    5,667,280
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.73
    Elapsed Cycles                cycle       33,143
    Memory Throughput                 %        66.45
    DRAM Throughput                   %        66.45
    Duration                         us        40.58
    L1/TEX Cache Throughput           %        38.28
    L2 Cache Throughput               %        33.12
    SM Active Cycles              cycle    31,487.73
    Compute (SM) Throughput           %        50.37
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.49
    Achieved Active Warps Per SM           warp        37.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.51%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   147,773.33
    Total DRAM Elapsed Cycles        cycle    1,334,272
    Average L1 Active Cycles         cycle    31,487.73
    Total L1 Elapsed Cycles          cycle      999,760
    Average L2 Active Cycles         cycle    28,790.12
    Total L2 Elapsed Cycles          cycle      758,712
    Average SM Active Cycles         cycle    31,487.73
    Total SM Elapsed Cycles          cycle      999,760
    Average SMSP Active Cycles       cycle    30,944.02
    Total SMSP Elapsed Cycles        cycle    3,999,040
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.39
    SM Frequency                    Mhz       795.85
    Elapsed Cycles                cycle        2,600
    Memory Throughput                 %         0.97
    DRAM Throughput                   %         0.46
    Duration                         us         3.26
    L1/TEX Cache Throughput           %         4.04
    L2 Cache Throughput               %         0.97
    SM Active Cycles              cycle       297.37
    Compute (SM) Throughput           %         0.65
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.96
    Achieved Active Warps Per SM           warp         7.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.04%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle           80
    Total DRAM Elapsed Cycles        cycle      105,472
    Average L1 Active Cycles         cycle       297.37
    Total L1 Elapsed Cycles          cycle       77,960
    Average L2 Active Cycles         cycle       199.46
    Total L2 Elapsed Cycles          cycle       59,472
    Average SM Active Cycles         cycle       297.37
    Total SM Elapsed Cycles          cycle       77,960
    Average SMSP Active Cycles       cycle       268.07
    Total SMSP Elapsed Cycles        cycle      311,840
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.691%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.21% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.226%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 70.05% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.691%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.21% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.646%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 70.14% above the average, while the minimum instance value is 81.45% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.14
    Elapsed Cycles                cycle       47,699
    Memory Throughput                 %        50.43
    DRAM Throughput                   %        50.43
    Duration                         us        58.37
    L1/TEX Cache Throughput           %        37.29
    L2 Cache Throughput               %        35.18
    SM Active Cycles              cycle    45,339.67
    Compute (SM) Throughput           %        69.19
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.73
    Achieved Active Warps Per SM           warp        37.79
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.27%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,477.33
    Total DRAM Elapsed Cycles        cycle    1,921,024
    Average L1 Active Cycles         cycle    45,339.67
    Total L1 Elapsed Cycles          cycle    1,422,190
    Average L2 Active Cycles         cycle    42,149.33
    Total L2 Elapsed Cycles          cycle    1,092,168
    Average SM Active Cycles         cycle    45,339.67
    Total SM Elapsed Cycles          cycle    1,422,190
    Average SMSP Active Cycles       cycle    45,053.14
    Total SMSP Elapsed Cycles        cycle    5,688,760
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.25
    Elapsed Cycles                cycle       35,028
    Memory Throughput                 %        68.59
    DRAM Throughput                   %        68.59
    Duration                         us        42.91
    L1/TEX Cache Throughput           %        38.58
    L2 Cache Throughput               %        31.34
    SM Active Cycles              cycle    30,712.33
    Compute (SM) Throughput           %        50.72
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.21
    Achieved Active Warps Per SM           warp        37.54
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.79%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,437.33
    Total DRAM Elapsed Cycles        cycle    1,412,096
    Average L1 Active Cycles         cycle    30,712.33
    Total L1 Elapsed Cycles          cycle      992,060
    Average L2 Active Cycles         cycle    28,910.54
    Total L2 Elapsed Cycles          cycle      801,936
    Average SM Active Cycles         cycle    30,712.33
    Total SM Elapsed Cycles          cycle      992,060
    Average SMSP Active Cycles       cycle    30,567.65
    Total SMSP Elapsed Cycles        cycle    3,968,240
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.33
    SM Frequency                    Mhz       797.43
    Elapsed Cycles                cycle        2,630
    Memory Throughput                 %         0.97
    DRAM Throughput                   %         0.49
    Duration                         us         3.30
    L1/TEX Cache Throughput           %         3.88
    L2 Cache Throughput               %         0.97
    SM Active Cycles              cycle       309.17
    Compute (SM) Throughput           %         0.65
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.03
    Achieved Active Warps Per SM           warp         7.21
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.97%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        85.33
    Total DRAM Elapsed Cycles        cycle      105,472
    Average L1 Active Cycles         cycle       309.17
    Total L1 Elapsed Cycles          cycle       77,580
    Average L2 Active Cycles         cycle       219.46
    Total L2 Elapsed Cycles          cycle       60,240
    Average SM Active Cycles         cycle       309.17
    Total SM Elapsed Cycles          cycle       77,580
    Average SMSP Active Cycles       cycle       277.23
    Total SMSP Elapsed Cycles        cycle      310,320
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.04%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.25% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.494%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 69.90% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.04%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.25% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.424%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 62.03% above the average, while the minimum instance value is 83.14% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.09
    Elapsed Cycles                cycle       47,592
    Memory Throughput                 %        50.62
    DRAM Throughput                   %        50.62
    Duration                         us        58.24
    L1/TEX Cache Throughput           %        37.37
    L2 Cache Throughput               %        35.17
    SM Active Cycles              cycle    45,248.77
    Compute (SM) Throughput           %        69.15
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.65
    Achieved Active Warps Per SM           warp        37.75
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.35%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,813.33
    Total DRAM Elapsed Cycles        cycle    1,917,952
    Average L1 Active Cycles         cycle    45,248.77
    Total L1 Elapsed Cycles          cycle    1,420,620
    Average L2 Active Cycles         cycle    42,164.79
    Total L2 Elapsed Cycles          cycle    1,089,816
    Average SM Active Cycles         cycle    45,248.77
    Total SM Elapsed Cycles          cycle    1,420,620
    Average SMSP Active Cycles       cycle    45,105.34
    Total SMSP Elapsed Cycles        cycle    5,682,480
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.38
    Elapsed Cycles                cycle       34,333
    Memory Throughput                 %        70.13
    DRAM Throughput                   %        70.13
    Duration                         us        42.05
    L1/TEX Cache Throughput           %        39.06
    L2 Cache Throughput               %        31.95
    SM Active Cycles              cycle    31,273.43
    Compute (SM) Throughput           %        51.29
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.10
    Achieved Active Warps Per SM           warp        37.01
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.9%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,704
    Total DRAM Elapsed Cycles        cycle    1,383,424
    Average L1 Active Cycles         cycle    31,273.43
    Total L1 Elapsed Cycles          cycle      979,940
    Average L2 Active Cycles         cycle    28,742.71
    Total L2 Elapsed Cycles          cycle      786,192
    Average SM Active Cycles         cycle    31,273.43
    Total SM Elapsed Cycles          cycle      979,940
    Average SMSP Active Cycles       cycle    30,641.35
    Total SMSP Elapsed Cycles        cycle    3,919,760
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.33
    SM Frequency                    Mhz       797.39
    Elapsed Cycles                cycle        2,604
    Memory Throughput                 %         0.99
    DRAM Throughput                   %         0.49
    Duration                         us         3.26
    L1/TEX Cache Throughput           %         4.06
    L2 Cache Throughput               %         0.99
    SM Active Cycles              cycle       295.70
    Compute (SM) Throughput           %         0.62
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.89
    Achieved Active Warps Per SM           warp         7.14
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.11%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (14.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        85.33
    Total DRAM Elapsed Cycles        cycle      104,448
    Average L1 Active Cycles         cycle       295.70
    Total L1 Elapsed Cycles          cycle       77,930
    Average L2 Active Cycles         cycle       218.25
    Total L2 Elapsed Cycles          cycle       59,616
    Average SM Active Cycles         cycle       295.70
    Total SM Elapsed Cycles          cycle       77,930
    Average SMSP Active Cycles       cycle       265.07
    Total SMSP Elapsed Cycles        cycle      311,720
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.652%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.22% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.137%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 69.95% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.652%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.22% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.841%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 66.47% above the average, while the minimum instance value is 83.05% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       817.05
    Elapsed Cycles                cycle       47,825
    Memory Throughput                 %        50.32
    DRAM Throughput                   %        50.32
    Duration                         us        58.53
    L1/TEX Cache Throughput           %        37.36
    L2 Cache Throughput               %        34.98
    SM Active Cycles              cycle    45,252.67
    Compute (SM) Throughput           %        69.12
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.68
    Achieved Active Warps Per SM           warp        37.77
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.32%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,538.67
    Total DRAM Elapsed Cycles        cycle    1,926,144
    Average L1 Active Cycles         cycle    45,252.67
    Total L1 Elapsed Cycles          cycle    1,419,700
    Average L2 Active Cycles         cycle    42,097.83
    Total L2 Elapsed Cycles          cycle    1,095,192
    Average SM Active Cycles         cycle    45,252.67
    Total SM Elapsed Cycles          cycle    1,419,700
    Average SMSP Active Cycles       cycle    44,924.03
    Total SMSP Elapsed Cycles        cycle    5,678,800
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.46
    Elapsed Cycles                cycle       34,544
    Memory Throughput                 %        69.54
    DRAM Throughput                   %        69.54
    Duration                         us        42.30
    L1/TEX Cache Throughput           %        39.28
    L2 Cache Throughput               %        31.78
    SM Active Cycles              cycle    30,530.53
    Compute (SM) Throughput           %        51.51
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.67
    Achieved Active Warps Per SM           warp        37.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.33%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,280
    Total DRAM Elapsed Cycles        cycle    1,391,616
    Average L1 Active Cycles         cycle    30,530.53
    Total L1 Elapsed Cycles          cycle      974,540
    Average L2 Active Cycles         cycle    29,038.42
    Total L2 Elapsed Cycles          cycle      791,040
    Average SM Active Cycles         cycle    30,530.53
    Total SM Elapsed Cycles          cycle      974,540
    Average SMSP Active Cycles       cycle    30,535.59
    Total SMSP Elapsed Cycles        cycle    3,898,160
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.44
    SM Frequency                    Mhz       802.48
    Elapsed Cycles                cycle        2,671
    Memory Throughput                 %         0.96
    DRAM Throughput                   %         0.46
    Duration                         us         3.33
    L1/TEX Cache Throughput           %         3.90
    L2 Cache Throughput               %         0.96
    SM Active Cycles              cycle       307.77
    Compute (SM) Throughput           %         0.65
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.02
    Achieved Active Warps Per SM           warp         7.21
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        82.67
    Total DRAM Elapsed Cycles        cycle      108,544
    Average L1 Active Cycles         cycle       307.77
    Total L1 Elapsed Cycles          cycle       77,200
    Average L2 Active Cycles         cycle       231.71
    Total L2 Elapsed Cycles          cycle       61,152
    Average SM Active Cycles         cycle       307.77
    Total SM Elapsed Cycles          cycle       77,200
    Average SMSP Active Cycles       cycle       276.80
    Total SMSP Elapsed Cycles        cycle      308,800
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.036%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.19% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.527%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 69.98% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.036%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.19% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.733%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.04% above the average, while the minimum instance value is 84.03% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.05
    Elapsed Cycles                cycle       47,643
    Memory Throughput                 %        50.62
    DRAM Throughput                   %        50.62
    Duration                         us        58.30
    L1/TEX Cache Throughput           %        37.35
    L2 Cache Throughput               %        35.20
    SM Active Cycles              cycle    45,273.37
    Compute (SM) Throughput           %        69.37
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.47
    Achieved Active Warps Per SM           warp        37.66
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.53%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,978.67
    Total DRAM Elapsed Cycles        cycle    1,920,000
    Average L1 Active Cycles         cycle    45,273.37
    Total L1 Elapsed Cycles          cycle    1,412,300
    Average L2 Active Cycles         cycle    42,151.54
    Total L2 Elapsed Cycles          cycle    1,090,968
    Average SM Active Cycles         cycle    45,273.37
    Total SM Elapsed Cycles          cycle    1,412,300
    Average SMSP Active Cycles       cycle    45,081.28
    Total SMSP Elapsed Cycles        cycle    5,649,200
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.91
    Elapsed Cycles                cycle       34,195
    Memory Throughput                 %        70.47
    DRAM Throughput                   %        70.47
    Duration                         us        41.86
    L1/TEX Cache Throughput           %        38.74
    L2 Cache Throughput               %        32.09
    SM Active Cycles              cycle    31,145.07
    Compute (SM) Throughput           %        50.76
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.20
    Achieved Active Warps Per SM           warp        37.05
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.8%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,650.67
    Total DRAM Elapsed Cycles        cycle    1,376,256
    Average L1 Active Cycles         cycle    31,145.07
    Total L1 Elapsed Cycles          cycle      987,820
    Average L2 Active Cycles         cycle    29,092.96
    Total L2 Elapsed Cycles          cycle      783,144
    Average SM Active Cycles         cycle    31,145.07
    Total SM Elapsed Cycles          cycle      987,820
    Average SMSP Active Cycles       cycle    30,841.52
    Total SMSP Elapsed Cycles        cycle    3,951,280
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.39
    SM Frequency                    Mhz       799.76
    Elapsed Cycles                cycle        2,637
    Memory Throughput                 %         0.97
    DRAM Throughput                   %         0.41
    Duration                         us         3.30
    L1/TEX Cache Throughput           %         4.03
    L2 Cache Throughput               %         0.97
    SM Active Cycles              cycle       297.90
    Compute (SM) Throughput           %         0.65
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.96
    Achieved Active Warps Per SM           warp         7.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.04%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle           72
    Total DRAM Elapsed Cycles        cycle      106,496
    Average L1 Active Cycles         cycle       297.90
    Total L1 Elapsed Cycles          cycle       77,210
    Average L2 Active Cycles         cycle       209.88
    Total L2 Elapsed Cycles          cycle       60,408
    Average SM Active Cycles         cycle       297.90
    Total SM Elapsed Cycles          cycle       77,210
    Average SMSP Active Cycles       cycle       271.07
    Total SMSP Elapsed Cycles        cycle      308,840
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.79%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.30% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.367%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 69.95% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.79%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.30% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.543%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 66.47% above the average, while the minimum instance value is 82.37% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.83
    Elapsed Cycles                cycle       47,654
    Memory Throughput                 %        50.72
    DRAM Throughput                   %        50.72
    Duration                         us        58.34
    L1/TEX Cache Throughput           %        37.44
    L2 Cache Throughput               %        35.11
    SM Active Cycles              cycle    45,156.80
    Compute (SM) Throughput           %        69.38
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.50
    Achieved Active Warps Per SM           warp        37.68
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.5%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   162,402.67
    Total DRAM Elapsed Cycles        cycle    1,921,024
    Average L1 Active Cycles         cycle    45,156.80
    Total L1 Elapsed Cycles          cycle    1,410,160
    Average L2 Active Cycles         cycle    42,114.54
    Total L2 Elapsed Cycles          cycle    1,091,208
    Average SM Active Cycles         cycle    45,156.80
    Total SM Elapsed Cycles          cycle    1,410,160
    Average SMSP Active Cycles       cycle    44,993.37
    Total SMSP Elapsed Cycles        cycle    5,640,640
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.99
    Elapsed Cycles                cycle       33,128
    Memory Throughput                 %        66.16
    DRAM Throughput                   %        66.16
    Duration                         us        40.54
    L1/TEX Cache Throughput           %        38.85
    L2 Cache Throughput               %        33.14
    SM Active Cycles              cycle    31,305.07
    Compute (SM) Throughput           %        50.81
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.34
    Achieved Active Warps Per SM           warp        37.12
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.66%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      147,352
    Total DRAM Elapsed Cycles        cycle    1,336,320
    Average L1 Active Cycles         cycle    31,305.07
    Total L1 Elapsed Cycles          cycle      985,320
    Average L2 Active Cycles         cycle    28,651.29
    Total L2 Elapsed Cycles          cycle      758,592
    Average SM Active Cycles         cycle    31,305.07
    Total SM Elapsed Cycles          cycle      985,320
    Average SMSP Active Cycles       cycle    30,717.07
    Total SMSP Elapsed Cycles        cycle    3,941,280
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.93
    Elapsed Cycles                cycle       47,475
    Memory Throughput                 %        46.89
    DRAM Throughput                   %        46.89
    Duration                         us        58.11
    L1/TEX Cache Throughput           %        37.46
    L2 Cache Throughput               %        35.27
    SM Active Cycles              cycle    45,134.97
    Compute (SM) Throughput           %        68.96
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.41
    Achieved Active Warps Per SM           warp        37.64
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.59%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      149,408
    Total DRAM Elapsed Cycles        cycle    1,911,808
    Average L1 Active Cycles         cycle    45,134.97
    Total L1 Elapsed Cycles          cycle    1,416,700
    Average L2 Active Cycles         cycle    42,004.75
    Total L2 Elapsed Cycles          cycle    1,087,176
    Average SM Active Cycles         cycle    45,134.97
    Total SM Elapsed Cycles          cycle    1,416,700
    Average SMSP Active Cycles       cycle    44,871.31
    Total SMSP Elapsed Cycles        cycle    5,666,800
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.92
    Elapsed Cycles                cycle       34,224
    Memory Throughput                 %        70.13
    DRAM Throughput                   %        70.13
    Duration                         us        41.89
    L1/TEX Cache Throughput           %        39.11
    L2 Cache Throughput               %        32.18
    SM Active Cycles              cycle    31,112.50
    Compute (SM) Throughput           %        51.10
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.81
    Achieved Active Warps Per SM           warp        36.87
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.19%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,101.33
    Total DRAM Elapsed Cycles        cycle    1,378,304
    Average L1 Active Cycles         cycle    31,112.50
    Total L1 Elapsed Cycles          cycle      978,690
    Average L2 Active Cycles         cycle    28,824.83
    Total L2 Elapsed Cycles          cycle      783,648
    Average SM Active Cycles         cycle    31,112.50
    Total SM Elapsed Cycles          cycle      978,690
    Average SMSP Active Cycles       cycle    30,754.91
    Total SMSP Elapsed Cycles        cycle    3,914,760
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.07
    Elapsed Cycles                cycle       47,460
    Memory Throughput                 %        46.79
    DRAM Throughput                   %        46.79
    Duration                         us        58.08
    L1/TEX Cache Throughput           %        37.41
    L2 Cache Throughput               %        35.20
    SM Active Cycles              cycle    45,195.23
    Compute (SM) Throughput           %        68.89
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.29
    Achieved Active Warps Per SM           warp        37.58
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.71%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      149,096
    Total DRAM Elapsed Cycles        cycle    1,911,808
    Average L1 Active Cycles         cycle    45,195.23
    Total L1 Elapsed Cycles          cycle    1,415,780
    Average L2 Active Cycles         cycle    41,975.17
    Total L2 Elapsed Cycles          cycle    1,086,672
    Average SM Active Cycles         cycle    45,195.23
    Total SM Elapsed Cycles          cycle    1,415,780
    Average SMSP Active Cycles       cycle    44,897.18
    Total SMSP Elapsed Cycles        cycle    5,663,120
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.89
    Elapsed Cycles                cycle       34,142
    Memory Throughput                 %        70.41
    DRAM Throughput                   %        70.41
    Duration                         us        41.79
    L1/TEX Cache Throughput           %        39.23
    L2 Cache Throughput               %        32.24
    SM Active Cycles              cycle    31,014.47
    Compute (SM) Throughput           %        51.19
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.56
    Achieved Active Warps Per SM           warp        37.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.44%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,376
    Total DRAM Elapsed Cycles        cycle    1,375,232
    Average L1 Active Cycles         cycle    31,014.47
    Total L1 Elapsed Cycles          cycle      975,560
    Average L2 Active Cycles         cycle    29,100.96
    Total L2 Elapsed Cycles          cycle      781,872
    Average SM Active Cycles         cycle    31,014.47
    Total SM Elapsed Cycles          cycle      975,560
    Average SMSP Active Cycles       cycle    30,823.64
    Total SMSP Elapsed Cycles        cycle    3,902,240
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.28
    SM Frequency                    Mhz       793.28
    Elapsed Cycles                cycle        2,615
    Memory Throughput                 %         0.97
    DRAM Throughput                   %         0.47
    Duration                         us         3.30
    L1/TEX Cache Throughput           %         3.65
    L2 Cache Throughput               %         0.97
    SM Active Cycles              cycle       328.60
    Compute (SM) Throughput           %         0.65
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.12
    Achieved Active Warps Per SM           warp         7.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.88%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        82.67
    Total DRAM Elapsed Cycles        cycle      104,448
    Average L1 Active Cycles         cycle       328.60
    Total L1 Elapsed Cycles          cycle       77,130
    Average L2 Active Cycles         cycle       221.08
    Total L2 Elapsed Cycles          cycle       59,832
    Average SM Active Cycles         cycle       328.60
    Total SM Elapsed Cycles          cycle       77,130
    Average SMSP Active Cycles       cycle       277.47
    Total SMSP Elapsed Cycles        cycle      308,520
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.573%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.07% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.551%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 69.97% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.573%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.07% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.398%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 60.87% above the average, while the minimum instance value is 83.26% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.17
    Elapsed Cycles                cycle       47,495
    Memory Throughput                 %        50.78
    DRAM Throughput                   %        50.78
    Duration                         us        58.11
    L1/TEX Cache Throughput           %        37.50
    L2 Cache Throughput               %        35.24
    SM Active Cycles              cycle    45,086.07
    Compute (SM) Throughput           %        68.75
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.31
    Achieved Active Warps Per SM           warp        37.59
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.69%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,986.67
    Total DRAM Elapsed Cycles        cycle    1,913,856
    Average L1 Active Cycles         cycle    45,086.07
    Total L1 Elapsed Cycles          cycle    1,416,200
    Average L2 Active Cycles         cycle    41,933.46
    Total L2 Elapsed Cycles          cycle    1,087,224
    Average SM Active Cycles         cycle    45,086.07
    Total SM Elapsed Cycles          cycle    1,416,200
    Average SMSP Active Cycles       cycle    44,777.81
    Total SMSP Elapsed Cycles        cycle    5,664,800
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.54
    Elapsed Cycles                cycle       34,261
    Memory Throughput                 %        70.19
    DRAM Throughput                   %        70.19
    Duration                         us        41.95
    L1/TEX Cache Throughput           %        39.35
    L2 Cache Throughput               %        32.03
    SM Active Cycles              cycle    30,796.53
    Compute (SM) Throughput           %        51.30
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.31
    Achieved Active Warps Per SM           warp        37.11
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.69%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,589.33
    Total DRAM Elapsed Cycles        cycle    1,381,376
    Average L1 Active Cycles         cycle    30,796.53
    Total L1 Elapsed Cycles          cycle      972,480
    Average L2 Active Cycles         cycle    28,926.42
    Total L2 Elapsed Cycles          cycle      784,320
    Average SM Active Cycles         cycle    30,796.53
    Total SM Elapsed Cycles          cycle      972,480
    Average SMSP Active Cycles       cycle    30,835.80
    Total SMSP Elapsed Cycles        cycle    3,889,920
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.07
    Elapsed Cycles                cycle       47,563
    Memory Throughput                 %        50.64
    DRAM Throughput                   %        50.64
    Duration                         us        58.21
    L1/TEX Cache Throughput           %        37.52
    L2 Cache Throughput               %        35.14
    SM Active Cycles              cycle    45,065.20
    Compute (SM) Throughput           %        68.88
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.16
    Achieved Active Warps Per SM           warp        37.52
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,778.67
    Total DRAM Elapsed Cycles        cycle    1,916,928
    Average L1 Active Cycles         cycle    45,065.20
    Total L1 Elapsed Cycles          cycle    1,411,390
    Average L2 Active Cycles         cycle    41,938.17
    Total L2 Elapsed Cycles          cycle    1,089,000
    Average SM Active Cycles         cycle    45,065.20
    Total SM Elapsed Cycles          cycle    1,411,390
    Average SMSP Active Cycles       cycle    44,667.93
    Total SMSP Elapsed Cycles        cycle    5,645,560
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.49
    Elapsed Cycles                cycle       34,286
    Memory Throughput                 %        70.02
    DRAM Throughput                   %        70.02
    Duration                         us        41.98
    L1/TEX Cache Throughput           %        39.50
    L2 Cache Throughput               %        32.01
    SM Active Cycles              cycle    30,655.33
    Compute (SM) Throughput           %        51.41
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.24
    Achieved Active Warps Per SM           warp        37.08
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.76%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,208
    Total DRAM Elapsed Cycles        cycle    1,381,376
    Average L1 Active Cycles         cycle    30,655.33
    Total L1 Elapsed Cycles          cycle      969,210
    Average L2 Active Cycles         cycle    29,067.92
    Total L2 Elapsed Cycles          cycle      785,040
    Average SM Active Cycles         cycle    30,655.33
    Total SM Elapsed Cycles          cycle      969,210
    Average SMSP Active Cycles       cycle    30,799.11
    Total SMSP Elapsed Cycles        cycle    3,876,840
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.32
    Elapsed Cycles                cycle       47,578
    Memory Throughput                 %        50.53
    DRAM Throughput                   %        50.53
    Duration                         us        58.21
    L1/TEX Cache Throughput           %        37.55
    L2 Cache Throughput               %        35.09
    SM Active Cycles              cycle    45,030.73
    Compute (SM) Throughput           %        68.63
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.38
    Achieved Active Warps Per SM           warp        37.62
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.62%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,450.67
    Total DRAM Elapsed Cycles        cycle    1,916,928
    Average L1 Active Cycles         cycle    45,030.73
    Total L1 Elapsed Cycles          cycle    1,414,260
    Average L2 Active Cycles         cycle    41,849.62
    Total L2 Elapsed Cycles          cycle    1,089,432
    Average SM Active Cycles         cycle    45,030.73
    Total SM Elapsed Cycles          cycle    1,414,260
    Average SMSP Active Cycles       cycle    44,680.66
    Total SMSP Elapsed Cycles        cycle    5,657,040
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.62
    Elapsed Cycles                cycle       33,243
    Memory Throughput                 %        66.67
    DRAM Throughput                   %        66.67
    Duration                         us        40.70
    L1/TEX Cache Throughput           %        39.20
    L2 Cache Throughput               %        33.03
    SM Active Cycles              cycle    30,649.67
    Compute (SM) Throughput           %        50.96
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.17
    Achieved Active Warps Per SM           warp        37.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.83%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      148,944
    Total DRAM Elapsed Cycles        cycle    1,340,416
    Average L1 Active Cycles         cycle    30,649.67
    Total L1 Elapsed Cycles          cycle      976,600
    Average L2 Active Cycles         cycle    28,957.79
    Total L2 Elapsed Cycles          cycle      761,208
    Average SM Active Cycles         cycle    30,649.67
    Total SM Elapsed Cycles          cycle      976,600
    Average SMSP Active Cycles       cycle    30,771.56
    Total SMSP Elapsed Cycles        cycle    3,906,400
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.33
    SM Frequency                    Mhz       796.26
    Elapsed Cycles                cycle        2,600
    Memory Throughput                 %         1.00
    DRAM Throughput                   %         0.51
    Duration                         us         3.26
    L1/TEX Cache Throughput           %         4.03
    L2 Cache Throughput               %         1.00
    SM Active Cycles              cycle       297.60
    Compute (SM) Throughput           %         0.65
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.96
    Achieved Active Warps Per SM           warp         7.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.04%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle           88
    Total DRAM Elapsed Cycles        cycle      104,448
    Average L1 Active Cycles         cycle       297.60
    Total L1 Elapsed Cycles          cycle       78,170
    Average L2 Active Cycles         cycle       224.33
    Total L2 Elapsed Cycles          cycle       59,496
    Average SM Active Cycles         cycle       297.60
    Total SM Elapsed Cycles          cycle       78,170
    Average SMSP Active Cycles       cycle       264.40
    Total SMSP Elapsed Cycles        cycle      312,680
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.678%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.22% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.112%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 70.09% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.678%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.22% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.955%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 65.80% above the average, while the minimum instance value is 83.51% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.05
    Elapsed Cycles                cycle       47,434
    Memory Throughput                 %        50.81
    DRAM Throughput                   %        50.81
    Duration                         us        58.05
    L1/TEX Cache Throughput           %        37.52
    L2 Cache Throughput               %        35.21
    SM Active Cycles              cycle    45,059.93
    Compute (SM) Throughput           %        68.82
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.38
    Achieved Active Warps Per SM           warp        37.62
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.62%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,800
    Total DRAM Elapsed Cycles        cycle    1,910,784
    Average L1 Active Cycles         cycle    45,059.93
    Total L1 Elapsed Cycles          cycle    1,408,600
    Average L2 Active Cycles         cycle    41,931.58
    Total L2 Elapsed Cycles          cycle    1,086,048
    Average SM Active Cycles         cycle    45,059.93
    Total SM Elapsed Cycles          cycle    1,408,600
    Average SMSP Active Cycles       cycle    44,748.49
    Total SMSP Elapsed Cycles        cycle    5,634,400
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.25
    Elapsed Cycles                cycle       34,353
    Memory Throughput                 %        70.19
    DRAM Throughput                   %        70.19
    Duration                         us        42.08
    L1/TEX Cache Throughput           %        39.10
    L2 Cache Throughput               %        31.96
    SM Active Cycles              cycle    30,956.57
    Compute (SM) Throughput           %        50.79
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.17
    Achieved Active Warps Per SM           warp        37.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.83%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,829.33
    Total DRAM Elapsed Cycles        cycle    1,383,424
    Average L1 Active Cycles         cycle    30,956.57
    Total L1 Elapsed Cycles          cycle      978,920
    Average L2 Active Cycles         cycle    28,685.25
    Total L2 Elapsed Cycles          cycle      786,384
    Average SM Active Cycles         cycle    30,956.57
    Total SM Elapsed Cycles          cycle      978,920
    Average SMSP Active Cycles       cycle    30,890.23
    Total SMSP Elapsed Cycles        cycle    3,915,680
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.90
    Elapsed Cycles                cycle       47,475
    Memory Throughput                 %        50.81
    DRAM Throughput                   %        50.81
    Duration                         us        58.11
    L1/TEX Cache Throughput           %        37.60
    L2 Cache Throughput               %        35.15
    SM Active Cycles              cycle    44,964.73
    Compute (SM) Throughput           %        68.98
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.29
    Achieved Active Warps Per SM           warp        37.58
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.71%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,989.33
    Total DRAM Elapsed Cycles        cycle    1,912,832
    Average L1 Active Cycles         cycle    44,964.73
    Total L1 Elapsed Cycles          cycle    1,403,300
    Average L2 Active Cycles         cycle    41,848.79
    Total L2 Elapsed Cycles          cycle    1,086,864
    Average SM Active Cycles         cycle    44,964.73
    Total SM Elapsed Cycles          cycle    1,403,300
    Average SMSP Active Cycles       cycle    44,737.64
    Total SMSP Elapsed Cycles        cycle    5,613,200
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.49
    Elapsed Cycles                cycle       34,285
    Memory Throughput                 %        70.36
    DRAM Throughput                   %        70.36
    Duration                         us        41.98
    L1/TEX Cache Throughput           %        38.99
    L2 Cache Throughput               %        32.02
    SM Active Cycles              cycle    30,879.83
    Compute (SM) Throughput           %        50.61
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.65
    Achieved Active Warps Per SM           warp        37.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.35%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   162,114.67
    Total DRAM Elapsed Cycles        cycle    1,382,400
    Average L1 Active Cycles         cycle    30,879.83
    Total L1 Elapsed Cycles          cycle      981,650
    Average L2 Active Cycles         cycle    29,041.67
    Total L2 Elapsed Cycles          cycle      784,848
    Average SM Active Cycles         cycle    30,879.83
    Total SM Elapsed Cycles          cycle      981,650
    Average SMSP Active Cycles       cycle    30,741.99
    Total SMSP Elapsed Cycles        cycle    3,926,600
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.06
    Elapsed Cycles                cycle       47,433
    Memory Throughput                 %        51.05
    DRAM Throughput                   %        51.05
    Duration                         us        58.05
    L1/TEX Cache Throughput           %        37.64
    L2 Cache Throughput               %        35.11
    SM Active Cycles              cycle    44,916.63
    Compute (SM) Throughput           %        68.94
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.22
    Achieved Active Warps Per SM           warp        37.55
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.78%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   162,589.33
    Total DRAM Elapsed Cycles        cycle    1,910,784
    Average L1 Active Cycles         cycle    44,916.63
    Total L1 Elapsed Cycles          cycle    1,402,720
    Average L2 Active Cycles         cycle    41,805.29
    Total L2 Elapsed Cycles          cycle    1,086,168
    Average SM Active Cycles         cycle    44,916.63
    Total SM Elapsed Cycles          cycle    1,402,720
    Average SMSP Active Cycles       cycle    44,682.94
    Total SMSP Elapsed Cycles        cycle    5,610,880
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.57
    Elapsed Cycles                cycle       33,579
    Memory Throughput                 %        65.42
    DRAM Throughput                   %        65.42
    Duration                         us        41.12
    L1/TEX Cache Throughput           %        38.64
    L2 Cache Throughput               %        32.78
    SM Active Cycles              cycle    30,706.80
    Compute (SM) Throughput           %        50.10
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.40
    Achieved Active Warps Per SM           warp        37.15
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.6%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   147,482.67
    Total DRAM Elapsed Cycles        cycle    1,352,704
    Average L1 Active Cycles         cycle    30,706.80
    Total L1 Elapsed Cycles          cycle      990,610
    Average L2 Active Cycles         cycle    28,663.88
    Total L2 Elapsed Cycles          cycle      768,864
    Average SM Active Cycles         cycle    30,706.80
    Total SM Elapsed Cycles          cycle      990,610
    Average SMSP Active Cycles       cycle    30,694.53
    Total SMSP Elapsed Cycles        cycle    3,962,440
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.38
    SM Frequency                    Mhz       799.50
    Elapsed Cycles                cycle        2,689
    Memory Throughput                 %         0.94
    DRAM Throughput                   %         0.41
    Duration                         us         3.36
    L1/TEX Cache Throughput           %         3.90
    L2 Cache Throughput               %         0.94
    SM Active Cycles              cycle       307.60
    Compute (SM) Throughput           %         0.63
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.95
    Achieved Active Warps Per SM           warp         7.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.05%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        74.67
    Total DRAM Elapsed Cycles        cycle      108,544
    Average L1 Active Cycles         cycle       307.60
    Total L1 Elapsed Cycles          cycle       77,210
    Average L2 Active Cycles         cycle       230.33
    Total L2 Elapsed Cycles          cycle       61,608
    Average SM Active Cycles         cycle       307.60
    Total SM Elapsed Cycles          cycle       77,210
    Average SMSP Active Cycles       cycle       278.57
    Total SMSP Elapsed Cycles        cycle      308,840
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.032%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.21% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.564%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 69.88% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.032%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.21% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.788%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 64.51% above the average, while the minimum instance value is 83.94% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.95
    Elapsed Cycles                cycle       47,141
    Memory Throughput                 %        47.12
    DRAM Throughput                   %        47.12
    Duration                         us        57.70
    L1/TEX Cache Throughput           %        37.55
    L2 Cache Throughput               %        35.45
    SM Active Cycles              cycle    45,024.60
    Compute (SM) Throughput           %        68.66
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.16
    Achieved Active Warps Per SM           warp        37.52
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   149,173.33
    Total DRAM Elapsed Cycles        cycle    1,899,520
    Average L1 Active Cycles         cycle    45,024.60
    Total L1 Elapsed Cycles          cycle    1,406,880
    Average L2 Active Cycles         cycle    41,744.08
    Total L2 Elapsed Cycles          cycle    1,079,160
    Average SM Active Cycles         cycle    45,024.60
    Total SM Elapsed Cycles          cycle    1,406,880
    Average SMSP Active Cycles       cycle    44,761.22
    Total SMSP Elapsed Cycles        cycle    5,627,520
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.31
    Elapsed Cycles                cycle       33,127
    Memory Throughput                 %        67.20
    DRAM Throughput                   %        67.20
    Duration                         us        40.58
    L1/TEX Cache Throughput           %        39.12
    L2 Cache Throughput               %        33.20
    SM Active Cycles              cycle    30,876.13
    Compute (SM) Throughput           %        50.71
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.70
    Achieved Active Warps Per SM           warp        36.82
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.3%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      149,664
    Total DRAM Elapsed Cycles        cycle    1,336,320
    Average L1 Active Cycles         cycle    30,876.13
    Total L1 Elapsed Cycles          cycle      978,470
    Average L2 Active Cycles         cycle    28,735.67
    Total L2 Elapsed Cycles          cycle      758,616
    Average SM Active Cycles         cycle    30,876.13
    Total SM Elapsed Cycles          cycle      978,470
    Average SMSP Active Cycles       cycle    30,759.66
    Total SMSP Elapsed Cycles        cycle    3,913,880
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.00
    Elapsed Cycles                cycle       47,378
    Memory Throughput                 %        50.80
    DRAM Throughput                   %        50.80
    Duration                         us        57.98
    L1/TEX Cache Throughput           %        37.61
    L2 Cache Throughput               %        35.13
    SM Active Cycles              cycle    44,955.77
    Compute (SM) Throughput           %        68.98
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.95
    Achieved Active Warps Per SM           warp        37.42
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.05%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,592
    Total DRAM Elapsed Cycles        cycle    1,908,736
    Average L1 Active Cycles         cycle    44,955.77
    Total L1 Elapsed Cycles          cycle    1,399,810
    Average L2 Active Cycles         cycle    41,814.12
    Total L2 Elapsed Cycles          cycle    1,084,776
    Average SM Active Cycles         cycle    44,955.77
    Total SM Elapsed Cycles          cycle    1,399,810
    Average SMSP Active Cycles       cycle    44,653.94
    Total SMSP Elapsed Cycles        cycle    5,599,240
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.51
    Elapsed Cycles                cycle       33,212
    Memory Throughput                 %        66.33
    DRAM Throughput                   %        66.33
    Duration                         us        40.67
    L1/TEX Cache Throughput           %        39.15
    L2 Cache Throughput               %        33.09
    SM Active Cycles              cycle    30,476.60
    Compute (SM) Throughput           %        50.72
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.12
    Achieved Active Warps Per SM           warp        37.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.88%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   148,074.67
    Total DRAM Elapsed Cycles        cycle    1,339,392
    Average L1 Active Cycles         cycle    30,476.60
    Total L1 Elapsed Cycles          cycle      977,700
    Average L2 Active Cycles         cycle    28,653.33
    Total L2 Elapsed Cycles          cycle      760,536
    Average SM Active Cycles         cycle    30,476.60
    Total SM Elapsed Cycles          cycle      977,700
    Average SMSP Active Cycles       cycle    30,283.26
    Total SMSP Elapsed Cycles        cycle    3,910,800
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.38
    SM Frequency                    Mhz       797.80
    Elapsed Cycles                cycle        2,735
    Memory Throughput                 %         0.94
    DRAM Throughput                   %         0.51
    Duration                         us         3.42
    L1/TEX Cache Throughput           %         3.99
    L2 Cache Throughput               %         0.94
    SM Active Cycles              cycle          301
    Compute (SM) Throughput           %         0.65
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.98
    Achieved Active Warps Per SM           warp         7.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.02%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        93.33
    Total DRAM Elapsed Cycles        cycle      110,592
    Average L1 Active Cycles         cycle          301
    Total L1 Elapsed Cycles          cycle       77,550
    Average L2 Active Cycles         cycle       226.71
    Total L2 Elapsed Cycles          cycle       62,424
    Average SM Active Cycles         cycle          301
    Total SM Elapsed Cycles          cycle       77,550
    Average SMSP Active Cycles       cycle       271.17
    Total SMSP Elapsed Cycles        cycle      310,200
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.826%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.21% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.343%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 70.00% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.826%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.21% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.555%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.73% above the average, while the minimum instance value is 83.68% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.36
    Elapsed Cycles                cycle       47,425
    Memory Throughput                 %        50.78
    DRAM Throughput                   %        50.78
    Duration                         us        58.02
    L1/TEX Cache Throughput           %        37.59
    L2 Cache Throughput               %        35.00
    SM Active Cycles              cycle    44,976.33
    Compute (SM) Throughput           %        68.73
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.21
    Achieved Active Warps Per SM           warp        37.54
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.79%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,714.67
    Total DRAM Elapsed Cycles        cycle    1,910,784
    Average L1 Active Cycles         cycle    44,976.33
    Total L1 Elapsed Cycles          cycle    1,403,920
    Average L2 Active Cycles         cycle    41,820.21
    Total L2 Elapsed Cycles          cycle    1,085,784
    Average SM Active Cycles         cycle    44,976.33
    Total SM Elapsed Cycles          cycle    1,403,920
    Average SMSP Active Cycles       cycle    44,711.34
    Total SMSP Elapsed Cycles        cycle    5,615,680
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.06
    Elapsed Cycles                cycle       33,130
    Memory Throughput                 %        66.96
    DRAM Throughput                   %        66.96
    Duration                         us        40.54
    L1/TEX Cache Throughput           %        38.80
    L2 Cache Throughput               %        33.18
    SM Active Cycles              cycle    31,056.17
    Compute (SM) Throughput           %        50.26
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.79
    Achieved Active Warps Per SM           warp        37.34
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.21%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      149,144
    Total DRAM Elapsed Cycles        cycle    1,336,320
    Average L1 Active Cycles         cycle    31,056.17
    Total L1 Elapsed Cycles          cycle      986,310
    Average L2 Active Cycles         cycle    28,664.62
    Total L2 Elapsed Cycles          cycle      758,544
    Average SM Active Cycles         cycle    31,056.17
    Total SM Elapsed Cycles          cycle      986,310
    Average SMSP Active Cycles       cycle    30,853.03
    Total SMSP Elapsed Cycles        cycle    3,945,240
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.39
    SM Frequency                    Mhz       798.24
    Elapsed Cycles                cycle        2,635
    Memory Throughput                 %         0.97
    DRAM Throughput                   %         0.54
    Duration                         us         3.30
    L1/TEX Cache Throughput           %         3.92
    L2 Cache Throughput               %         0.97
    SM Active Cycles              cycle       306.03
    Compute (SM) Throughput           %         0.59
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.78
    Achieved Active Warps Per SM           warp         7.10
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.22%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (14.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle           96
    Total DRAM Elapsed Cycles        cycle      106,496
    Average L1 Active Cycles         cycle       306.03
    Total L1 Elapsed Cycles          cycle       76,670
    Average L2 Active Cycles         cycle       216.42
    Total L2 Elapsed Cycles          cycle       60,168
    Average SM Active Cycles         cycle       306.03
    Total SM Elapsed Cycles          cycle       76,670
    Average SMSP Active Cycles       cycle       274.27
    Total SMSP Elapsed Cycles        cycle      306,680
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.047%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.20% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.515%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 70.03% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.047%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.20% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.361%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 62.10% above the average, while the minimum instance value is 82.90% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.78
    Elapsed Cycles                cycle       47,391
    Memory Throughput                 %        50.68
    DRAM Throughput                   %        50.68
    Duration                         us        58.02
    L1/TEX Cache Throughput           %        37.49
    L2 Cache Throughput               %        35.22
    SM Active Cycles              cycle    45,106.73
    Compute (SM) Throughput           %        68.57
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.85
    Achieved Active Warps Per SM           warp        37.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.15%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,306.67
    Total DRAM Elapsed Cycles        cycle    1,909,760
    Average L1 Active Cycles         cycle    45,106.73
    Total L1 Elapsed Cycles          cycle    1,406,250
    Average L2 Active Cycles         cycle    41,910.04
    Total L2 Elapsed Cycles          cycle    1,085,232
    Average SM Active Cycles         cycle    45,106.73
    Total SM Elapsed Cycles          cycle    1,406,250
    Average SMSP Active Cycles       cycle    44,680.71
    Total SMSP Elapsed Cycles        cycle    5,625,000
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.89
    Elapsed Cycles                cycle       34,063
    Memory Throughput                 %        70.45
    DRAM Throughput                   %        70.45
    Duration                         us        41.70
    L1/TEX Cache Throughput           %        39.20
    L2 Cache Throughput               %        32.32
    SM Active Cycles              cycle    31,198.83
    Compute (SM) Throughput           %        50.74
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.04
    Achieved Active Warps Per SM           warp        36.98
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.96%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,242.67
    Total DRAM Elapsed Cycles        cycle    1,373,184
    Average L1 Active Cycles         cycle    31,198.83
    Total L1 Elapsed Cycles          cycle      976,270
    Average L2 Active Cycles         cycle    28,935.42
    Total L2 Elapsed Cycles          cycle      779,976
    Average SM Active Cycles         cycle    31,198.83
    Total SM Elapsed Cycles          cycle      976,270
    Average SMSP Active Cycles       cycle    30,274.98
    Total SMSP Elapsed Cycles        cycle    3,905,080
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.97
    Elapsed Cycles                cycle       47,296
    Memory Throughput                 %        50.82
    DRAM Throughput                   %        50.82
    Duration                         us        57.89
    L1/TEX Cache Throughput           %        37.62
    L2 Cache Throughput               %        35.19
    SM Active Cycles              cycle    44,944.13
    Compute (SM) Throughput           %        68.61
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.89
    Achieved Active Warps Per SM           warp        37.39
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.11%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   161,338.67
    Total DRAM Elapsed Cycles        cycle    1,904,640
    Average L1 Active Cycles         cycle    44,944.13
    Total L1 Elapsed Cycles          cycle    1,403,440
    Average L2 Active Cycles         cycle    41,809.75
    Total L2 Elapsed Cycles          cycle    1,082,976
    Average SM Active Cycles         cycle    44,944.13
    Total SM Elapsed Cycles          cycle    1,403,440
    Average SMSP Active Cycles       cycle    44,664.90
    Total SMSP Elapsed Cycles        cycle    5,613,760
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.80
    Elapsed Cycles                cycle       33,275
    Memory Throughput                 %        66.51
    DRAM Throughput                   %        66.51
    Duration                         us        40.74
    L1/TEX Cache Throughput           %        38.80
    L2 Cache Throughput               %        33.03
    SM Active Cycles              cycle    30,547.87
    Compute (SM) Throughput           %        50.17
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.16
    Achieved Active Warps Per SM           warp        37.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      148,688
    Total DRAM Elapsed Cycles        cycle    1,341,440
    Average L1 Active Cycles         cycle    30,547.87
    Total L1 Elapsed Cycles          cycle      986,380
    Average L2 Active Cycles         cycle    28,992.17
    Total L2 Elapsed Cycles          cycle      761,976
    Average SM Active Cycles         cycle    30,547.87
    Total SM Elapsed Cycles          cycle      986,380
    Average SMSP Active Cycles       cycle    30,906.75
    Total SMSP Elapsed Cycles        cycle    3,945,520
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.20
    Elapsed Cycles                cycle       47,206
    Memory Throughput                 %        51.17
    DRAM Throughput                   %        51.17
    Duration                         us        57.76
    L1/TEX Cache Throughput           %        37.64
    L2 Cache Throughput               %        35.32
    SM Active Cycles              cycle    44,918.10
    Compute (SM) Throughput           %        68.52
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.84
    Achieved Active Warps Per SM           warp        37.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.16%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      162,176
    Total DRAM Elapsed Cycles        cycle    1,901,568
    Average L1 Active Cycles         cycle    44,918.10
    Total L1 Elapsed Cycles          cycle    1,403,440
    Average L2 Active Cycles         cycle    41,814.17
    Total L2 Elapsed Cycles          cycle    1,080,792
    Average SM Active Cycles         cycle    44,918.10
    Total SM Elapsed Cycles          cycle    1,403,440
    Average SMSP Active Cycles       cycle    44,542.79
    Total SMSP Elapsed Cycles        cycle    5,613,760
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.51
    Elapsed Cycles                cycle       34,414
    Memory Throughput                 %        69.95
    DRAM Throughput                   %        69.95
    Duration                         us        42.14
    L1/TEX Cache Throughput           %        39.17
    L2 Cache Throughput               %        31.95
    SM Active Cycles              cycle    30,432.30
    Compute (SM) Throughput           %        50.59
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.57
    Achieved Active Warps Per SM           warp        37.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.43%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,768
    Total DRAM Elapsed Cycles        cycle    1,387,520
    Average L1 Active Cycles         cycle    30,432.30
    Total L1 Elapsed Cycles          cycle      977,270
    Average L2 Active Cycles         cycle    28,705.71
    Total L2 Elapsed Cycles          cycle      788,088
    Average SM Active Cycles         cycle    30,432.30
    Total SM Elapsed Cycles          cycle      977,270
    Average SMSP Active Cycles       cycle    30,491.23
    Total SMSP Elapsed Cycles        cycle    3,909,080
    -------------------------- ----------- ------------

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       817.25
    Elapsed Cycles                cycle       47,080
    Memory Throughput                 %        47.28
    DRAM Throughput                   %        47.28
    Duration                         us        57.60
    L1/TEX Cache Throughput           %        37.69
    L2 Cache Throughput               %        35.38
    SM Active Cycles              cycle    44,867.40
    Compute (SM) Throughput           %        68.03
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.04
    Achieved Active Warps Per SM           warp        37.46
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.96%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   149,530.67
    Total DRAM Elapsed Cycles        cycle    1,897,472
    Average L1 Active Cycles         cycle    44,867.40
    Total L1 Elapsed Cycles          cycle    1,411,760
    Average L2 Active Cycles         cycle    41,754.33
    Total L2 Elapsed Cycles          cycle    1,077,912
    Average SM Active Cycles         cycle    44,867.40
    Total SM Elapsed Cycles          cycle    1,411,760
    Average SMSP Active Cycles       cycle    44,572.44
    Total SMSP Elapsed Cycles        cycle    5,647,040
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.59
    Elapsed Cycles                cycle       34,314
    Memory Throughput                 %        70.44
    DRAM Throughput                   %        70.44
    Duration                         us        42.02
    L1/TEX Cache Throughput           %        39.41
    L2 Cache Throughput               %        32.09
    SM Active Cycles              cycle    31,042.23
    Compute (SM) Throughput           %        50.85
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.80
    Achieved Active Warps Per SM           warp        36.86
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.2%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   162,282.67
    Total DRAM Elapsed Cycles        cycle    1,382,400
    Average L1 Active Cycles         cycle    31,042.23
    Total L1 Elapsed Cycles          cycle      971,170
    Average L2 Active Cycles         cycle    28,861.62
    Total L2 Elapsed Cycles          cycle      785,832
    Average SM Active Cycles         cycle    31,042.23
    Total SM Elapsed Cycles          cycle      971,170
    Average SMSP Active Cycles       cycle    30,653.67
    Total SMSP Elapsed Cycles        cycle    3,884,680
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.38
    SM Frequency                    Mhz       798.38
    Elapsed Cycles                cycle        2,659
    Memory Throughput                 %         0.97
    DRAM Throughput                   %         0.48
    Duration                         us         3.33
    L1/TEX Cache Throughput           %         4.06
    L2 Cache Throughput               %         0.97
    SM Active Cycles              cycle       295.50
    Compute (SM) Throughput           %         0.65
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.95
    Achieved Active Warps Per SM           warp         7.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.05%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        85.33
    Total DRAM Elapsed Cycles        cycle      107,520
    Average L1 Active Cycles         cycle       295.50
    Total L1 Elapsed Cycles          cycle       77,740
    Average L2 Active Cycles         cycle       226.46
    Total L2 Elapsed Cycles          cycle       60,816
    Average SM Active Cycles         cycle       295.50
    Total SM Elapsed Cycles          cycle       77,740
    Average SMSP Active Cycles       cycle       265.74
    Total SMSP Elapsed Cycles        cycle      310,960
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.668%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.24% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.193%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 70.14% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.668%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.24% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.512%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 61.68% above the average, while the minimum instance value is 83.66% below the      
          average.                                                                                                      

  heat_kernel_2d(float *, float *, int, int, float, float, float, float, BoundaryCondition) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.48
    SM Frequency                    Mhz       816.99
    Elapsed Cycles                cycle       46,985
    Memory Throughput                 %        47.39
    DRAM Throughput                   %        47.39
    Duration                         us        57.50
    L1/TEX Cache Throughput           %        37.77
    L2 Cache Throughput               %        35.45
    SM Active Cycles              cycle    44,770.30
    Compute (SM) Throughput           %        68.40
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.84
    Achieved Active Warps Per SM           warp        37.36
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.16%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      149,472
    Total DRAM Elapsed Cycles        cycle    1,892,352
    Average L1 Active Cycles         cycle    44,770.30
    Total L1 Elapsed Cycles          cycle    1,402,170
    Average L2 Active Cycles         cycle    41,854.04
    Total L2 Elapsed Cycles          cycle    1,075,896
    Average SM Active Cycles         cycle    44,770.30
    Total SM Elapsed Cycles          cycle    1,402,170
    Average SMSP Active Cycles       cycle    44,579.15
    Total SMSP Elapsed Cycles        cycle    5,608,680
    -------------------------- ----------- ------------

  heat_to_color_kernel_2d(float *, uchar4 *, int, int) (63, 63, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.49
    SM Frequency                    Mhz       816.62
    Elapsed Cycles                cycle       33,793
    Memory Throughput                 %        70.92
    DRAM Throughput                   %        70.92
    Duration                         us        41.38
    L1/TEX Cache Throughput           %        39.24
    L2 Cache Throughput               %        32.48
    SM Active Cycles              cycle    30,642.87
    Compute (SM) Throughput           %        50.58
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,969
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread       1,016,064
    Uses Green Context                                             0
    Waves Per SM                                               22.05
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.34
    Achieved Active Warps Per SM           warp        37.12
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.66%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      161,104
    Total DRAM Elapsed Cycles        cycle    1,362,944
    Average L1 Active Cycles         cycle    30,642.87
    Total L1 Elapsed Cycles          cycle      975,240
    Average L2 Active Cycles         cycle    28,567.50
    Total L2 Elapsed Cycles          cycle      773,880
    Average SM Active Cycles         cycle    30,642.87
    Total SM Elapsed Cycles          cycle      975,240
    Average SMSP Active Cycles       cycle    30,416.77
    Total SMSP Elapsed Cycles        cycle    3,900,960
    -------------------------- ----------- ------------

  add_heat_kernel_2d(float *, int, int, int, int) (1, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.33
    SM Frequency                    Mhz       796.42
    Elapsed Cycles                cycle        2,626
    Memory Throughput                 %         0.97
    DRAM Throughput                   %         0.44
    Duration                         us         3.30
    L1/TEX Cache Throughput           %         3.91
    L2 Cache Throughput               %         0.97
    SM Active Cycles              cycle       307.17
    Compute (SM) Throughput           %         0.66
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     10
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Threads                                   thread           2,560
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 66.67%                                                                                          
          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 30             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.02
    Achieved Active Warps Per SM           warp         7.21
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        77.33
    Total DRAM Elapsed Cycles        cycle      105,472
    Average L1 Active Cycles         cycle       307.17
    Total L1 Elapsed Cycles          cycle       76,820
    Average L2 Active Cycles         cycle       200.67
    Total L2 Elapsed Cycles          cycle       60,000
    Average SM Active Cycles         cycle       307.17
    Total SM Elapsed Cycles          cycle       76,820
    Average SMSP Active Cycles       cycle       282.10
    Total SMSP Elapsed Cycles        cycle      307,280
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.063%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 67.22% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.7%                                                                                            
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 69.89% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.063%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.22% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.425%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.58% above the average, while the minimum instance value is 81.56% below the      
          average.                                                                                                      

